<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>无监督学习 | xaz的花花世界</title>
  <meta name="keywords" content="">
  <meta name="description" content="无监督学习 | xaz的花花世界">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="description" content="Here&#39;s something encrypted, password is required to continue reading.">
<meta property="og:type" content="article">
<meta property="og:title" content="使用sscha计算非谐声子">
<meta property="og:url" content="http://example.com/2025/11/04/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%BD%BF%E7%94%A8sscha%E8%AE%A1%E7%AE%97%E9%9D%9E%E8%B0%90%E5%A3%B0%E5%AD%90/index.html">
<meta property="og:site_name" content="xaz的花花世界">
<meta property="og:description" content="Here&#39;s something encrypted, password is required to continue reading.">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-11-04T01:36:56.000Z">
<meta property="article:modified_time" content="2025-11-04T01:37:52.605Z">
<meta property="article:author" content="xaz">
<meta name="twitter:card" content="summary">


<link rel="icon" href="/img/1.png">

<link href="/css/style.css?v=1.1.0" rel="stylesheet">

<link href="/css/hl_theme/atom-light.css?v=1.1.0" rel="stylesheet">

<link href="//cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css" rel="stylesheet">

<script src="//cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="/js/titleTip.js?v=1.1.0" ></script>

<script src="//cdn.jsdelivr.net/npm/highlightjs@9.16.2/highlight.pack.min.js"></script>
<script>
    hljs.initHighlightingOnLoad();
</script>

<script src="//cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js"></script>



<script src="//cdn.jsdelivr.net/npm/jquery.cookie@1.4.1/jquery.cookie.min.js" ></script>

<script src="/js/iconfont.js?v=1.1.0" ></script>

<meta name="generator" content="Hexo 7.2.0"><link rel="alternate" href="/atom.xml" title="xaz的花花世界" type="application/atom+xml">
</head>
<div style="display: none">
  <input class="theme_disqus_on" value="false">
  <input class="theme_preload_comment" value="">
  <input class="theme_blog_path" value="">
  <input id="theme_shortcut" value="true" />
  <input id="theme_highlight_on" value="true" />
  <input id="theme_code_copy" value="true" />
</div>



<body>
<aside class="nav">
    <div class="nav-left">
        <a href="/"
   class="avatar_target">
    <img class="avatar"
         src="/img/1.png"/>
</a>
<div class="author">
    <span>xaz</span>
</div>

<div class="icon">
    
</div>





<ul>
    <li>
        <div class="all active" data-rel="全部文章">全部文章
            
                <small>(98)</small>
            
        </div>
    </li>
    
        
            
                
    <li>
        <div data-rel="学习笔记">
            
            学习笔记
            <small>(13)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="模拟方法">
            
            模拟方法
            <small>(14)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="分子动力学">
            <i class="fold iconfont icon-right"></i>
            分子动力学
            <small>(20)</small>
        </div>
        
            <ul class="sub hide">
                
                    
    <li>
        <div data-rel="分子动力学&lt;---&gt;分析方法">
            <i class="fold iconfont icon-right"></i>
            分析方法
            <small>(8)</small>
        </div>
        
            <ul class="sub hide">
                
                    
    <li>
        <div data-rel="分子动力学&lt;---&gt;分析方法&lt;---&gt;msd">
            
            msd
            <small>(3)</small>
        </div>
        
    </li>

                
            </ul>
        
    </li>

                
                    
    <li>
        <div data-rel="分子动力学&lt;---&gt;lammps模拟">
            
            lammps模拟
            <small>(9)</small>
        </div>
        
    </li>

                
                    
    <li>
        <div data-rel="分子动力学&lt;---&gt;vasp模拟">
            
            vasp模拟
            <small>(3)</small>
        </div>
        
    </li>

                
            </ul>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="机器学习">
            
            机器学习
            <small>(9)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="linux学习使用">
            
            linux学习使用
            <small>(10)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="课题研究">
            
            课题研究
            <small>(7)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="绘图与写作">
            
            绘图与写作
            <small>(6)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="杂谈">
            
            杂谈
            <small>(13)</small>
        </div>
        
    </li>

            
        
    
        
            
                
    <li>
        <div data-rel="网站收集">
            
            网站收集
            <small>(5)</small>
        </div>
        
    </li>

            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
        
            
        
    
</ul>
<div class="left-bottom">
    <div class="menus">
        
            
            
            
    </div>
    <div>
        
            <a class="about  hasFriend  site_url"
               
               href="/about">关于</a>
        
        <a style="width: 50%"
                
                                           class="friends">友链</a>
        
    </div>
</div>
<input type="hidden" id="yelog_site_posts_number" value="98">
<input type="hidden" id="yelog_site_word_count" value="162k">
<div style="display: none">
    <span id="busuanzi_value_site_uv"></span>
    <span id="busuanzi_value_site_pv"></span>
</div>

    </div>
    <div class="nav-right">
        <div class="friends-area">
    <div class="friends-title">
        友情链接
        <i class="iconfont icon-left"></i>
    </div>
    <div class="friends-content">
        <ul>
            
            <li><a target="_blank" href="https://www.ruanyifeng.com/blog/">阮一峰的网络日志</a></li>
            
            <li><a target="_blank" href="https://www.fomal.cn/social/link/">Fomalhaut</a></li>
            
            <li><a target="_blank" href="https://fe32.top/">唐志远</a></li>
            
            <li><a target="_blank" href="https://clashgithub.com/">clash节点分享</a></li>
            
            <li><a target="_blank" href="https://study.hycbook.com/">兼一书虫</a></li>
            
            <li><a target="_blank" href="https://blognas.hwb0307.com/">Bensz</a></li>
            
            <li><a target="_blank" href="https://wiki.cheng-group.net/">厦门大学程俊课题组</a></li>
            
            <li><a target="_blank" href="https://advancedmath.org/">advancedmath</a></li>
            
            <li><a target="_blank" href="https://blog.shishiruqi.com/">世事如棋</a></li>
            
            <li><a target="_blank" href="https://qijingzheng.github.io/">郑奇靖老师</a></li>
            
            <li><a target="_blank" href="https://blog.yizhou.ac.cn/">某个学弟</a></li>
            
            <li><a target="_blank" href="https://wiki.scio.icu/index.php/">一个wiki格式的博客</a></li>
            
            <li><a target="_blank" href="https://liuyuyang.net/">一个很励志的博客</a></li>
            
            <li><a target="_blank" href="https://wukan.me/">一个程序员</a></li>
            
            <li><a target="_blank" href="https://whuzbf.github.io/">理论物理方向一个博主郑卜凡</a></li>
            
            <li><a target="_blank" href="https://egoist.dev/">一个独立开发者chatwise作者</a></li>
            
            <li><a target="_blank" href="https://www.skyue.com/">拾月</a></li>
            
            <li><a target="_blank" href="https://www.damtp.cam.ac.uk/user/tong/index.html/">Professor David Tong</a></li>
            
        </ul>
    </div>
</div>
        <div class="title-list">
    <div class="right-top">
        <div id="default-panel">
            <i class="iconfont icon-search" data-title="搜索 快捷键 i"></i>
            <div class="right-title">全部文章</div>
            <i class="iconfont icon-file-tree" data-title="切换到大纲视图 快捷键 w"></i>
        </div>
        <div id="search-panel">
            <i class="iconfont icon-left" data-title="返回"></i>
            <input id="local-search-input" autocomplete="off"/>
            <label class="border-line" for="input"></label>
            <i class="iconfont icon-case-sensitive" data-title="大小写敏感"></i>
            <i class="iconfont icon-tag" data-title="标签"></i>
        </div>
        <div id="outline-panel" style="display: none">
            <div class="right-title">大纲</div>
            <i class="iconfont icon-list" data-title="切换到文章列表"></i>
        </div>
    </div>

    <div class="tags-list">
    <input id="tag-search" />
    <div class="tag-wrapper">
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>节点</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>影视 音乐 游戏 pt站</a>
            </li>
        
            <li class="article-tag-list-item">
                <i class="iconfont icon-tag"></i><a>msd</a>
            </li>
        
    </div>

</div>

    
    <div id="local-search-result">

    </div>
    
    <nav id="title-list-nav">
        
        
        <a id="top" class="全部文章 学习笔记 "
           href="/2024/05/27/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%BE%85%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="待学习内容">待学习内容</span>
            <span class="post-date" title="2024-05-27 09:28:23">2024/05/27</span>
        </a>
        
        
        <a id="top" class="全部文章 杂谈 "
           href="/2024/05/25/8-%E6%9D%82%E8%B0%88/%E7%AC%AC%E4%B8%80-%E7%9C%81%E8%87%AA%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="省自集">省自集</span>
            <span class="post-date" title="2024-05-25 21:14:43">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2025/11/04/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%BD%BF%E7%94%A8sscha%E8%AE%A1%E7%AE%97%E9%9D%9E%E8%B0%90%E5%A3%B0%E5%AD%90/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="使用sscha计算非谐声子">使用sscha计算非谐声子</span>
            <span class="post-date" title="2025-11-04 09:36:56">2025/11/04</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2025/10/31/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/NaXSiO%E5%B7%A5%E4%BD%9C%E6%B5%81/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="NaXSiO工作流">NaXSiO工作流</span>
            <span class="post-date" title="2025-10-31 16:28:51">2025/10/31</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2025/10/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/openmx%E4%BD%BF%E7%94%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="openmx使用">openmx使用</span>
            <span class="post-date" title="2025-10-25 09:37:08">2025/10/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2025/09/22/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%94%B5%E6%B1%A0%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="电池基本知识">电池基本知识</span>
            <span class="post-date" title="2025-09-22 11:15:08">2025/09/22</span>
        </a>
        
        
        <a  class="全部文章 "
           href="/2025/09/08/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="无监督学习">无监督学习</span>
            <span class="post-date" title="2025-09-08 10:48:28">2025/09/08</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2025/06/03/8-%E6%9D%82%E8%B0%88/%E5%90%8D%E5%AE%B6%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="名家集">名家集</span>
            <span class="post-date" title="2025-06-03 09:59:13">2025/06/03</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2025/05/27/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BC%96%E8%AF%91ACNN/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="编译ACNN">编译ACNN</span>
            <span class="post-date" title="2025-05-27 09:30:30">2025/05/27</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2025/04/16/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/%E5%85%B3%E4%BA%8E%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="关于毕业论文">关于毕业论文</span>
            <span class="post-date" title="2025-04-16 10:29:27">2025/04/16</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 vasp模拟 "
           href="/2025/04/14/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/vasp%E4%B8%AD%E7%9A%84%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%B3%BB%E7%BB%BC/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="vasp中的分子动力学系综">vasp中的分子动力学系综</span>
            <span class="post-date" title="2025-04-14 09:17:38">2025/04/14</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2025/04/09/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="vscode快捷键">vscode快捷键</span>
            <span class="post-date" title="2025-04-09 20:47:00">2025/04/09</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2025/04/07/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/python%E7%BB%98%E5%9B%BE%E5%9B%BE%E6%A1%88%E8%A7%84%E8%8C%83/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="python绘图图案规范">python绘图图案规范</span>
            <span class="post-date" title="2025-04-07 09:12:00">2025/04/07</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2025/02/17/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/bader%E7%94%B5%E8%8D%B7%E5%88%86%E6%9E%90/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="bader电荷分析">bader电荷分析</span>
            <span class="post-date" title="2025-02-17 14:21:06">2025/02/17</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2025/02/14/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/mattersim-x/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="mattersim-x">mattersim-x</span>
            <span class="post-date" title="2025-02-14 13:55:22">2025/02/14</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2025/02/11/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%8A%9F%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="功函数计算">功函数计算</span>
            <span class="post-date" title="2025-02-11 11:18:17">2025/02/11</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2025/01/21/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="大模型">大模型</span>
            <span class="post-date" title="2025-01-21 21:22:55">2025/01/21</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2025/01/09/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E4%B8%80%E4%BA%9B%E8%AF%BE%E9%A2%98/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="一些课题">一些课题</span>
            <span class="post-date" title="2025-01-09 14:10:02">2025/01/09</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2025/01/02/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E9%95%B6%E5%B5%8C%E5%9B%BE%E5%BD%A2/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="镶嵌图形">镶嵌图形</span>
            <span class="post-date" title="2025-01-02 11:07:02">2025/01/02</span>
        </a>
        
        
        <a  class="全部文章 网站收集 "
           href="/2024/12/25/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E4%BA%A4%E6%B5%81%E7%AB%99/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="交流站">交流站</span>
            <span class="post-date" title="2024-12-25 16:15:50">2024/12/25</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2024/12/17/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E6%B2%B8%E7%9F%B3%E7%BB%93%E6%9E%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="沸石结构">沸石结构</span>
            <span class="post-date" title="2024-12-17 10:58:35">2024/12/17</span>
        </a>
        
        
        <a  class="全部文章 网站收集 "
           href="/2024/12/16/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E8%B5%84%E6%BA%90%E7%BD%91%E7%AB%99/"
           data-tag="影视 音乐 游戏 pt站"
           data-author="" >
            <span class="post-title" title="资源网站">资源网站</span>
            <span class="post-date" title="2024-12-16 16:56:28">2024/12/16</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/10/25/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="git使用">git使用</span>
            <span class="post-date" title="2024-10-25 08:55:46">2024/10/25</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/10/03/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/manjaro%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="manjaro学习笔记">manjaro学习笔记</span>
            <span class="post-date" title="2024-10-03 11:38:17">2024/10/03</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/08/29/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%8D%E5%90%8C%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="不同的主动学习方法">不同的主动学习方法</span>
            <span class="post-date" title="2024-08-29 08:08:58">2024/08/29</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/08/14/8-%E6%9D%82%E8%B0%88/%E8%AF%BB%E4%B9%A6%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="读书集">读书集</span>
            <span class="post-date" title="2024-08-14 15:53:48">2024/08/14</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/08/10/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E9%AB%98%E7%B2%BE%E5%BA%A6%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9E%84%E5%BB%BA%E6%AD%A5%E9%AA%A4/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="高精度数据集构建步骤">高精度数据集构建步骤</span>
            <span class="post-date" title="2024-08-10 14:04:46">2024/08/10</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/07/18/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/OER%E5%92%8CHER/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="OER台阶图">OER台阶图</span>
            <span class="post-date" title="2024-07-18 20:55:56">2024/07/18</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/06/21/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="机器学习的理论基础">机器学习的理论基础</span>
            <span class="post-date" title="2024-06-21 19:40:04">2024/06/21</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/06/17/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BB%9F%E8%AE%A1%E5%8A%9B%E5%AD%A6/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="统计力学">统计力学</span>
            <span class="post-date" title="2024-06-17 21:25:23">2024/06/17</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/06/05/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/dpmd-dpgen/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="dpmd-dpgen">dpmd-dpgen</span>
            <span class="post-date" title="2024-06-05 15:22:59">2024/06/05</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/06/05/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/meta-dynamics-plumed/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="meta_dynamics_plumed">meta_dynamics_plumed</span>
            <span class="post-date" title="2024-06-05 15:22:59">2024/06/05</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/06/05/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%BB%8EMP%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8F%90%E5%8F%96%E7%BB%93%E6%9E%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="从MP数据库提取结构">从MP数据库提取结构</span>
            <span class="post-date" title="2024-06-05 10:19:13">2024/06/05</span>
        </a>
        
        
        <a  class="全部文章 网站收集 "
           href="/2024/06/01/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E6%95%85%E4%BA%8B%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="故事集">故事集</span>
            <span class="post-date" title="2024-06-01 20:29:00">2024/06/01</span>
        </a>
        
        
        <a  class="全部文章 网站收集 "
           href="/2024/05/31/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E7%9F%A5%E8%AF%86%E7%BD%91%E7%AB%99/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="知识网站">知识网站</span>
            <span class="post-date" title="2024-05-31 10:08:33">2024/05/31</span>
        </a>
        
        
        <a  class="全部文章 网站收集 "
           href="/2024/05/30/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E5%B7%A5%E5%85%B7%E7%BD%91%E7%AB%99/"
           data-tag="节点"
           data-author="" >
            <span class="post-title" title="工具网站">工具网站</span>
            <span class="post-date" title="2024-05-30 21:49:59">2024/05/30</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/30/8-%E6%9D%82%E8%B0%88/%E9%A3%8E%E7%89%A9%E9%9B%B6%E9%A3%9F%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="风物零食集">风物零食集</span>
            <span class="post-date" title="2024-05-30 19:01:58">2024/05/30</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/28/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/zotero%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="zotero使用技巧">zotero使用技巧</span>
            <span class="post-date" title="2024-05-28 16:16:10">2024/05/28</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2024/05/27/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/%E5%86%99%E4%BD%9C%E5%B7%A5%E5%85%B7/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="写作工具">写作工具</span>
            <span class="post-date" title="2024-05-27 19:37:31">2024/05/27</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/27/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/slurm%E7%B3%BB%E7%BB%9F/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="slurm系统">slurm系统</span>
            <span class="post-date" title="2024-05-27 16:36:48">2024/05/27</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/27/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/lammps_plumed%E5%AE%89%E8%A3%85/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="lammps_plumed安装">lammps_plumed安装</span>
            <span class="post-date" title="2024-05-27 09:36:27">2024/05/27</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/26/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/windows%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="windows使用技巧">windows使用技巧</span>
            <span class="post-date" title="2024-05-26 12:59:08">2024/05/26</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E9%AB%98%E9%80%9A%E9%87%8F%E7%AD%9B%E9%80%89%E7%BB%93%E6%9E%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="高通量筛选结构">高通量筛选结构</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E8%A1%A8%E9%9D%A2%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="表面计算">表面计算</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BD%91%E6%98%93%E5%85%AC%E5%BC%80%E8%AF%BE_%E6%B0%A2%E5%8E%9F%E5%AD%90%E6%B3%A2%E5%87%BD%E6%95%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="网易公开课_氢原子波函数">网易公开课_氢原子波函数</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BC%BA%E9%99%B7%E8%83%BD/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="缺陷能">缺陷能</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2024/05/25/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/%E7%BB%98%E5%9B%BE%E7%BB%86%E8%8A%82/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="绘图细节">绘图细节</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%BA%B3%E7%B1%B3%E9%9A%8F%E6%9C%BA%E7%83%AD%E5%8A%9B%E5%AD%A6%E6%9A%91%E6%9C%9F%E8%AE%B2%E4%B9%A0%E7%8F%AD%202023/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="纳米随机热力学暑期讲习班">纳米随机热力学暑期讲习班</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E7%94%BB%E7%9B%B8%E5%9B%BE-%E8%AE%A1%E7%AE%97%E5%90%88%E6%88%90%E8%B7%AF%E5%BE%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="画相图-计算可能的合成路径">画相图-计算可能的合成路径</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/25/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/linux%E4%B8%AD%E7%9A%84%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="linux中的软件安装">linux中的软件安装</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E7%94%B5%E6%B1%A0%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="电池计算">电池计算</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E7%94%B5%E5%AD%90%E8%83%BD%E5%B8%A6%E7%90%86%E8%AE%BA/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="电子能带理论">电子能带理论</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/05/25/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="机器学习与主动学习">机器学习与主动学习</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%99%B6%E6%A0%BC%E6%8C%AF%E5%8A%A8%E4%B8%8E%E5%A3%B0%E5%AD%90/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="晶格振动与声子">晶格振动与声子</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%A4%84%E7%90%86%E5%88%86%E6%95%B0%E5%8D%A0%E6%8D%AE%E7%BB%93%E6%9E%84/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="处理分数占据结构">处理分数占据结构</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%90%86%E8%A7%A3%E5%8F%8A%E5%8F%AF%E5%88%A9%E7%94%A8%E7%9A%84%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="分子动力学理解及可利用的计算">分子动力学理解及可利用的计算</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/25/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/VDOS%20and%20band%20center/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="VDOS and band center">VDOS and band center</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2024/05/25/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E5%85%83%E7%B4%A0%E4%BB%B7%E6%80%81/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="元素价态">元素价态</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/vasp%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="vasp使用教程">vasp使用教程</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2024/05/25/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/Te%E7%BA%B3%E7%B1%B3%E7%BA%BF/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="Te纳米线">Te纳米线</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%B8%80%E4%BA%9Bpython%E8%84%9A%E6%9C%AC/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="一些python脚本">一些python脚本</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 模拟方法 "
           href="/2024/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/NEB%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="NEB计算">NEB计算</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 绘图与写作 "
           href="/2024/05/25/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/markdown&latex%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="markdown&amp;latex使用教程">markdown&amp;latex使用教程</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/25/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/linux%E6%93%8D%E4%BD%9C/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="linux操作">linux操作</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 linux学习使用 "
           href="/2024/05/25/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/ABACUS%E4%BD%BF%E7%94%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="ABACUS使用">ABACUS使用</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 课题研究 "
           href="/2024/05/25/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/can_do/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="can_do">can_do</span>
            <span class="post-date" title="2024-05-25 22:01:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/lammps%E8%AE%A1%E7%AE%97msd/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="lammps计算msd">lammps计算msd</span>
            <span class="post-date" title="2024-05-25 10:42:50">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/%E8%BD%AC%E6%8D%A2lammps%E8%BD%A8%E8%BF%B9%E4%B8%BAXDATCAR/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="转换lammps轨迹为XDATCAR">转换lammps轨迹为XDATCAR</span>
            <span class="post-date" title="2024-05-25 10:41:42">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/lammps%E5%A4%84%E7%90%86%E8%AE%A1%E7%AE%97%E7%BB%93%E6%9E%9C/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="lammps处理计算结果">lammps处理计算结果</span>
            <span class="post-date" title="2024-05-25 10:32:15">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/lammps%E6%89%B9%E9%87%8F%E6%8F%90%E4%BB%BB%E5%8A%A1/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="lammps批量提任务">lammps批量提任务</span>
            <span class="post-date" title="2024-05-25 10:31:33">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 机器学习 "
           href="/2024/05/25/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%85%B3%E4%BA%8Emaxvol/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="关于maxvol">关于maxvol</span>
            <span class="post-date" title="2024-05-25 10:30:25">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/%E8%BD%AC%E6%8D%A2POSCAR/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="转换POSCAR">转换POSCAR</span>
            <span class="post-date" title="2024-05-25 10:27:07">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 msd "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/npt%E7%B3%BB%E7%BB%BC%E4%B8%AD%E7%9A%84msd/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="npt系综中的msd">npt系综中的msd</span>
            <span class="post-date" title="2024-05-25 10:16:55">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/lammps%E7%9A%84%E8%BE%93%E5%87%BA/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="lammps的输出">lammps的输出</span>
            <span class="post-date" title="2024-05-25 10:15:56">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/%E5%86%85%E7%BD%AE%E7%9A%84msd%E7%AE%97%E6%B3%95&%E8%B4%A8%E5%BF%83%E9%A3%98%E7%A7%BB%E5%8A%A8/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="内置的msd算法&amp;质心飘移&amp;TMSD与MSD">内置的msd算法&amp;质心飘移&amp;TMSD与MSD</span>
            <span class="post-date" title="2024-05-25 10:11:17">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/%E5%8F%82%E6%95%B0variable/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="参数variable">参数variable</span>
            <span class="post-date" title="2024-05-25 10:09:44">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 lammps模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/lammps%E6%A8%A1%E6%8B%9F/in-lmp%E5%8F%82%E6%95%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="in.lmp参数">in.lmp参数</span>
            <span class="post-date" title="2024-05-25 10:05:19">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 vasp模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/vasp%E6%89%B9%E9%87%8F%E6%8F%90%E4%BB%BB%E5%8A%A1/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="vasp批量提任务">vasp批量提任务</span>
            <span class="post-date" title="2024-05-25 09:36:55">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/van-hove-%E5%87%BD%E6%95%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="van hove 函数">van hove 函数</span>
            <span class="post-date" title="2024-05-25 09:23:40">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E5%A4%9A%E9%9D%A2%E4%BD%93%E6%8C%87%E6%95%B0%E8%AE%A1%E7%AE%97/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="多面体指数计算">多面体指数计算</span>
            <span class="post-date" title="2024-05-25 09:22:20">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 vasp模拟 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/%E5%8D%87%E6%B8%A9%E8%BF%87%E7%A8%8B%E8%BE%A8%E6%9E%90/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="升温过程辨析">升温过程辨析</span>
            <span class="post-date" title="2024-05-25 09:21:27">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E5%BE%84%E5%90%91%E5%88%86%E5%B8%83%E5%87%BD%E6%95%B0/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="径向分布函数-rdf">径向分布函数-rdf</span>
            <span class="post-date" title="2024-05-25 09:19:29">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%9B%BE/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="概率密度图">概率密度图</span>
            <span class="post-date" title="2024-05-25 09:18:21">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 msd "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/msd%E6%94%B6%E6%95%9B%E6%80%A7%E5%88%86%E6%9E%90/"
           data-tag="msd"
           data-author="" >
            <span class="post-title" title="msd收敛性分析">msd收敛性分析</span>
            <span class="post-date" title="2024-05-25 09:09:28">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 分子动力学 分析方法 msd "
           href="/2024/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/vasp%E6%A0%BC%E5%BC%8F%E8%AE%A1%E7%AE%97msd/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="vasp格式计算msd">vasp格式计算msd</span>
            <span class="post-date" title="2024-05-25 09:03:59">2024/05/25</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/24/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E8%B6%85%E7%A6%BB%E5%AD%90%E5%AF%BC%E4%BD%93%E4%B8%AD%E9%98%BF%E4%BC%A6%E5%B0%BC%E4%B9%8C%E6%96%AF%E5%85%AC%E5%BC%8F%E7%9A%84%E7%89%A9%E7%90%86%E8%B5%B7%E6%BA%90/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="超离子导体中阿伦尼乌斯公式的物理起源">超离子导体中阿伦尼乌斯公式的物理起源</span>
            <span class="post-date" title="2024-05-24 21:17:54">2024/05/24</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/24/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%9B%BA%E6%80%81%E7%A6%BB%E5%AD%90%E5%AD%A6/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="固态离子学">固态离子学</span>
            <span class="post-date" title="2024-05-24 00:00:00">2024/05/24</span>
        </a>
        
        
        <a  class="全部文章 学习笔记 "
           href="/2024/05/23/1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB%E6%A0%87%E5%87%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="文献阅读标准">文献阅读标准</span>
            <span class="post-date" title="2024-05-23 21:26:05">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E7%A9%BA%E6%9A%87%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="空暇集">空暇集</span>
            <span class="post-date" title="2024-05-23 20:47:46">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E7%96%BE%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="疾集">疾集</span>
            <span class="post-date" title="2024-05-23 14:34:22">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E4%BF%A1%E5%BF%B5%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="信念集">信念集</span>
            <span class="post-date" title="2024-05-23 14:19:39">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E6%84%A4%E6%85%A8%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="愤慨集">愤慨集</span>
            <span class="post-date" title="2024-05-23 14:15:43">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/other/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="other">other</span>
            <span class="post-date" title="2024-05-23 00:14:33">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/future/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="future">future</span>
            <span class="post-date" title="2024-05-23 00:14:33">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E4%BB%BB%E5%8A%A1%E6%B8%85%E5%8D%95/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="任务清单">任务清单</span>
            <span class="post-date" title="2024-05-23 00:14:33">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E5%A5%87%E6%80%9D%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="奇思集">奇思集</span>
            <span class="post-date" title="2024-05-23 00:14:33">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/23/8-%E6%9D%82%E8%B0%88/%E5%B9%B4%E5%8D%8E%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="年华集">年华集</span>
            <span class="post-date" title="2024-05-23 00:14:33">2024/05/23</span>
        </a>
        
        
        <a  class="全部文章 杂谈 "
           href="/2024/05/11/8-%E6%9D%82%E8%B0%88/%E7%A4%BE%E4%BC%9A%E9%9B%86/"
           data-tag=""
           data-author="" >
            <span class="post-title" title="社会集">社会集</span>
            <span class="post-date" title="2024-05-11 09:02:21">2024/05/11</span>
        </a>
        
        <div id="no-item-tips">

        </div>
    </nav>
    <div id="outline-list">
    </div>
</div>

    </div>
    <div class="hide-list">
        <div class="semicircle" data-title="切换全屏 快捷键 s">
            <div class="brackets first"><</div>
            <div class="brackets">&gt;</div>
        </div>
    </div>
</aside>
<div id="post">
    <div class="pjax">
        <article id="post-4-机器学习/无监督学习" class="article article-type-post" itemscope itemprop="blogPost">
    
        <h1 class="article-title">无监督学习</h1>
    
    <div class="article-meta">
        
        
        
        
    </div>
    <div class="article-meta">
        
            发布时间 : <time class="date" title='最后更新: 2025-09-12 20:25:32'>2025-09-08 10:48</time>
        
    </div>
    <div class="article-meta">
        
        <span>字数:24.9k</span>
        
        
        
    </div>
    
    <div class="toc-ref">
    
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="toc-text">1. 无监督学习介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">2. 无监督学习和监督学习的区别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E8%81%9A%E7%B1%BB"><span class="toc-text">3.聚类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E9%99%8D%E7%BB%B4"><span class="toc-text">4.降维</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gemini"><span class="toc-text">Gemini</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-text">什么是无监督学习？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E7%9B%AE%E6%A0%87"><span class="toc-text">核心思想与目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95"><span class="toc-text">主要类型与常见算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89"><span class="toc-text">1. 聚类（Clustering）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%99%8D%E7%BB%B4%EF%BC%88Dimensionality-Reduction%EF%BC%89"><span class="toc-text">2. 降维（Dimensionality Reduction）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%EF%BC%88Association-Rule-Mining%EF%BC%89"><span class="toc-text">3. 关联规则挖掘（Association Rule Mining）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-vs-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">无监督学习 vs. 监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AF%B9%E2%80%9C%E6%95%B4%E4%BD%93%E2%80%9D%E8%BF%9B%E8%A1%8C%E6%8E%A2%E7%B4%A2%E5%92%8C%E6%9E%84%E5%BB%BA"><span class="toc-text">无监督学习：对“整体”进行探索和构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AF%B9%E2%80%9C%E4%B8%AA%E4%BD%93%E2%80%9D%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E5%92%8C%E5%88%A4%E6%96%AD"><span class="toc-text">监督学习：对“个体”进行预测和判断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B%E6%82%A8%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9A"><span class="toc-text">总结一下您的理解：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%81%9A%E7%B1%BB%EF%BC%9F"><span class="toc-text">什么是聚类？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E6%98%AF%E5%A6%82%E4%BD%95%E2%80%9C%E8%87%AA%E5%8A%A8%E2%80%9D%E5%88%86%E7%B1%BB%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94%E4%BB%A5K-Means%E7%AE%97%E6%B3%95%E4%B8%BA%E4%BE%8B"><span class="toc-text">聚类是如何“自动”分类的？——以K-Means算法为例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E6%82%A8%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-text">第一步：您需要输入的信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C-%E4%B8%89-%E5%9B%9B%E6%AD%A5%EF%BC%9A%E7%AE%97%E6%B3%95%E7%9A%84%E2%80%9C%E8%87%AA%E5%8A%A8%E2%80%9D%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">第二-三-四步：算法的“自动”工作流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E6%94%B6%E6%95%9B%E4%B8%8E%E7%BB%93%E6%9E%9C"><span class="toc-text">第五步：收敛与结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%82%A8%E7%9A%84%E7%96%91%E9%97%AE"><span class="toc-text">总结您的疑问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87-%E2%80%94%E2%80%94-%E5%AE%9A%E4%B9%89%E4%BD%A0%E7%9A%84%E2%80%9C%E6%A0%87%E5%87%86%E2%80%9D%EF%BC%8C%E5%B9%B6%E7%BF%BB%E8%AF%91%E7%BB%99%E6%9C%BA%E5%99%A8"><span class="toc-text">第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-Feature-Engineering"><span class="toc-text">1. 特征工程 (Feature Engineering)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-Feature-Scaling"><span class="toc-text">2. 特征缩放 (Feature Scaling)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E7%AE%97%E6%B3%95%E9%80%89%E6%8B%A9%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4-%E2%80%94%E2%80%94-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E2%80%9C%E5%B0%BA%E5%AD%90%E2%80%9D"><span class="toc-text">第二阶段：算法选择与参数调整 —— 选择合适的“尺子”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-text">1. 选择合适的聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%B0%83%E6%95%B4%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0-Hyperparameter-Tuning"><span class="toc-text">2. 调整关键参数 (Hyperparameter Tuning)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%90%8E%E6%9C%9F%E9%AA%8C%E8%AF%81%E4%B8%8E%E8%A7%A3%E8%AF%BB-%E2%80%94%E2%80%94-%E4%BA%BA%E5%B7%A5%E7%9A%84%E2%80%9C%E6%9C%80%E7%BB%88%E9%AA%8C%E6%94%B6%E2%80%9D"><span class="toc-text">第三阶段：后期验证与解读 —— 人工的“最终验收”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E7%94%BB%E5%83%8F-Profiling"><span class="toc-text">1. 结果分析与画像 (Profiling)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%9A%E5%8A%A1%E9%AA%8C%E8%AF%81"><span class="toc-text">2. 业务验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E6%8C%89%E5%8D%95%E7%A7%91%E6%88%90%E7%BB%A9%E6%8E%92%E5%BA%8F%EF%BC%88%E6%82%A8%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-text">场景一：按单科成绩排序（您的方法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E5%AF%BB%E6%89%BE%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AD%A6%E7%94%9F%E7%BE%A4%E4%BD%93%EF%BC%88%E6%97%A0%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB%EF%BC%89"><span class="toc-text">场景二：寻找不同类型的学生群体（无监督聚类）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-text">核心区别总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%EF%BC%9A%E6%88%91%E5%BA%94%E8%AF%A5%E7%94%A8%E5%93%AA%E4%B8%AA%EF%BC%9F"><span class="toc-text">结论：我应该用哪个？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%99%8D%E7%BB%B4%EF%BC%9F"><span class="toc-text">我们为什么需要降维？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94-%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%9A%84%E7%B1%BB%E6%AF%94"><span class="toc-text">降维是如何实现的？—— 一个直观的类比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E4%B8%BB%E8%A6%81%E7%9A%84%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95"><span class="toc-text">两种主要的降维方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Feature-Selection"><span class="toc-text">1. 特征选择 (Feature Selection)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-Feature-Extraction"><span class="toc-text">2. 特征提取 (Feature Extraction)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%9B%A0%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E4%BB%AC%E9%83%BD%E5%8F%AB%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-text">核心原因：为什么它们都叫无监督学习？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%9A%84%E5%85%B6%E4%BB%96%E7%9B%B8%E4%BC%BC%E7%82%B9"><span class="toc-text">降维与聚类的其他相似点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E4%B8%80%E4%B8%AA%E6%AF%94%E5%96%BB%E6%9D%A5%E7%90%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E7%9A%84%E5%BC%82%E5%90%8C"><span class="toc-text">用一个比喻来理解它们的异同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%9C%8B%E6%87%82%E5%85%B3%E7%B3%BB"><span class="toc-text">总结：一张图看懂关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E7%BB%8F%E5%85%B8%E6%95%85%E4%BA%8B%EF%BC%9A%E2%80%9C%E5%95%A4%E9%85%92%E4%B8%8E%E5%B0%BF%E5%B8%83%E2%80%9D"><span class="toc-text">核心思想与经典故事：“啤酒与尿布”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E7%9A%84%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5"><span class="toc-text">关联规则的关键概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%A1%B9%E9%9B%86-Itemset"><span class="toc-text">1. 项集 (Itemset)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%94%AF%E6%8C%81%E5%BA%A6-Support"><span class="toc-text">2. 支持度 (Support)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%BD%AE%E4%BF%A1%E5%BA%A6-Confidence"><span class="toc-text">3. 置信度 (Confidence)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%8F%90%E5%8D%87%E5%BA%A6-Lift"><span class="toc-text">4. 提升度 (Lift)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94-Apriori%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">关联规则挖掘是如何工作的？—— Apriori算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">关联规则挖掘的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%EF%BC%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E4%BA%A7%E5%87%BA%E5%AE%8C%E5%85%A8%E4%B8%8D%E5%90%8C"><span class="toc-text">核心区别：目标与产出完全不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-vs-%E9%99%8D%E7%BB%B4%EF%BC%9A%E8%AF%A6%E7%BB%86%E5%BC%82%E5%90%8C%E5%AF%B9%E6%AF%94"><span class="toc-text">关联规则挖掘 vs. 降维：详细异同对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%83%E4%BB%AC%E7%9A%84%E7%9B%B8%E4%BC%BC%E7%82%B9%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="toc-text">它们的相似点在哪里？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%88%B0%E6%82%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">回到您的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E6%9D%A5%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81%E5%86%97%E4%BD%99%E6%98%AF%E2%80%9C%E4%B8%8D%E5%90%88%E9%80%82%E2%80%9D%E7%9A%84%EF%BC%9F"><span class="toc-text">为什么用关联规则挖掘来处理特征冗余是“不合适”的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81%E5%86%97%E4%BD%99%E7%9A%84%E2%80%9C%E6%AD%A3%E7%A1%AE%E5%B7%A5%E5%85%B7%E2%80%9D%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">处理特征冗余的“正确工具”是什么？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90-Correlation-Analysis-%E2%80%94%E2%80%94-%E9%92%88%E5%AF%B9%E6%95%B0%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%81"><span class="toc-text">方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95-Dimensionality-Reduction-%E2%80%94%E2%80%94-%E6%9B%B4%E4%BC%98%E8%B6%8A%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E6%96%B9%E6%A1%88"><span class="toc-text">方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E6%82%A8%E7%9A%84%E6%80%9D%E8%B7%AF%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%8D%87%E7%BA%A7%E8%B7%AF%E5%BE%84"><span class="toc-text">总结：您的思路的正确升级路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E2%80%9C%E9%BB%84%E9%87%91%E7%BB%84%E5%90%88%E2%80%9D%EF%BC%9F"><span class="toc-text">为什么这是一个“黄金组合”？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%94%BB%E5%85%8B%E2%80%9C%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE%E2%80%9D-Curse-of-Dimensionality"><span class="toc-text">1. 攻克“维度灾难” (Curse of Dimensionality)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8F%90%E5%8D%87%E8%81%9A%E7%B1%BB%E6%95%88%E6%9E%9C%E4%B8%8E%E9%80%9F%E5%BA%A6-Improving-Quality-and-Speed"><span class="toc-text">2. 提升聚类效果与速度 (Improving Quality and Speed)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%96%B9%E4%BE%BF%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3-Enabling-Visualization-and-Understanding"><span class="toc-text">3. 方便可视化与理解 (Enabling Visualization and Understanding)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%85%B8%E5%9E%8B%E7%9A%84%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B-A-Typical-Workflow"><span class="toc-text">一个典型的操作流程 (A Typical Workflow)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%94%9F%E5%8A%A8%E7%9A%84%E6%AF%94%E5%96%BB%EF%BC%9A%E9%9B%95%E5%A1%91%E5%AE%B6%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">一个生动的比喻：雕塑家的工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E5%86%B5%E4%B8%80%EF%BC%9A%E5%BD%93%E2%80%9C%E5%85%B3%E8%81%94%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90%E2%80%9D%E6%8C%87%E4%BB%A3%E2%80%9C%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E2%80%9D%E6%97%B6%EF%BC%8C%E6%82%A8%E7%9A%84%E8%AF%B4%E6%B3%95%E5%AE%8C%E5%85%A8%E6%AD%A3%E7%A1%AE%EF%BC%81"><span class="toc-text">情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E5%86%B5%E4%BA%8C%EF%BC%9A%E5%BD%93%E2%80%9C%E5%85%B3%E8%81%94%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90%E2%80%9D%E6%8C%87%E4%BB%A3%E2%80%9C%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E2%80%9D%E6%97%B6%EF%BC%8C%E6%82%A8%E7%9A%84%E8%AF%B4%E6%B3%95%E5%B0%B1%E4%B8%8D%E6%AD%A3%E7%A1%AE%E4%BA%86%E3%80%82"><span class="toc-text">情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94"><span class="toc-text">总结对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB-K-Means-Clustering"><span class="toc-text">1. K-均值聚类 (K-Means Clustering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise"><span class="toc-text">2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB-Hierarchical-Clustering"><span class="toc-text">3. 层次聚类 (Hierarchical Clustering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B-Gaussian-Mixture-Model-GMM"><span class="toc-text">4. 高斯混合模型 (Gaussian Mixture Model, GMM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-text">总结：如何选择？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%90%83%E5%BD%A2-%E5%87%B8%E5%BD%A2-Spherical-Convex-Shape"><span class="toc-text">1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A4%AD%E5%9C%86%E5%BD%A2-Elliptical-Shape"><span class="toc-text">2. 椭圆形 (Elliptical Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6-Arbitrary-Shape"><span class="toc-text">3. 任意形状 (Arbitrary Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%E7%AE%97%E6%B3%95%E5%81%8F%E5%A5%BD%E4%B8%8D%E5%90%8C%E5%BD%A2%E7%8A%B6%EF%BC%9F"><span class="toc-text">为什么不同算法偏好不同形状？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E5%A4%A7%E9%99%8D%E7%BB%B4%E6%B4%BE%E7%B3%BB"><span class="toc-text">两大降维派系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Feature-Selection-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">一、特征选择 (Feature Selection) 的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BF%87%E6%BB%A4%E6%B3%95-Filter-Methods"><span class="toc-text">1. 过滤法 (Filter Methods)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8C%85%E8%A3%85%E6%B3%95-Wrapper-Methods"><span class="toc-text">2. 包装法 (Wrapper Methods)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B5%8C%E5%85%A5%E6%B3%95-Embedded-Methods"><span class="toc-text">3. 嵌入法 (Embedded Methods)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-Feature-Extraction-%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-text">二、特征提取 (Feature Extraction) 的算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95"><span class="toc-text">A. 线性降维算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%EF%BC%88%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0-Manifold-Learning%EF%BC%89"><span class="toc-text">B. 非线性降维算法（流形学习 Manifold Learning）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F-1"><span class="toc-text">总结：如何选择？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E5%B7%AE-%E4%BF%A1%E6%81%AF%E9%87%8F"><span class="toc-text">5.什么是数据的方差&#x2F;信息量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A9%E6%88%91%E4%BB%AC%E7%94%A8%E4%B8%80%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%9D%A5%E4%B8%BE%E4%BE%8B"><span class="toc-text">让我们用一个具体的描述符来举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%8D%95%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E6%96%B9%E5%B7%AE%E5%88%B0%E2%80%9C%E6%80%BB%E6%96%B9%E5%B7%AE%E2%80%9D"><span class="toc-text">从单个特征的方差到“总方差”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E2%80%9C%E6%96%B9%E5%B7%AE%E2%80%9D%E7%9A%84%EF%BC%9F"><span class="toc-text">PCA是如何利用“方差”的？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E8%81%9A%E7%B1%BB"><span class="toc-text">6. 聚类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-DBSCAN%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="toc-text">7. DBSCAN聚类方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E8%B7%9D%E7%A6%BB%E5%9B%BE%EF%BC%88K-distance-Graph%EF%BC%89%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-text">K距离图（K-distance Graph）的含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E2%80%9C%E6%8B%90%E7%82%B9%E2%80%9D%EF%BC%88Elbow%EF%BC%89%E7%9A%84%E7%89%B9%E6%AE%8A%E6%84%8F%E4%B9%89"><span class="toc-text">2. “拐点”（Elbow）的特殊意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B8%AA%E8%BE%B9%E7%95%8C%E5%B0%B1%E6%98%AF-eps%EF%BC%9F"><span class="toc-text">3. 为什么这个边界就是 eps？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-PCA%E4%B8%8EUMAP%E5%AF%B9%E6%AF%94"><span class="toc-text">8. PCA与UMAP对比</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-KDBSCAN%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-text">9. KDBSCAN中的参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%94%9F%E5%8A%A8%E7%9A%84%E6%AF%94%E5%96%BB%EF%BC%9A%E5%AF%BB%E6%89%BE%E6%B4%BE%E5%AF%B9%E4%B8%8A%E7%9A%84%E7%A4%BE%E4%BA%A4%E5%9C%88"><span class="toc-text">一个生动的比喻：寻找派对上的社交圈</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-eps-Epsilon-%E9%82%BB%E5%9F%9F%E5%8D%8A%E5%BE%84"><span class="toc-text">1. eps (Epsilon) - 邻域半径</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-min-samples-%E6%A0%B8%E5%BF%83%E7%82%B9%E6%9C%80%E5%B0%8F%E6%A0%B7%E6%9C%AC%E6%95%B0"><span class="toc-text">2. min_samples - 核心点最小样本数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E8%A1%A8%E6%A0%BC"><span class="toc-text">总结表格</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-UMAP-HDBSCAN-or-PCA-HDBSCAN"><span class="toc-text">10. UMAP+HDBSCAN or PCA+HDBSCAN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%98%E5%85%88%E9%80%89%E6%8B%A9-UMAP-HDBSCAN%EF%BC%9F"><span class="toc-text">为什么优先选择 UMAP + HDBSCAN？</span></a></li></ol></li></ol></li></ol>
    
<style>
    .left-col .switch-btn,
    .left-col .switch-area {
        display: none;
    }
    .toc-level-4 i,
    .toc-level-4 ol {
        display: none !important;
    }
</style>
</div>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><div class='inner-toc'><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D"><span class="toc-text">1. 无监督学习介绍</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">2. 无监督学习和监督学习的区别</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E8%81%9A%E7%B1%BB"><span class="toc-text">3.聚类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E9%99%8D%E7%BB%B4"><span class="toc-text">4.降维</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Gemini"><span class="toc-text">Gemini</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-text">什么是无监督学习？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E7%9B%AE%E6%A0%87"><span class="toc-text">核心思想与目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E7%B1%BB%E5%9E%8B%E4%B8%8E%E5%B8%B8%E8%A7%81%E7%AE%97%E6%B3%95"><span class="toc-text">主要类型与常见算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%81%9A%E7%B1%BB%EF%BC%88Clustering%EF%BC%89"><span class="toc-text">1. 聚类（Clustering）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E9%99%8D%E7%BB%B4%EF%BC%88Dimensionality-Reduction%EF%BC%89"><span class="toc-text">2. 降维（Dimensionality Reduction）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%EF%BC%88Association-Rule-Mining%EF%BC%89"><span class="toc-text">3. 关联规则挖掘（Association Rule Mining）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0-vs-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-text">无监督学习 vs. 监督学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AF%B9%E2%80%9C%E6%95%B4%E4%BD%93%E2%80%9D%E8%BF%9B%E8%A1%8C%E6%8E%A2%E7%B4%A2%E5%92%8C%E6%9E%84%E5%BB%BA"><span class="toc-text">无监督学习：对“整体”进行探索和构建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9A%E5%AF%B9%E2%80%9C%E4%B8%AA%E4%BD%93%E2%80%9D%E8%BF%9B%E8%A1%8C%E9%A2%84%E6%B5%8B%E5%92%8C%E5%88%A4%E6%96%AD"><span class="toc-text">监督学习：对“个体”进行预测和判断</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E4%B8%80%E4%B8%8B%E6%82%A8%E7%9A%84%E7%90%86%E8%A7%A3%EF%BC%9A"><span class="toc-text">总结一下您的理解：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E8%81%9A%E7%B1%BB%EF%BC%9F"><span class="toc-text">什么是聚类？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%9A%E7%B1%BB%E6%98%AF%E5%A6%82%E4%BD%95%E2%80%9C%E8%87%AA%E5%8A%A8%E2%80%9D%E5%88%86%E7%B1%BB%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94%E4%BB%A5K-Means%E7%AE%97%E6%B3%95%E4%B8%BA%E4%BE%8B"><span class="toc-text">聚类是如何“自动”分类的？——以K-Means算法为例</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E6%82%A8%E9%9C%80%E8%A6%81%E8%BE%93%E5%85%A5%E7%9A%84%E4%BF%A1%E6%81%AF"><span class="toc-text">第一步：您需要输入的信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C-%E4%B8%89-%E5%9B%9B%E6%AD%A5%EF%BC%9A%E7%AE%97%E6%B3%95%E7%9A%84%E2%80%9C%E8%87%AA%E5%8A%A8%E2%80%9D%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">第二-三-四步：算法的“自动”工作流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E6%AD%A5%EF%BC%9A%E6%94%B6%E6%95%9B%E4%B8%8E%E7%BB%93%E6%9E%9C"><span class="toc-text">第五步：收敛与结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E6%82%A8%E7%9A%84%E7%96%91%E9%97%AE"><span class="toc-text">总结您的疑问</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87-%E2%80%94%E2%80%94-%E5%AE%9A%E4%B9%89%E4%BD%A0%E7%9A%84%E2%80%9C%E6%A0%87%E5%87%86%E2%80%9D%EF%BC%8C%E5%B9%B6%E7%BF%BB%E8%AF%91%E7%BB%99%E6%9C%BA%E5%99%A8"><span class="toc-text">第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-Feature-Engineering"><span class="toc-text">1. 特征工程 (Feature Engineering)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-Feature-Scaling"><span class="toc-text">2. 特征缩放 (Feature Scaling)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%98%B6%E6%AE%B5%EF%BC%9A%E7%AE%97%E6%B3%95%E9%80%89%E6%8B%A9%E4%B8%8E%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4-%E2%80%94%E2%80%94-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E2%80%9C%E5%B0%BA%E5%AD%90%E2%80%9D"><span class="toc-text">第二阶段：算法选择与参数调整 —— 选择合适的“尺子”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%80%89%E6%8B%A9%E5%90%88%E9%80%82%E7%9A%84%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95"><span class="toc-text">1. 选择合适的聚类算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E8%B0%83%E6%95%B4%E5%85%B3%E9%94%AE%E5%8F%82%E6%95%B0-Hyperparameter-Tuning"><span class="toc-text">2. 调整关键参数 (Hyperparameter Tuning)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%98%B6%E6%AE%B5%EF%BC%9A%E5%90%8E%E6%9C%9F%E9%AA%8C%E8%AF%81%E4%B8%8E%E8%A7%A3%E8%AF%BB-%E2%80%94%E2%80%94-%E4%BA%BA%E5%B7%A5%E7%9A%84%E2%80%9C%E6%9C%80%E7%BB%88%E9%AA%8C%E6%94%B6%E2%80%9D"><span class="toc-text">第三阶段：后期验证与解读 —— 人工的“最终验收”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90%E4%B8%8E%E7%94%BB%E5%83%8F-Profiling"><span class="toc-text">1. 结果分析与画像 (Profiling)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%9A%E5%8A%A1%E9%AA%8C%E8%AF%81"><span class="toc-text">2. 业务验证</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E6%8C%89%E5%8D%95%E7%A7%91%E6%88%90%E7%BB%A9%E6%8E%92%E5%BA%8F%EF%BC%88%E6%82%A8%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%89"><span class="toc-text">场景一：按单科成绩排序（您的方法）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E5%AF%BB%E6%89%BE%E4%B8%8D%E5%90%8C%E7%B1%BB%E5%9E%8B%E7%9A%84%E5%AD%A6%E7%94%9F%E7%BE%A4%E4%BD%93%EF%BC%88%E6%97%A0%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB%EF%BC%89"><span class="toc-text">场景二：寻找不同类型的学生群体（无监督聚类）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-text">核心区别总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%EF%BC%9A%E6%88%91%E5%BA%94%E8%AF%A5%E7%94%A8%E5%93%AA%E4%B8%AA%EF%BC%9F"><span class="toc-text">结论：我应该用哪个？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E9%99%8D%E7%BB%B4%EF%BC%9F"><span class="toc-text">我们为什么需要降维？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E6%98%AF%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94-%E4%B8%80%E4%B8%AA%E7%9B%B4%E8%A7%82%E7%9A%84%E7%B1%BB%E6%AF%94"><span class="toc-text">降维是如何实现的？—— 一个直观的类比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E4%B8%BB%E8%A6%81%E7%9A%84%E9%99%8D%E7%BB%B4%E6%96%B9%E6%B3%95"><span class="toc-text">两种主要的降维方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Feature-Selection"><span class="toc-text">1. 特征选择 (Feature Selection)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-Feature-Extraction"><span class="toc-text">2. 特征提取 (Feature Extraction)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8E%9F%E5%9B%A0%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E4%BB%AC%E9%83%BD%E5%8F%AB%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%9F"><span class="toc-text">核心原因：为什么它们都叫无监督学习？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%99%8D%E7%BB%B4%E4%B8%8E%E8%81%9A%E7%B1%BB%E7%9A%84%E5%85%B6%E4%BB%96%E7%9B%B8%E4%BC%BC%E7%82%B9"><span class="toc-text">降维与聚类的其他相似点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E4%B8%80%E4%B8%AA%E6%AF%94%E5%96%BB%E6%9D%A5%E7%90%86%E8%A7%A3%E5%AE%83%E4%BB%AC%E7%9A%84%E5%BC%82%E5%90%8C"><span class="toc-text">用一个比喻来理解它们的异同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E7%9C%8B%E6%87%82%E5%85%B3%E7%B3%BB"><span class="toc-text">总结：一张图看懂关系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E7%BB%8F%E5%85%B8%E6%95%85%E4%BA%8B%EF%BC%9A%E2%80%9C%E5%95%A4%E9%85%92%E4%B8%8E%E5%B0%BF%E5%B8%83%E2%80%9D"><span class="toc-text">核心思想与经典故事：“啤酒与尿布”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E7%9A%84%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5"><span class="toc-text">关联规则的关键概念</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%A1%B9%E9%9B%86-Itemset"><span class="toc-text">1. 项集 (Itemset)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%94%AF%E6%8C%81%E5%BA%A6-Support"><span class="toc-text">2. 支持度 (Support)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E7%BD%AE%E4%BF%A1%E5%BA%A6-Confidence"><span class="toc-text">3. 置信度 (Confidence)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E6%8F%90%E5%8D%87%E5%BA%A6-Lift"><span class="toc-text">4. 提升度 (Lift)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84%EF%BC%9F%E2%80%94%E2%80%94-Apriori%E7%AE%97%E6%B3%95%E7%AE%80%E4%BB%8B"><span class="toc-text">关联规则挖掘是如何工作的？—— Apriori算法简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">关联规则挖掘的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%EF%BC%9A%E7%9B%AE%E6%A0%87%E4%B8%8E%E4%BA%A7%E5%87%BA%E5%AE%8C%E5%85%A8%E4%B8%8D%E5%90%8C"><span class="toc-text">核心区别：目标与产出完全不同</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98-vs-%E9%99%8D%E7%BB%B4%EF%BC%9A%E8%AF%A6%E7%BB%86%E5%BC%82%E5%90%8C%E5%AF%B9%E6%AF%94"><span class="toc-text">关联规则挖掘 vs. 降维：详细异同对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%83%E4%BB%AC%E7%9A%84%E7%9B%B8%E4%BC%BC%E7%82%B9%E5%9C%A8%E5%93%AA%E9%87%8C%EF%BC%9F"><span class="toc-text">它们的相似点在哪里？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%88%B0%E6%82%A8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">回到您的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E6%9D%A5%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81%E5%86%97%E4%BD%99%E6%98%AF%E2%80%9C%E4%B8%8D%E5%90%88%E9%80%82%E2%80%9D%E7%9A%84%EF%BC%9F"><span class="toc-text">为什么用关联规则挖掘来处理特征冗余是“不合适”的？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%84%E7%90%86%E7%89%B9%E5%BE%81%E5%86%97%E4%BD%99%E7%9A%84%E2%80%9C%E6%AD%A3%E7%A1%AE%E5%B7%A5%E5%85%B7%E2%80%9D%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">处理特征冗余的“正确工具”是什么？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%B8%80%EF%BC%9A%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90-Correlation-Analysis-%E2%80%94%E2%80%94-%E9%92%88%E5%AF%B9%E6%95%B0%E5%80%BC%E5%9E%8B%E7%89%B9%E5%BE%81"><span class="toc-text">方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E4%BA%8C%EF%BC%9A%E4%BD%BF%E7%94%A8%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95-Dimensionality-Reduction-%E2%80%94%E2%80%94-%E6%9B%B4%E4%BC%98%E8%B6%8A%E7%9A%84%E8%87%AA%E5%8A%A8%E5%8C%96%E6%96%B9%E6%A1%88"><span class="toc-text">方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E6%82%A8%E7%9A%84%E6%80%9D%E8%B7%AF%E7%9A%84%E6%AD%A3%E7%A1%AE%E5%8D%87%E7%BA%A7%E8%B7%AF%E5%BE%84"><span class="toc-text">总结：您的思路的正确升级路径</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E2%80%9C%E9%BB%84%E9%87%91%E7%BB%84%E5%90%88%E2%80%9D%EF%BC%9F"><span class="toc-text">为什么这是一个“黄金组合”？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%94%BB%E5%85%8B%E2%80%9C%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE%E2%80%9D-Curse-of-Dimensionality"><span class="toc-text">1. 攻克“维度灾难” (Curse of Dimensionality)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%8F%90%E5%8D%87%E8%81%9A%E7%B1%BB%E6%95%88%E6%9E%9C%E4%B8%8E%E9%80%9F%E5%BA%A6-Improving-Quality-and-Speed"><span class="toc-text">2. 提升聚类效果与速度 (Improving Quality and Speed)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E6%96%B9%E4%BE%BF%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B8%8E%E7%90%86%E8%A7%A3-Enabling-Visualization-and-Understanding"><span class="toc-text">3. 方便可视化与理解 (Enabling Visualization and Understanding)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E5%85%B8%E5%9E%8B%E7%9A%84%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B-A-Typical-Workflow"><span class="toc-text">一个典型的操作流程 (A Typical Workflow)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%94%9F%E5%8A%A8%E7%9A%84%E6%AF%94%E5%96%BB%EF%BC%9A%E9%9B%95%E5%A1%91%E5%AE%B6%E7%9A%84%E5%B7%A5%E4%BD%9C"><span class="toc-text">一个生动的比喻：雕塑家的工作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E5%86%B5%E4%B8%80%EF%BC%9A%E5%BD%93%E2%80%9C%E5%85%B3%E8%81%94%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90%E2%80%9D%E6%8C%87%E4%BB%A3%E2%80%9C%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98%E2%80%9D%E6%97%B6%EF%BC%8C%E6%82%A8%E7%9A%84%E8%AF%B4%E6%B3%95%E5%AE%8C%E5%85%A8%E6%AD%A3%E7%A1%AE%EF%BC%81"><span class="toc-text">情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E5%86%B5%E4%BA%8C%EF%BC%9A%E5%BD%93%E2%80%9C%E5%85%B3%E8%81%94%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90%E2%80%9D%E6%8C%87%E4%BB%A3%E2%80%9C%E7%9B%B8%E5%85%B3%E6%80%A7%E5%88%86%E6%9E%90%E2%80%9D%E6%97%B6%EF%BC%8C%E6%82%A8%E7%9A%84%E8%AF%B4%E6%B3%95%E5%B0%B1%E4%B8%8D%E6%AD%A3%E7%A1%AE%E4%BA%86%E3%80%82"><span class="toc-text">情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94"><span class="toc-text">总结对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB-K-Means-Clustering"><span class="toc-text">1. K-均值聚类 (K-Means Clustering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise"><span class="toc-text">2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB-Hierarchical-Clustering"><span class="toc-text">3. 层次聚类 (Hierarchical Clustering)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B-Gaussian-Mixture-Model-GMM"><span class="toc-text">4. 高斯混合模型 (Gaussian Mixture Model, GMM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-text">总结：如何选择？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%90%83%E5%BD%A2-%E5%87%B8%E5%BD%A2-Spherical-Convex-Shape"><span class="toc-text">1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A4%AD%E5%9C%86%E5%BD%A2-Elliptical-Shape"><span class="toc-text">2. 椭圆形 (Elliptical Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6-Arbitrary-Shape"><span class="toc-text">3. 任意形状 (Arbitrary Shape)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C%E7%AE%97%E6%B3%95%E5%81%8F%E5%A5%BD%E4%B8%8D%E5%90%8C%E5%BD%A2%E7%8A%B6%EF%BC%9F"><span class="toc-text">为什么不同算法偏好不同形状？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-3"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E5%A4%A7%E9%99%8D%E7%BB%B4%E6%B4%BE%E7%B3%BB"><span class="toc-text">两大降维派系</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-Feature-Selection-%E7%9A%84%E6%96%B9%E6%B3%95"><span class="toc-text">一、特征选择 (Feature Selection) 的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%BF%87%E6%BB%A4%E6%B3%95-Filter-Methods"><span class="toc-text">1. 过滤法 (Filter Methods)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8C%85%E8%A3%85%E6%B3%95-Wrapper-Methods"><span class="toc-text">2. 包装法 (Wrapper Methods)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%B5%8C%E5%85%A5%E6%B3%95-Embedded-Methods"><span class="toc-text">3. 嵌入法 (Embedded Methods)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96-Feature-Extraction-%E7%9A%84%E7%AE%97%E6%B3%95"><span class="toc-text">二、特征提取 (Feature Extraction) 的算法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#A-%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95"><span class="toc-text">A. 线性降维算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#B-%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%EF%BC%88%E6%B5%81%E5%BD%A2%E5%AD%A6%E4%B9%A0-Manifold-Learning%EF%BC%89"><span class="toc-text">B. 非线性降维算法（流形学习 Manifold Learning）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%EF%BC%9A%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F-1"><span class="toc-text">总结：如何选择？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E4%BB%80%E4%B9%88%E6%98%AF%E6%95%B0%E6%8D%AE%E7%9A%84%E6%96%B9%E5%B7%AE-%E4%BF%A1%E6%81%AF%E9%87%8F"><span class="toc-text">5.什么是数据的方差&#x2F;信息量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A9%E6%88%91%E4%BB%AC%E7%94%A8%E4%B8%80%E4%B8%AA%E5%85%B7%E4%BD%93%E7%9A%84%E6%8F%8F%E8%BF%B0%E7%AC%A6%E6%9D%A5%E4%B8%BE%E4%BE%8B"><span class="toc-text">让我们用一个具体的描述符来举例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E5%8D%95%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E6%96%B9%E5%B7%AE%E5%88%B0%E2%80%9C%E6%80%BB%E6%96%B9%E5%B7%AE%E2%80%9D"><span class="toc-text">从单个特征的方差到“总方差”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#PCA%E6%98%AF%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E2%80%9C%E6%96%B9%E5%B7%AE%E2%80%9D%E7%9A%84%EF%BC%9F"><span class="toc-text">PCA是如何利用“方差”的？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E8%81%9A%E7%B1%BB"><span class="toc-text">6. 聚类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-DBSCAN%E8%81%9A%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="toc-text">7. DBSCAN聚类方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#K%E8%B7%9D%E7%A6%BB%E5%9B%BE%EF%BC%88K-distance-Graph%EF%BC%89%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-text">K距离图（K-distance Graph）的含义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E2%80%9C%E6%8B%90%E7%82%B9%E2%80%9D%EF%BC%88Elbow%EF%BC%89%E7%9A%84%E7%89%B9%E6%AE%8A%E6%84%8F%E4%B9%89"><span class="toc-text">2. “拐点”（Elbow）的特殊意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%99%E4%B8%AA%E8%BE%B9%E7%95%8C%E5%B0%B1%E6%98%AF-eps%EF%BC%9F"><span class="toc-text">3. 为什么这个边界就是 eps？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-4"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-PCA%E4%B8%8EUMAP%E5%AF%B9%E6%AF%94"><span class="toc-text">8. PCA与UMAP对比</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-KDBSCAN%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-text">9. KDBSCAN中的参数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%94%9F%E5%8A%A8%E7%9A%84%E6%AF%94%E5%96%BB%EF%BC%9A%E5%AF%BB%E6%89%BE%E6%B4%BE%E5%AF%B9%E4%B8%8A%E7%9A%84%E7%A4%BE%E4%BA%A4%E5%9C%88"><span class="toc-text">一个生动的比喻：寻找派对上的社交圈</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-eps-Epsilon-%E9%82%BB%E5%9F%9F%E5%8D%8A%E5%BE%84"><span class="toc-text">1. eps (Epsilon) - 邻域半径</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-min-samples-%E6%A0%B8%E5%BF%83%E7%82%B9%E6%9C%80%E5%B0%8F%E6%A0%B7%E6%9C%AC%E6%95%B0"><span class="toc-text">2. min_samples - 核心点最小样本数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93%E8%A1%A8%E6%A0%BC"><span class="toc-text">总结表格</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-text"></span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#10-UMAP-HDBSCAN-or-PCA-HDBSCAN"><span class="toc-text">10. UMAP+HDBSCAN or PCA+HDBSCAN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%98%E5%85%88%E9%80%89%E6%8B%A9-UMAP-HDBSCAN%EF%BC%9F"><span class="toc-text">为什么优先选择 UMAP + HDBSCAN？</span></a></li></ol></li></ol></li></ol></div></p>
<h1 id="1-无监督学习介绍"><a href="#1-无监督学习介绍" class="headerlink" title="1. 无监督学习介绍"></a>1. 无监督学习介绍</h1><p>无监督学习（Unsupervised Learning）是机器学习的一个核心分支，其主要特点是使用<strong>未经标记</strong>的数据进行训练。无监督学习算法需要自行在数据中探索和发现隐藏的结构、模式和关系。</p>
<p>无监督学习主要包括以下几种任务类型：</p>
<p>聚类：对数据进行分类。相比于排序，聚类的优势是能够在多维分析。聚类不是为了“排名”，而是为了“画像”</p>
<p>降维：减少数据特征的数量。降维就是在尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</p>
<p>关联规则挖掘：在数据中发现项与项的关联关系。关联特征分析不处理特征数值的大小，只处理特征之间出现的协同性。处理特征数值大小之间的关系的是相关性分析。</p>
<h1 id="2-无监督学习和监督学习的区别"><a href="#2-无监督学习和监督学习的区别" class="headerlink" title="2. 无监督学习和监督学习的区别"></a>2. 无监督学习和监督学习的区别</h1><p>无监督学习是对整体进行操作分类，监督学习是在训练好后对个体的预测</p>
<h1 id="3-聚类"><a href="#3-聚类" class="headerlink" title="3.聚类"></a>3.聚类</h1><p>聚类的结果，高度依赖于你喂给它的特征。你想得到什么样的分类，就必须提供与之强相关的特征数据</p>
<p>相比于排序，聚类的优势是能够在多维分析</p>
<p>聚类不是为了“排名”，而是为了“画像”</p>
<h1 id="4-降维"><a href="#4-降维" class="headerlink" title="4.降维"></a>4.降维</h1><p>降维就是在尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</p>
<p>降维主要通过两种途径实现：特征选择和特征提取</p>
<p>算法的任务不是去“学习一个映射关系去预测某个标签”，而是必须在数据内部“自力更生”，仅通过分析数据点本身，去发现其中隐藏的、有意义的结构和模式。</p>
<ul>
<li>聚类如何体现无监督？ 它在拿到一堆客户数据时，不需要预先知道“哪个客户是高价值客户”或“哪个客户即将流失”。它完全不依赖任何已有标签，而是自主地、仅仅根据客户数据特征（如消费金额、频率）之间的相似性，去划分出不同的客户群体。</li>
<li>降维如何体现无监督？ 它同样不需要任何标签来指导。它在拿到描述汽车的上百个特征时，不需要知道“哪辆车是豪车”或“哪辆车性能好”。它只是自主地分析这些特征之间的内在关系（如相关性、方差分布），然后找到一种更简洁、更高效的方式来表达整个数据集，比如用几个“主成分”来概括。</li>
</ul>
<p>所以，它们最根本、最核心的相似点就是：</p>
<p>它们的目标都不是“预测”一个外部标签，而是“理解”数据自身的内在结构。</p>
<hr>
<h1 id="Gemini"><a href="#Gemini" class="headerlink" title="Gemini"></a>Gemini</h1><p>什么是无监督学习 </p>
<h2 id="什么是无监督学习？"><a href="#什么是无监督学习？" class="headerlink" title="什么是无监督学习？"></a>什么是无监督学习？</h2><p>无监督学习（Unsupervised Learning）是机器学习的一个核心分支，其主要特点是使用<strong>未经标记</strong>的数据进行训练。与监督学习需要“正确答案”来指导模型不同，无监督学习算法需要自行在数据中探索和发现隐藏的结构、模式和关系。</p>
<p>可以把它想象成给你一大箱混杂的乐高积木，但没有任何说明书。你的任务不是搭建一个特定的模型（比如一辆车或一座房子），而是自己动手把这些积木根据颜色、形状、大小等特征进行分类和整理。最终，你可能会把所有红色的方块放在一起，所有蓝色的长条放在另一堆，这个过程就是无监督学习。</p>
<h3 id="核心思想与目标"><a href="#核心思想与目标" class="headerlink" title="核心思想与目标"></a><strong>核心思想与目标</strong></h3><p>无监督学习的核心思想是<strong>让算法自主地从数据中学习</strong>，其主要目标包括：</p>
<ul>
<li><strong>发现数据的内在结构与模式：</strong> 识别数据点之间自然的聚集或分组，即“物以类聚”。</li>
<li><strong>数据降维：</strong> 在保留数据关键信息的同时，减少数据的复杂性，方便分析和可视化。</li>
<li><strong>异常检测：</strong> 找出数据集中与其他数据点显著不同的“异常值”或“离群点”。</li>
<li><strong>关联规则挖掘：</strong> 发现数据项之间有趣的关联关系，例如“购买了商品A的顾客也很可能购买商品B”。</li>
</ul>
<h3 id="主要类型与常见算法"><a href="#主要类型与常见算法" class="headerlink" title="主要类型与常见算法"></a><strong>主要类型与常见算法</strong></h3><p>无监督学习主要包含以下几种任务类型，每种类型都有其代表性的算法：</p>
<h4 id="1-聚类（Clustering）"><a href="#1-聚类（Clustering）" class="headerlink" title="1. 聚类（Clustering）"></a>1. <strong>聚类（Clustering）</strong></h4><p>这是最常见的无监督学习任务，目的是将数据集中的样本划分为若干个不相交的子集，即“簇”（Cluster）。同一簇内的数据点彼此相似，而不同簇的数据点则相异。</p>
<ul>
<li><strong>应用场景：</strong><ul>
<li><strong>客户细分：</strong> 根据用户的购买行为、浏览历史等将客户分成不同群体，以便进行精准营销。</li>
<li><strong>图像分割：</strong> 将图像中相似的像素点分组，以识别出不同的物体。</li>
<li><strong>生物信息学：</strong> 根据基因表达数据对样本进行分类。</li>
</ul>
</li>
<li><strong>常见算法：</strong><ul>
<li><strong>K-均值聚类（K-Means Clustering）：</strong> 一种简单高效的算法，通过迭代将数据分配到预设的K个簇中。</li>
<li><strong>层次聚类（Hierarchical Clustering）：</strong> 构建数据的层次结构，可以像树状图一样展示聚类过程。</li>
<li><strong>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）：</strong> 基于密度的聚类算法，能够发现任意形状的簇并识别噪声点。</li>
</ul>
</li>
</ul>
<h4 id="2-降维（Dimensionality-Reduction）"><a href="#2-降维（Dimensionality-Reduction）" class="headerlink" title="2. 降维（Dimensionality Reduction）"></a>2. <strong>降维（Dimensionality Reduction）</strong></h4><p>当数据具有非常多的特征（维度）时，会给分析和建模带来困难（即“维度灾难”）。降维技术旨在减少特征数量，同时尽可能保留原始数据中的重要信息。</p>
<ul>
<li><strong>应用场景：</strong><ul>
<li><strong>数据可视化：</strong> 将高维数据（如上百个特征）降低到2维或3维，以便在图表上直观展示。</li>
<li><strong>特征提取：</strong> 为监督学习算法准备更有效、更精简的特征，提高模型性能和训练速度。</li>
<li><strong>数据压缩：</strong> 减少存储空间和计算成本。</li>
</ul>
</li>
<li><strong>常见算法：</strong><ul>
<li><strong>主成分分析（Principal Component Analysis, PCA）：</strong> 找到数据中方差最大的方向，将数据投影到这些方向上，从而实现降维。</li>
<li><strong>t-分布随机邻域嵌入（t-SNE）：</strong> 一种非常擅长高维数据可视化的技术。</li>
</ul>
</li>
</ul>
<h4 id="3-关联规则挖掘（Association-Rule-Mining）"><a href="#3-关联规则挖掘（Association-Rule-Mining）" class="headerlink" title="3. 关联规则挖掘（Association Rule Mining）"></a>3. <strong>关联规则挖掘（Association Rule Mining）</strong></h4><p>用于在一系列交易或事件中发现项与项之间的关联关系。</p>
<ul>
<li><strong>应用场景：</strong><ul>
<li><strong>市场篮子分析：</strong> 零售商分析购物篮数据，发现哪些商品经常被一起购买，如“啤酒与尿布”的经典案例，从而优化商品摆放和促销策略。</li>
<li><strong>推荐系统：</strong> 根据用户的历史行为，推荐与其兴趣相关的其他内容。</li>
</ul>
</li>
<li><strong>常见算法：</strong><ul>
<li><strong>Apriori算法</strong></li>
<li><strong>FP-Growth算法</strong></li>
</ul>
</li>
</ul>
<h3 id="无监督学习-vs-监督学习"><a href="#无监督学习-vs-监督学习" class="headerlink" title="无监督学习 vs. 监督学习"></a><strong>无监督学习 vs. 监督学习</strong></h3><table>
<thead>
<tr>
<th>特征</th>
<th><strong>无监督学习 (Unsupervised Learning)</strong></th>
<th><strong>监督学习 (Supervised Learning)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>训练数据</strong></td>
<td>未标记数据 (Raw Data)</td>
<td>已标记数据 (Labeled Data)</td>
</tr>
<tr>
<td><strong>目标</strong></td>
<td>发现数据的内在结构、模式或分布</td>
<td>预测一个明确的目标或输出</td>
</tr>
<tr>
<td><strong>过程</strong></td>
<td>探索性的，算法自行发现规律</td>
<td>指导性的，算法从“正确答案”中学习</td>
</tr>
<tr>
<td><strong>常见任务</strong></td>
<td>聚类、降维、关联规则</td>
<td>分类、回归</td>
</tr>
<tr>
<td><strong>例子</strong></td>
<td>将客户自动分为不同群体</td>
<td>识别一封邮件是否是垃圾邮件</td>
</tr>
</tbody></table>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>总而言之，无监督学习是人工智能领域中一种强大的工具，它使我们能够在没有先验知识和人工标记的情况下，从海量数据中挖掘出有价值的洞见。随着数据量的爆炸式增长，无监督学习在商业智能、科学研究和数据探索等领域的应用正变得越来越重要。</p>
<p> 我是不是可以这样理解，无监督学习是对整体进行操作，监督学习是在训练好后对个体的预测 </p>
<p>您这个理解非常棒，可以说抓住了两者在<strong>目标和应用</strong>上的核心区别！这是一个非常形象和准确的直觉性总结。</p>
<p>我们可以把您的理解稍微展开一下，就更完美了：</p>
<h3 id="无监督学习：对“整体”进行探索和构建"><a href="#无监督学习：对“整体”进行探索和构建" class="headerlink" title="无监督学习：对“整体”进行探索和构建"></a><strong>无监督学习：对“整体”进行探索和构建</strong></h3><p>您说的“对整体进行操作”非常贴切。无监督学习的核心是<strong>探索</strong>。它在拿到一堆数据时，并不知道要干什么，就像是面对一个满是陌生人的房间。它的任务是在这个<strong>整体</strong>中寻找内在的结构和关系。</p>
<ul>
<li><strong>目标：</strong> 发现数据本身的结构、模式、或“抱团”的群体。</li>
<li><strong>操作：</strong><ul>
<li><strong>聚类 (Clustering):</strong> 把整个房间里的人根据亲疏远近分成不同的小圈子（群组）。</li>
<li><strong>降维 (Dimensionality Reduction):</strong> 忽略每个人的次要特征（比如鞋子颜色），只关注最重要的特征（比如性格、兴趣），来更宏观地理解整个房间的人际关系。</li>
</ul>
</li>
<li><strong>结果：</strong> 得到一个关于<strong>整体数据</strong>的结构性认知（比如，这批客户可以分为三个群体）。</li>
</ul>
<hr>
<h3 id="监督学习：对“个体”进行预测和判断"><a href="#监督学习：对“个体”进行预测和判断" class="headerlink" title="监督学习：对“个体”进行预测和判断"></a><strong>监督学习：对“个体”进行预测和判断</strong></h3><p>您说的“对个体进行预测”也非常准确。监督学习的核心是<strong>预测</strong>。它像一个经验丰富的老师，已经通过批改大量的作业（已标记的训练数据）学会了判断标准。</p>
<ul>
<li><strong>目标：</strong> 根据已有的经验，对一个<strong>新的、独立的个体</strong>做出判断。</li>
<li><strong>操作：</strong><ul>
<li><strong>训练阶段：</strong> 老师学习大量的“问题-答案”对（比如，房子的面积、位置 -&gt; 房价）。</li>
<li><strong>预测阶段：</strong> 当拿到一个<strong>新的个体</strong>（一套新房子）时，老师运用学到的知识，给出一个具体的预测（这套新房子的价格）。</li>
</ul>
</li>
<li><strong>结果：</strong> 对<strong>单个未知数据</strong>给出一个具体的预测标签或数值。</li>
</ul>
<h3 id="总结一下您的理解："><a href="#总结一下您的理解：" class="headerlink" title="总结一下您的理解："></a><strong>总结一下您的理解：</strong></h3><ul>
<li><strong>无监督学习</strong>更像是<strong>人口普查或社会学分析</strong>，着眼于<strong>整体</strong>，目的是了解人群的构成和分布。</li>
<li><strong>监督学习</strong>更像是<strong>高考阅卷或医生看病</strong>，着眼于<strong>个体</strong>，目的是对单个考生或病人给出分数或诊断。</li>
</ul>
<p>所以，您的理解完全正确，并且是区分这两种机器学习方法的一个非常好的切入点！</p>
<p> 详细解释一下无监督学习的聚类，我不需要输入任何信息就可以用算法自动去分类吗 </p>
<p>当然，我们来详细解释一下聚类（Clustering），并解答您关于“是否需要输入信息”的疑问。</p>
<p>您的理解非常接近核心了，但有一个关键点需要澄清：您<strong>确实需要提供数据</strong>，但<strong>不需要提供“答案”或“标签”</strong>。</p>
<p>这就像您想整理书架上的书，您得先把所有的书（<strong>数据</strong>）都放在算法面前，但您不需要告诉算法哪本书属于“小说类”，哪本书属于“历史类”（<strong>标签</strong>）。算法会自己通过分析书的厚度、封面颜色、开本大小等特征，把它们分成几堆。</p>
<hr>
<h3 id="什么是聚类？"><a href="#什么是聚类？" class="headerlink" title="什么是聚类？"></a><strong>什么是聚类？</strong></h3><p>聚类是无监督学习中最核心、最常见的任务之一。它的目标是将一个数据集中的所有样本（数据点）分成若干个组或“簇”（Cluster）。</p>
<p><strong>核心原则是：</strong></p>
<ul>
<li><strong>簇内相似性高 (Intra-cluster similarity):</strong> 同一个簇内的数据点彼此之间应该尽可能相似。</li>
<li><strong>簇间相似性低 (Inter-cluster similarity):</strong> 不同簇之间的数据点应该尽可能不相似。</li>
</ul>
<p>这个“相似性”是如何定义的呢？通常是通过计算数据点之间的<strong>距离</strong>来衡量的。在二维或三维空间中，我们可以很容易地理解为欧几里得距离（两点之间的直线距离）。在高维数据中，虽然无法可视化，但数学原理是相通的。距离越近，相似性就越高。</p>
<h3 id="聚类是如何“自动”分类的？——以K-Means算法为例"><a href="#聚类是如何“自动”分类的？——以K-Means算法为例" class="headerlink" title="聚类是如何“自动”分类的？——以K-Means算法为例"></a><strong>聚类是如何“自动”分类的？——以K-Means算法为例</strong></h3><p>为了让您更直观地理解，我们用最经典、最简单的聚类算法<strong>K-Means（K-均值）</strong>来举例说明其工作流程。</p>
<p>假设我们有一堆客户数据，每个客户有两个特征：<strong>“每月消费金额”和“每月消费次数”</strong>。我们想把这些客户分成3个不同的群体（比如“高价值客户”、“潜力客户”、“低价值客户”）。</p>
<p><strong>步骤如下：</strong></p>
<h4 id="第一步：您需要输入的信息"><a href="#第一步：您需要输入的信息" class="headerlink" title="第一步：您需要输入的信息"></a><strong>第一步：您需要输入的信息</strong></h4><ol>
<li><strong>数据 (Data):</strong> 您必须提供所有客户的消费数据点。这是算法分析的对象。</li>
<li><strong>簇的数量 K (Number of clusters):</strong> 您需要<strong>预先指定</strong>要把数据分成几类。在这个例子中，我们指定 <strong>K&#x3D;3</strong>。这是K-Means算法中一个需要人工设定的重要参数。</li>
</ol>
<p><strong>看，这里您输入了数据和K值，但完全没有输入任何关于“哪个客户属于哪个群体”的标签信息。</strong></p>
<h4 id="第二-三-四步：算法的“自动”工作流程"><a href="#第二-三-四步：算法的“自动”工作流程" class="headerlink" title="第二-三-四步：算法的“自动”工作流程"></a><strong>第二-三-四步：算法的“自动”工作流程</strong></h4><p><em>（这是一个K-Means算法动态过程的经典图示）</em></p>
<ol>
<li><strong>随机初始化 (Initialization):</strong> 算法会从您的数据点中随机挑选 K 个点（这里是3个点）作为初始的“簇中心”（Centroids）。可以把它们想象成三个群体的最初的“核心代表”。</li>
<li><strong>分配 (Assignment):</strong> 算法会遍历<strong>每一个</strong>客户数据点，计算它到这3个簇中心的距离。然后，<strong>将这个客户点分配给离它最近的那个簇中心</strong>。完成这一步后，所有的客户就被初步分成了3个群组。</li>
<li><strong>更新 (Update):</strong> 对于刚刚形成的每一个群组，算法会重新计算这个群组所有点的<strong>平均值</strong>（几何中心），并将这个新的平均值点作为<strong>新的簇中心</strong>。旧的簇中心就被抛弃了。</li>
<li><strong>迭代 (Iteration):</strong> 算法会<strong>重复执行第2步和第3步</strong>。<ul>
<li>重新将所有点分配给新的、离它最近的簇中心。</li>
<li>分配完毕后，再次更新每个群组的簇中心。</li>
<li>……如此循环往复。</li>
</ul>
</li>
</ol>
<h4 id="第五步：收敛与结果"><a href="#第五步：收敛与结果" class="headerlink" title="第五步：收敛与结果"></a><strong>第五步：收敛与结果</strong></h4><p>这个迭代过程什么时候会停止呢？当<strong>簇中心的位置不再发生变化</strong>（或者变化非常小），或者<strong>所有数据点的归属不再改变</strong>时，算法就<strong>收敛</strong>（Converged）了。</p>
<p>此时，聚类就完成了！您最终得到了3个稳定的客户群体，以及每个群体对应的中心点。您可以通过分析每个群体中心点的特征（比如A群消费金额高、次数也高；B群金额低、次数高……）来给这些群体命名和定义，从而获得商业洞见。</p>
<h3 id="总结您的疑问"><a href="#总结您的疑问" class="headerlink" title="总结您的疑问"></a><strong>总结您的疑问</strong></h3><p><strong>“我不需要输入任何信息就可以用算法自动去分类吗？”</strong></p>
<p><strong>回答：</strong></p>
<ul>
<li><strong>“不需要输入标签信息”：是的。</strong> 您完全不需要告诉算法任何关于“正确分类”的答案。这是无监督学习的核心。算法是<strong>自动</strong>发现分类结构的。</li>
<li><strong>“不需要输入任何信息”：不是的。</strong> 您至少需要提供：<ol>
<li><strong>待分析的完整数据集。</strong></li>
<li><strong>一些算法运行所需的超参数（Hyperparameters）。</strong> 对于K-Means来说，最重要的超参数就是您希望分成的簇数 <strong>K</strong>。对于其他聚类算法，可能需要设定别的参数，比如距离阈值等。</li>
</ol>
</li>
</ul>
<p>如何确定最佳的K值也是聚类分析中的一个重要课题（常用“肘部法则”等方法来辅助判断），但这已经属于更深入的细节了。</p>
<p>希望这个详细的解释能帮助您彻底理解聚类的概念和工作方式！</p>
<p> 这样如何保证聚类的标准就是我想要的标准呢 </p>
<p>这是一个非常非常好的问题，直接触及了无监督学习在实际应用中的核心挑战！</p>
<p>您问得没错，算法本身没有“常识”或“商业头脑”，它只懂数学。它优化的标准（比如“让簇内数据点距离尽可能近”）<strong>不一定</strong>就是您心中想要的那个“商业标准”（比如“想把客户分成‘高价值’‘潜力股’和‘需唤醒’三类”）。</p>
<p>那么如何保证或引导它们统一呢？答案是：<strong>您无法100%保证，但您可以通过一系列关键操作，最大限度地引导算法，使其聚类的结果符合您的标准。</strong></p>
<p>这更像是一个“人机协作”的探索过程，而不是一个“全自动”的过程。您可以从以下三个阶段进行把控：</p>
<hr>
<h3 id="第一阶段：前期准备-——-定义你的“标准”，并翻译给机器"><a href="#第一阶段：前期准备-——-定义你的“标准”，并翻译给机器" class="headerlink" title="第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器"></a><strong>第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器</strong></h3><p>这是最最重要的一步。机器不懂你的商业语言，你必须把你的“标准”翻译成它能听懂的数学语言——也就是<strong>特征（Features）</strong>。</p>
<h4 id="1-特征工程-Feature-Engineering"><a href="#1-特征工程-Feature-Engineering" class="headerlink" title="1. 特征工程 (Feature Engineering)"></a><strong>1. 特征工程 (Feature Engineering)</strong></h4><p>这是你的“指挥棒”。你想按照什么标准来分，就给算法提供什么维度的特征。</p>
<ul>
<li><strong>例子1：你想按“客户价值”聚类</strong><ul>
<li><strong>错误做法：</strong> 只给算法输入客户的年龄、性别、注册时长。算法可能会把所有“30岁男性”聚成一类，但这和“客户价值”关系不大。</li>
<li><strong>正确做法：</strong> 精心构造能体现客户价值的特征，比如：<ul>
<li><strong>R (Recency):</strong> 最近一次消费时间</li>
<li><strong>F (Frequency):</strong> 消费频率</li>
<li><strong>M (Monetary):</strong> 总消费金额</li>
</ul>
</li>
<li>当你把著名的 <strong>RFM 模型</strong>特征喂给算法时，它就会自然而然地把“R&#x2F;F&#x2F;M 都很高的客户”聚成一类，这就是你想要的“高价值客户”。</li>
</ul>
</li>
<li><strong>例子2：你想按“用户活跃度”聚类</strong><ul>
<li>你应该提供<strong>登录频率、页面停留时间、点赞&#x2F;评论数量</strong>等特征，而不是消费金额。</li>
</ul>
</li>
</ul>
<p><strong>核心思想：聚类的结果，高度依赖于你喂给它的特征。你想得到什么样的分类，就必须提供与之强相关的特征数据。</strong></p>
<h4 id="2-特征缩放-Feature-Scaling"><a href="#2-特征缩放-Feature-Scaling" class="headerlink" title="2. 特征缩放 (Feature Scaling)"></a><strong>2. 特征缩放 (Feature Scaling)</strong></h4><p>如果你的特征单位不统一（比如消费金额是几千上万，消费次数是几次），金额这个特征会不成比例地主导聚类结果。你需要将所有特征进行<strong>标准化&#x2F;归一化</strong>，让它们在同一起跑线上，确保每个特征都被公平地考虑。</p>
<hr>
<h3 id="第二阶段：算法选择与参数调整-——-选择合适的“尺子”"><a href="#第二阶段：算法选择与参数调整-——-选择合适的“尺子”" class="headerlink" title="第二阶段：算法选择与参数调整 —— 选择合适的“尺子”"></a><strong>第二阶段：算法选择与参数调整 —— 选择合适的“尺子”</strong></h3><p>不同的场景需要用不同的“尺子”去衡量。</p>
<h4 id="1-选择合适的聚类算法"><a href="#1-选择合适的聚类算法" class="headerlink" title="1. 选择合适的聚类算法"></a><strong>1. 选择合适的聚类算法</strong></h4><ul>
<li><strong>K-Means:</strong> 适合于你<strong>大致知道要分成几类</strong>，且每个类别的数据分布比较<strong>均衡、呈球状</strong>的场景。</li>
<li><strong>DBSCAN:</strong> 适合于<strong>不知道要分几类</strong>，且类别形状可能<strong>很不规则</strong>（比如月牙形、甜甜圈形）的场景，它还能帮你找出<strong>噪声点&#x2F;异常点</strong>。</li>
<li><strong>层次聚类 (Hierarchical Clustering):</strong> 适合于你希望看到数据<strong>层级关系</strong>的场景，比如生物学中的物种分类谱系。</li>
</ul>
<p>选择错误的算法，就像用直尺去量一个圆的周长，结果自然不准。</p>
<h4 id="2-调整关键参数-Hyperparameter-Tuning"><a href="#2-调整关键参数-Hyperparameter-Tuning" class="headerlink" title="2. 调整关键参数 (Hyperparameter Tuning)"></a><strong>2. 调整关键参数 (Hyperparameter Tuning)</strong></h4><p>对于K-Means，最重要的参数就是 <strong>K 值（簇的数量）</strong>。</p>
<ul>
<li>如果你想把客户分成“高、中、低”三档，你就设置 K&#x3D;3。</li>
<li>如果你想分得更细，比如“高价值-活跃”、“高价值-沉睡”、“潜力客户”、“一般客户”、“流失风险客户”，你可能就需要设置 K&#x3D;5。</li>
</ul>
<p>K值的选择可以依靠业务经验预设，也可以通过“肘部法则”等技术手段辅助判断，但最终还是需要结合业务来决定哪个K值划分出的群体最有意义。</p>
<hr>
<h3 id="第三阶段：后期验证与解读-——-人工的“最终验收”"><a href="#第三阶段：后期验证与解读-——-人工的“最终验收”" class="headerlink" title="第三阶段：后期验证与解读 —— 人工的“最终验收”"></a><strong>第三阶段：后期验证与解读 —— 人工的“最终验收”</strong></h3><p>算法给出结果后，绝不是直接就用，而是需要人来进行验证和解读。</p>
<h4 id="1-结果分析与画像-Profiling"><a href="#1-结果分析与画像-Profiling" class="headerlink" title="1. 结果分析与画像 (Profiling)"></a><strong>1. 结果分析与画像 (Profiling)</strong></h4><p>你需要分析每个簇的特征。比如，算法把客户分成了3类：</p>
<ul>
<li><strong>簇1：</strong> RFM 平均值都很高。</li>
<li><strong>簇2：</strong> R 值很低（很久没来），但 F&#x2F;M 值曾经很高。</li>
<li><strong>簇3：</strong> RFM 值都很低。</li>
</ul>
<p>这时你就可以给它们贴上你想要的业务标签：</p>
<ul>
<li><strong>簇1 -&gt; “高价值忠诚客户”</strong></li>
<li><strong>簇2 -&gt; “沉睡待唤醒客户”</strong></li>
<li><strong>簇3 -&gt; “低价值客户”</strong></li>
</ul>
<p>如果聚出来的结果无法解释，或者不符合业务逻辑（比如把明显是高价值的客户和流失客户分到了一起），那就说明你的第一或第二阶段出了问题，需要返回去调整特征或算法参数。</p>
<h4 id="2-业务验证"><a href="#2-业务验证" class="headerlink" title="2. 业务验证"></a><strong>2. 业务验证</strong></h4><p>最终的“金标准”是：<strong>这个聚类结果有用吗？</strong> 你能否针对“沉睡待唤醒客户”这个群体，设计一套精准的营销方案（比如发放大额优惠券）并取得比撒网式营销更好的效果？如果可以，那么这个聚类标准就是成功的、符合你需求的。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><table>
<thead>
<tr>
<th>阶段</th>
<th>你的角色</th>
<th>关键操作</th>
</tr>
</thead>
<tbody><tr>
<td><strong>前期准备</strong></td>
<td><strong>战略家</strong></td>
<td><strong>定义标准</strong>，通过<strong>特征工程</strong>将标准翻译给机器。</td>
</tr>
<tr>
<td><strong>算法&#x2F;参数</strong></td>
<td><strong>工程师</strong></td>
<td><strong>选择合适的工具和参数</strong> (K值等)。</td>
</tr>
<tr>
<td><strong>后期验证</strong></td>
<td><strong>分析师&#x2F;决策者</strong></td>
<td><strong>解读、验证、命名</strong>聚类结果，并判断其<strong>商业价值</strong>。</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p>所以，与其说是“保证”，不如说是通过<strong>精心的特征设计</strong>、<strong>恰当的算法选择</strong>和<strong>深入的结果解读</strong>，来<strong>引导和塑造</strong>聚类分析，使其最终服务于你的商业目标。</p>
<p> 如果我已经有了数据的一些特征，那直接按从高到低排序不就好了，为什么还需要用无监督来聚类 </p>
<p>您又提出了一个非常深刻且实际的问题！这正好点出了<strong>一维分析</strong>和<strong>多维分析</strong>的根本区别。</p>
<p>您的想法完全正确，如果只关心<strong>单一指标</strong>，直接排序就是最简单、最有效的办法。但现实世界的问题往往是复杂的、多维度的。</p>
<p>我们还是用一个生动的例子来说明：假设您是一位班主任，手上有所有学生的成绩单。</p>
<h3 id="场景一：按单科成绩排序（您的方法）"><a href="#场景一：按单科成绩排序（您的方法）" class="headerlink" title="场景一：按单科成绩排序（您的方法）"></a><strong>场景一：按单科成绩排序（您的方法）</strong></h3><p>您想评选“数学单科状元”。</p>
<ul>
<li><strong>操作：</strong> 把所有学生的“数学”成绩从高到低排序。</li>
<li><strong>结果：</strong> 排名第一的就是状元。</li>
<li><strong>优点：</strong> 简单、直接、清晰。对于<strong>“谁的数学最好”</strong>这个问题，这是完美的解决方案。</li>
</ul>
<h3 id="场景二：寻找不同类型的学生群体（无监督聚类）"><a href="#场景二：寻找不同类型的学生群体（无监督聚类）" class="headerlink" title="场景二：寻找不同类型的学生群体（无监督聚类）"></a><strong>场景二：寻找不同类型的学生群体（无监督聚类）</strong></h3><p>现在，您的目标变了。您不想只看单科，而是想了解班级里存在<strong>哪些典型的学生群体</strong>，以便因材施教。您手上的特征有三门：<strong>数学、语文、体育</strong>。</p>
<p>如果只按数学排序，您会忽略一个重要的问题：</p>
<ul>
<li>一个数学<code>99</code>分、语文<code>60</code>分、体育<code>50</code>分的孩子。</li>
<li>一个数学<code>95</code>分、语文<code>95</code>分、体育<code>95</code>分的孩子。</li>
</ul>
<p>在“数学”这个维度上，前者排名更高。但您凭直觉就知道，后者是完全不同类型的“学霸”。<strong>只靠排序，无法发现这种“类型”差异。</strong></p>
<p>现在我们使用<strong>聚类</strong>，把“数学”、“语文”、“体育”三个特征（三个维度）同时交给算法。算法可能会自动帮您发现以下几个群体（簇）：</p>
<ul>
<li><strong>群体A：全面发展型学霸</strong><ul>
<li><strong>特征：</strong> 数学、语文、体育三科成绩都非常高。</li>
<li><strong>排序无法发现：</strong> 如果单按数学排，他们可能排不过那些偏科天才。</li>
</ul>
</li>
<li><strong>群体B：文理偏科型学才</strong><ul>
<li><strong>特征：</strong> 数学很高，但体育和语文成绩可能很一般。</li>
<li><strong>排序能发现：</strong> 他们会在数学大榜上名列前茅。</li>
</ul>
</li>
<li><strong>群体C：运动健将型</strong><ul>
<li><strong>特征：</strong> 体育成绩突出，但文化课成绩可能不高。</li>
<li><strong>排序无法发现：</strong> 在任何文化课的排序中，他们都会被埋没在末尾。</li>
</ul>
</li>
<li><strong>群体D：成绩平均型</strong><ul>
<li><strong>特征：</strong> 各科成绩都处于中游，不突出也不落后。</li>
</ul>
</li>
</ul>
<p><strong>看到了吗？聚类不是为了“排名”，而是为了“画像”（Profiling）。</strong>它在多维空间中寻找“物以类聚”的群体，发现数据的内在结构。</p>
<hr>
<h3 id="核心区别总结"><a href="#核心区别总结" class="headerlink" title="核心区别总结"></a><strong>核心区别总结</strong></h3><table>
<thead>
<tr>
<th>特性</th>
<th><strong>按特征排序 (Sorting)</strong></th>
<th><strong>无监督聚类 (Clustering)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>处理维度</strong></td>
<td><strong>一维</strong>：一次只能根据一个特征进行排序。</td>
<td><strong>多维</strong>：同时考虑所有输入特征，发现综合模式。</td>
</tr>
<tr>
<td><strong>核心问题</strong></td>
<td>回答 <strong>“谁最高&#x2F;最低？”</strong></td>
<td>回答 <strong>“存在哪些不同的群体？”</strong></td>
</tr>
<tr>
<td><strong>产出结果</strong></td>
<td>一个**有序的列表 (Ordered List)**。</td>
<td>若干个**无序的群组 (Unordered Groups)**。</td>
</tr>
<tr>
<td><strong>揭示的洞察</strong></td>
<td><strong>线性的、显著的</strong>洞察。</td>
<td><strong>结构性的、隐藏的</strong>洞察，描绘用户画像。</td>
</tr>
<tr>
<td><strong>商业应用举例</strong></td>
<td>找到消费金额最高的Top 100客户。</td>
<td>将客户细分为“忠诚客户”、“潜力新客”、“流失风险客户”等群体。</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<h3 id="结论：我应该用哪个？"><a href="#结论：我应该用哪个？" class="headerlink" title="结论：我应该用哪个？"></a><strong>结论：我应该用哪个？</strong></h3><p>它们不是替代关系，而是<strong>互补关系</strong>，服务于不同的分析目标。</p>
<ul>
<li>当你的问题是<strong>“我想找到在『某个单一标准』下表现最好&#x2F;最差的个体”</strong>时，请使用<strong>排序</strong>。<ul>
<li><em>“找出上个月销量最高的商品。”</em></li>
<li><em>“找出互动最少的粉丝。”</em></li>
</ul>
</li>
<li>当你的问题是<strong>“我想了解我的数据里都包含哪些『有共性的群体』，以便对他们进行差异化对待”</strong>时，请使用<strong>聚类</strong>。<ul>
<li><em>“我想看看我的用户可以被分成哪几类，以便进行精准营销。”</em></li>
<li><em>“我想分析一下这些故障报告，看看是否存在几种典型的故障模式。”</em></li>
</ul>
</li>
</ul>
<p>所以，不是有了特征就只用排序，而是要看您想从数据中挖掘出什么样的故事。如果您想看的是一个英雄榜，就用排序；如果您想看的是一幅《清明上河图》，了解市井百态，那就需要聚类。</p>
<p> 什么是无监督学习的降维 </p>
<p>好的，我们来详细解释一下无监督学习中的另一个重要分支——<strong>降维（Dimensionality Reduction）</strong>。</p>
<p>简单来说，降维就是在<strong>尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</strong>。</p>
<p>想象一下，你想描述一辆汽车。你可能会列出几十上百个特征：</p>
<ul>
<li>长度、宽度、高度、轴距、轮距、离地间隙… (尺寸相关)</li>
<li>发动机排量、马力、扭矩、0-100km&#x2F;h加速时间… (性能相关)</li>
<li>油箱容积、百公里油耗、综合油耗… (能耗相关)</li>
<li>座椅材质、音响品牌、屏幕尺寸… (配置相关)</li>
<li>…等等</li>
</ul>
<p>这么多特征（维度）对于计算机来说是“烫手山芋”，不仅计算量巨大，而且很多信息是冗余的。比如，汽车的“长度”和“轴距”通常是高度相关的，知道了其中一个，另一个也能猜个大概。</p>
<p>降维技术就像一位<strong>高明的总结者</strong>，它能抓住这些特征背后的核心信息，用几个新的、综合性的特征来替代原来那一堆复杂的特征。</p>
<hr>
<h3 id="我们为什么需要降维？"><a href="#我们为什么需要降维？" class="headerlink" title="我们为什么需要降维？"></a><strong>我们为什么需要降维？</strong></h3><p>处理高维度数据会遇到很多麻烦，统称为**“维度灾难” (Curse of Dimensionality)**。降维主要就是为了解决这些问题：</p>
<ol>
<li><strong>提升算法性能和效率：</strong> 特征越少，模型训练和数据计算的速度就越快，占用的内存也越少。</li>
<li><strong>数据可视化 (Data Visualization)：</strong> 人的眼睛只能理解二维或三维空间。降维可以将上百维的数据降低到2D或3D，以便我们能以图表的形式直观地观察数据分布、结构和聚类情况。</li>
<li><strong>消除噪声和冗余：</strong> 原始数据中可能包含不相关的“噪声”特征或高度相关的“冗余”特征。降维可以剔除这些干扰，保留最核心的“信号”，有助于提升模型的准确性。</li>
<li><strong>简化问题：</strong> 更少的特征意味着模型更简单，更容易解释和理解。</li>
</ol>
<hr>
<h3 id="降维是如何实现的？——-一个直观的类比"><a href="#降维是如何实现的？——-一个直观的类比" class="headerlink" title="降维是如何实现的？—— 一个直观的类比"></a><strong>降维是如何实现的？—— 一个直观的类比</strong></h3><p>想象一下你在户外，阳光下有一个立体的、复杂的三维物体（比如一个手办模型）。</p>
<ul>
<li><strong>原始高维数据：</strong> 就是这个三维手办本身，它有长、宽、高三个维度，包含了所有细节。</li>
<li><strong>降维过程：</strong> 太阳光照射这个手办，在地面上投下一个**影子 (Shadow)**。</li>
<li><strong>降维后的数据：</strong> 地面上的这个二维影子。</li>
</ul>
<p>这个过程发生了什么？</p>
<ul>
<li><strong>维度降低了：</strong> 从三维（物体）降到了二维（影子）。</li>
<li><strong>信息有损失，但保留了核心：</strong> 影子的信息肯定不如实体完整（比如你看不见颜色和材质），但它依然保留了物体最主要的<strong>轮廓和形状</strong>信息。</li>
<li><strong>投影角度很重要：</strong> 如果太阳从正上方照射，影子可能只是一个小点，信息损失就太大了。你需要找到一个<strong>最佳的投影角度</strong>，让影子能最大程度地反映物体的原始形状。</li>
</ul>
<p>降维算法的核心任务，就是去寻找这个<strong>“最佳的投影角度”</strong>，以确保信息损失最小。</p>
<hr>
<h3 id="两种主要的降维方法"><a href="#两种主要的降维方法" class="headerlink" title="两种主要的降维方法"></a><strong>两种主要的降维方法</strong></h3><p>降维主要通过两种途径实现：</p>
<h4 id="1-特征选择-Feature-Selection"><a href="#1-特征选择-Feature-Selection" class="headerlink" title="1. 特征选择 (Feature Selection)"></a><strong>1. 特征选择 (Feature Selection)</strong></h4><p>这种方法比较“直接”，就是从原始的一堆特征中，直接<strong>挑选出</strong>一部分最重要的特征，把剩下的扔掉。</p>
<ul>
<li><strong>优点：</strong> 实现简单，保留了原始特征，可解释性强（你清楚地知道留下了哪些特征）。</li>
<li><strong>缺点：</strong> 被丢弃的特征里可能也包含部分有用信息，造成信息浪费。</li>
<li><strong>例子：</strong> 比如在分析房价时，有100个特征，通过分析发现“房屋面积”、“地段”、“学区”这三个特征最重要，于是就只用这三个特征来建模。</li>
</ul>
<h4 id="2-特征提取-Feature-Extraction"><a href="#2-特征提取-Feature-Extraction" class="headerlink" title="2. 特征提取 (Feature Extraction)"></a><strong>2. 特征提取 (Feature Extraction)</strong></h4><p>这种方法更“高明”，它不是简单地挑选，而是将原始特征进行<strong>融合与重组</strong>，创造出全新的、数量更少的特征来代表原始数据。我们前面提到的“影子”类比就属于特征提取。</p>
<ul>
<li><strong>优点：</strong> 能够综合所有原始特征的信息，信息保留更全面。</li>
<li><strong>缺点：</strong> 新生成的特征是原始特征的线性组合，失去了原有的物理含义，可解释性变差。</li>
</ul>
<p><strong>代表算法：主成分分析 (Principal Component Analysis, PCA)</strong></p>
<p>PCA 是最经典、最常用的特征提取降维算法。它的核心思想可以通俗地理解为：</p>
<ol>
<li><strong>寻找数据变化最大的方向：</strong> 在所有数据点中，找到一个方向（可以想象成一条轴），当所有数据点都投影到这个轴上时，投影点的分布最为分散（方差最大）。这个方向就包含了最多的原始信息，被称为**第一主成分 (PC1)**。</li>
<li><strong>寻找第二个方向：</strong> 接着，在与第一个方向<strong>正交（垂直）</strong> 的平面中，寻找一个能让数据次级分散的方向。这就是**第二主成分 (PC2)**。</li>
<li><strong>以此类推：</strong> 不断寻找相互正交且方差次大的方向，直到找到所有主成分。</li>
</ol>
<p>最后，你会得到一组全新的特征（PC1, PC2, PC3, …）。这些新特征是按其包含信息量的大小来排序的。你只需要选择前几个（比如前2个或前3个）最重要的主成分，就能代表原始数据绝大部分的信息，从而实现降维。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><table>
<thead>
<tr>
<th>特性</th>
<th><strong>聚类 (Clustering)</strong></th>
<th><strong>降维 (Dimensionality Reduction)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>目标</strong></td>
<td><strong>发现数据的群体结构</strong></td>
<td><strong>简化数据的表示方式</strong></td>
</tr>
<tr>
<td><strong>操作对象</strong></td>
<td>数据的**样本 (行)**，将行分组</td>
<td>数据的**特征 (列)**，减少列的数量</td>
</tr>
<tr>
<td><strong>核心问题</strong></td>
<td>“这些数据可以分成哪几类？”</td>
<td>“如何用更少的特征来描述这些数据？”</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p>总而言之，降维是处理现代海量复杂数据时一个不可或缺的预处理步骤，它像一个高效的过滤器和压缩器，让后续的数据分析和机器学习任务变得更加轻松和高效。</p>
<p> 降维和聚类有什么相似点，为什么都叫无监督学习 </p>
<p>您这个问题非常精彩，因为它触及了“无监督学习”这个概念的本质！降维和聚类看起来在做完全不同的事情，但它们的“哲学思想”是高度一致的，这也是它们都被归为无监督学习的原因。</p>
<p>我们来一步步拆解。</p>
<h3 id="核心原因：为什么它们都叫无监督学习？"><a href="#核心原因：为什么它们都叫无监督学习？" class="headerlink" title="核心原因：为什么它们都叫无监督学习？"></a><strong>核心原因：为什么它们都叫无监督学习？</strong></h3><p>这个问题的答案是理解它们相似点的关键。</p>
<p><strong>无监督学习的核心标志是：我们只向算法提供输入数据（特征 <code>X</code>），而不提供任何“正确答案”（标签 <code>y</code>）。</strong></p>
<p>算法的任务不是去“学习一个映射关系去预测某个标签”，而是必须<strong>在数据内部“自力更生”</strong>，仅通过分析数据点本身，去发现其中隐藏的、有意义的结构和模式。</p>
<ul>
<li><strong>聚类如何体现无监督？</strong> 它在拿到一堆客户数据时，不需要预先知道“哪个客户是高价值客户”或“哪个客户即将流失”。它完全不依赖任何已有标签，而是自主地、仅仅根据客户数据特征（如消费金额、频率）之间的相似性，去划分出不同的客户群体。</li>
<li><strong>降维如何体现无监督？</strong> 它同样不需要任何标签来指导。它在拿到描述汽车的上百个特征时，不需要知道“哪辆车是豪车”或“哪辆车性能好”。它只是自主地分析这些特征之间的内在关系（如相关性、方差分布），然后找到一种更简洁、更高效的方式来表达整个数据集，比如用几个“主成分”来概括。</li>
</ul>
<p><strong>所以，它们最根本、最核心的相似点就是：</strong></p>
<p><strong>它们的目标都不是“预测”一个外部标签，而是“理解”数据自身的内在结构。</strong></p>
<hr>
<h3 id="降维与聚类的其他相似点"><a href="#降维与聚类的其他相似点" class="headerlink" title="降维与聚类的其他相似点"></a><strong>降维与聚类的其他相似点</strong></h3><p>基于上述核心，我们可以引申出它们其他几个共通之处：</p>
<ol>
<li><strong>都是探索性数据分析（EDA）的利器：</strong> 在正式建模之前，我们通常对数据知之甚少。聚类可以帮我们了解数据中存在哪些天然的分组，而降维（尤其是为了可视化）可以帮我们直观地看到数据的分布形态。它们都是我们“认识”数据的重要手段。</li>
<li><strong>都依赖于距离或方差的度量：</strong> 两种算法的底层逻辑都与“数据点在空间中的分布”有关。<ul>
<li><strong>聚类</strong>通常依赖于<strong>样本与样本之间</strong>的距离（比如K-Means），距离近的被归为一类。</li>
<li><strong>降维</strong>通常依赖于<strong>特征与特征之间</strong>的关系和数据在不同方向上的方差（比如PCA），方差大的方向被认为是更重要的。</li>
</ul>
</li>
<li><strong>常被用作监督学习的预处理步骤：</strong><ul>
<li><strong>聚类</strong>的结果（每个样本属于哪个簇）可以作为一个新的特征，喂给后续的监督学习模型。</li>
<li><strong>降维</strong>后的数据可以作为监督学习模型的输入，以提高训练速度和避免维度灾难。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="用一个比喻来理解它们的异同"><a href="#用一个比喻来理解它们的异同" class="headerlink" title="用一个比喻来理解它们的异同"></a><strong>用一个比喻来理解它们的异同</strong></h3><p>想象你是一位图书管理员，面对一整个房间杂乱无章的书籍（原始数据集）。</p>
<ul>
<li><strong>聚类（Clustering）在做什么？</strong> 你在<strong>整理这些书（样本&#x2F;行）</strong>。你根据书的内容、类型（小说、历史、科学），把它们<strong>分成了几大堆</strong>。整理完后，书的总数没变，但它们被分门别类地放在了不同的书架上。<ul>
<li><strong>操作对象：</strong> 书本（数据样本）。</li>
<li><strong>目标：</strong> 将样本分组。</li>
</ul>
</li>
<li><strong>降维（Dimensionality Reduction）在做什么？</strong> 你在<strong>简化描述每本书的方式（特征&#x2F;列）</strong>。原来每本书都有一张卡片，上面记录了“页数”、“重量”、“长度”、“宽度”、“作者国籍”、“出版年份”等等几十项信息。你发现很多信息是冗余的，于是你设计了一套新的、更简洁的描述体系，比如只用三个新指标：“内容容量”、“文学价值”、“物理尺寸”。<ul>
<li><strong>操作对象：</strong> 描述书本的卡片信息（数据特征）。</li>
<li><strong>目标：</strong> 简化特征。</li>
</ul>
</li>
</ul>
<p><strong>相似点在哪里？</strong> 在这整个过程中，<strong>没有任何人提前告诉你“哪本书应该属于哪一类”或者“你应该如何总结这些书的信息”</strong>。所有的整理和总结工作，都是你通过观察书籍本身的属性和它们之间的关系来完成的。<strong>这就是“无监督”的精髓。</strong></p>
<h3 id="总结：一张图看懂关系"><a href="#总结：一张图看懂关系" class="headerlink" title="总结：一张图看懂关系"></a><strong>总结：一张图看懂关系</strong></h3><table>
<thead>
<tr>
<th></th>
<th><strong>聚类 (Clustering)</strong></th>
<th><strong>降维 (Dimensionality Reduction)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心思想</strong></td>
<td><strong>物以类聚</strong></td>
<td><strong>化繁为简</strong></td>
</tr>
<tr>
<td><strong>操作对象</strong></td>
<td><strong>数据样本 (行)</strong></td>
<td><strong>数据特征 (列)</strong></td>
</tr>
<tr>
<td><strong>主要目标</strong></td>
<td>将数据集<strong>划分</strong>成不同群组</td>
<td><strong>压缩</strong>数据集的特征数量</td>
</tr>
<tr>
<td><strong>回答的问题</strong></td>
<td>“我的数据可以分成哪几类？”</td>
<td>“如何用更少的信息来代表我的数据？”</td>
</tr>
<tr>
<td><strong>共同的哲学</strong></td>
<td><strong>都在无标签的情况下，探索和揭示数据自身的内在结构</strong></td>
<td></td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p> 什么是关联规则挖掘 </p>
<p>好的，我们来详细解释无监督学习中另一项非常有趣且实用的技术——<strong>关联规则挖掘（Association Rule Mining）</strong>。</p>
<p>简单来说，关联规则挖掘就是<strong>在一个数据集中发现项与项之间有趣的关联或相关关系</strong>。它最经典的应用场景就是购物篮分析（Market Basket Analysis），目的是找出“哪些商品经常被顾客一起购买”。</p>
<h3 id="核心思想与经典故事：“啤酒与尿布”"><a href="#核心思想与经典故事：“啤酒与尿布”" class="headerlink" title="核心思想与经典故事：“啤酒与尿布”"></a><strong>核心思想与经典故事：“啤酒与尿布”</strong></h3><p>关联规则挖掘最著名的案例就是沃尔玛的“啤酒与尿布”的故事。</p>
<blockquote>
<p>传说在20世纪90年代，沃尔玛的数据分析师发现，周五晚上，很多年轻的父亲在给孩子买尿布的同时，会顺便给自己买几瓶啤酒。</p>
<p><strong>关联规则：</strong> 如果一个顾客购买了尿布，那么他有很大概率也会购买啤酒。 <strong>规则形式：</strong> <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code></p>
</blockquote>
<p>发现这个看似毫不相关的商品组合后，沃尔玛将啤酒货架和尿布货架放在一起，结果大大提升了这两种商品的销量。</p>
<p>这个故事完美地诠释了关联规则挖掘的核心价值：<strong>从事物看似无关的表象下，挖掘出隐藏的、有商业价值的强关联模式。</strong></p>
<hr>
<h3 id="关联规则的关键概念"><a href="#关联规则的关键概念" class="headerlink" title="关联规则的关键概念"></a><strong>关联规则的关键概念</strong></h3><p>为了衡量一个规则是否有价值，我们需要几个关键的指标来“量化”它。我们以 <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 这个规则为例：</p>
<h4 id="1-项集-Itemset"><a href="#1-项集-Itemset" class="headerlink" title="1. 项集 (Itemset)"></a>1. <strong>项集 (Itemset)</strong></h4><p>一个或多个项（商品）的集合。</p>
<ul>
<li><code>&#123;尿布&#125;</code> 是一个1项集。</li>
<li><code>&#123;尿布, 啤酒&#125;</code> 是一个2项集。</li>
</ul>
<h4 id="2-支持度-Support"><a href="#2-支持度-Support" class="headerlink" title="2. 支持度 (Support)"></a>2. <strong>支持度 (Support)</strong></h4><p>这是衡量一个项集在所有交易中<strong>“普遍性”</strong>或<strong>“受欢迎程度”</strong>的指标。</p>
<ul>
<li><strong>公式：</strong> <code>支持度(&#123;X&#125;) = (包含项集X的交易数量) / (总交易数量)</code></li>
<li><strong>例子：</strong> 如果在1000笔交易中，有100笔交易同时包含了尿布和啤酒，那么项集<code>&#123;尿布, 啤酒&#125;</code>的支持度就是 <code>100 / 1000 = 10%</code>。</li>
<li><strong>作用：</strong> 支持度太低的项集通常意义不大，因为它们出现的频率太低，可能是偶然事件。我们会设定一个<strong>最小支持度阈值</strong>，来过滤掉这些无意义的组合。</li>
</ul>
<h4 id="3-置信度-Confidence"><a href="#3-置信度-Confidence" class="headerlink" title="3. 置信度 (Confidence)"></a>3. <strong>置信度 (Confidence)</strong></h4><p>这是衡量规则<strong>“准确性”</strong>的指标。它表示当A出现时，B有多大概率也会出现。</p>
<ul>
<li><strong>公式：</strong> <code>置信度(&#123;A&#125; -&gt; &#123;B&#125;) = (包含A和B的交易数量) / (包含A的交易数量)</code></li>
<li><strong>例子：</strong> 在所有购买了尿布的200笔交易中，有100笔也购买了啤酒。那么规则 <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 的置信度就是 <code>100 / 200 = 50%</code>。这意味着，购买尿布的顾客中，有50%的人会同时购买啤酒。</li>
<li><strong>作用：</strong> 置信度衡量了关联的强度。我们会设定一个<strong>最小置信度阈值</strong>来确保规则的可靠性。</li>
</ul>
<h4 id="4-提升度-Lift"><a href="#4-提升度-Lift" class="headerlink" title="4. 提升度 (Lift)"></a>4. <strong>提升度 (Lift)</strong></h4><p>这是衡量规则<strong>“有趣性”</strong>或<strong>“相关性”</strong>的指标。它告诉我们，项集A的出现，对于项集B的出现概率是提升了还是降低了。</p>
<ul>
<li><strong>公式：</strong> <code>提升度(&#123;A&#125; -&gt; &#123;B&#125;) = 置信度(&#123;A&#125; -&gt; &#123;B&#125;) / 支持度(&#123;B&#125;)</code></li>
<li><strong>解读：</strong><ul>
<li><strong>Lift &gt; 1：</strong> A的存在对B的出现有<strong>积极促进</strong>作用（正相关）。<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code>的提升度大于1，说明买尿布这件事确实能提升买啤酒的概率。</li>
<li><strong>Lift &#x3D; 1：</strong> A和B<strong>相互独立</strong>，没有关联。</li>
<li>**Lift &lt; 1：** A的存在对B的出现有**抑制**作用（负相关）。比如<code>&#123;牛奶&#125; -&gt; &#123;可乐&#125;</code>的提升度可能小于1，说明买牛奶的人通常不怎么买可乐。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="关联规则挖掘是如何工作的？——-Apriori算法简介"><a href="#关联规则挖掘是如何工作的？——-Apriori算法简介" class="headerlink" title="关联规则挖掘是如何工作的？—— Apriori算法简介"></a><strong>关联规则挖掘是如何工作的？—— Apriori算法简介</strong></h3><p>挖掘关联规则最经典的算法是 <strong>Apriori 算法</strong>。它的核心思想分为两步：</p>
<ol>
<li><strong>找出所有的频繁项集 (Frequent Itemsets)：</strong><ul>
<li>首先，设定一个最小支持度阈值。</li>
<li>算法会先找出所有出现频率超过该阈值的<strong>单个商品</strong>（频繁1项集）。</li>
<li>然后，将这些频繁的单个商品两两组合，去计算<strong>2项集</strong>的支持度，再次筛选出频繁2项集。</li>
<li>…以此类推，不断组合、筛选，直到找不到更长的频繁项集为止。</li>
<li><strong>Apriori 原理：</strong> 如果一个项集是频繁的，那么它的所有子集也一定是频繁的。反之，如果一个项集的子集是非频繁的，那么这个项集本身也一定是非频繁的（可以被提前剪枝，提高效率）。</li>
</ul>
</li>
<li><strong>从频繁项集中生成强关联规则：</strong><ul>
<li>在上一步找到的所有频繁项集（比如<code>&#123;尿布, 啤酒&#125;</code>）中，我们开始生成规则。</li>
<li>对于每个频繁项集，我们尝试所有可能的规则划分（如<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 和 <code>&#123;啤酒&#125; -&gt; &#123;尿布&#125;</code>）。</li>
<li>然后计算这些规则的<strong>置信度</strong>，并筛选出那些满足最小置信度阈值的规则，这些就是最终的“强关联规则”。</li>
</ul>
</li>
</ol>
<h3 id="关联规则挖掘的应用场景"><a href="#关联规则挖掘的应用场景" class="headerlink" title="关联规则挖掘的应用场景"></a><strong>关联规则挖掘的应用场景</strong></h3><p>虽然起源于零售业，但关联规则的应用远不止于此：</p>
<ul>
<li><strong>电商推荐：</strong> “购买了这本书的读者也购买了…”、“看了这部电影的用户也喜欢…”。</li>
<li><strong>网页浏览分析：</strong> 分析用户访问路径，发现“访问了页面A的用户，下一步最可能访问页面C”，从而优化网站导航。</li>
<li><strong>医疗诊断：</strong> 发现不同症状之间的关联，比如“出现症状X和症状Y的病人，很大概率患有疾病Z”。</li>
<li><strong>金融风控：</strong> 发现不同交易行为之间的模式，用于检测欺诈。</li>
<li><strong>自然语言处理：</strong> 分析文本中词语的共现关系。</li>
</ul>
<p>总而言之，关联规则挖掘是一种强大的无监督学习技术，它能帮助我们从海量数据中发现看似无关却非常有价值的连接，为决策提供有力支持。</p>
<p> 关联规则挖掘和降维有什么异同，我挖掘了一个关联规则不就相当于提取到了一个特征来降维吗 </p>
<p>您这个问题非常有启发性！它触及了两种技术在“提取信息”这个层面上的相似感。您的直觉是对的——它们都在从数据中提炼知识，但它们提炼出的“知识”类型和用途，以及它们的操作方式，是截然不同的。</p>
<p>让我们先直接回答您的核心问题：</p>
<p><strong>挖掘一个关联规则 *不是* 提取一个特征来降维。</strong></p>
<p>这是一个非常普遍的混淆点，下面我们来详细拆解它们的异同。</p>
<hr>
<h3 id="核心区别：目标与产出完全不同"><a href="#核心区别：目标与产出完全不同" class="headerlink" title="核心区别：目标与产出完全不同"></a><strong>核心区别：目标与产出完全不同</strong></h3><p>这是两者最根本的区别。</p>
<ul>
<li><strong>关联规则挖掘 (ARM) 的目标是“发现模式”。</strong><ul>
<li>它想回答的问题是：“数据中存在哪些有趣的‘如果…那么…’(If-Then)关系？”</li>
<li>它的<strong>产出</strong>是一系列<strong>规则</strong>，这些规则是关于数据<strong>内容</strong>的<strong>洞察和知识</strong>。例如，<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code>。</li>
<li><strong>它不改变原始数据集的结构（维度）。</strong> 挖掘结束后，你的交易数据还是那么多行、那么多列，你只是额外得到了一份“知识报告”。</li>
</ul>
</li>
<li><strong>降维 (Dimensionality Reduction) 的目标是“简化表达”。</strong><ul>
<li>它想回答的问题是：“我能否用更少的信息（特征）来表示我的原始数据集，同时又不会丢失太多关键信息？”</li>
<li>它的<strong>产出</strong>是一个<strong>全新的、维度更低的数据集</strong>。原来的特征列会被新的、更少的特征列所取代。</li>
<li><strong>它直接改变了数据集的结构（维度）。</strong> 这是一个数据转换和压缩的过程。</li>
</ul>
</li>
</ul>
<p><strong>用一个比喻来理解：</strong></p>
<p>假设你的数据是一家图书馆的所有藏书记录。</p>
<ul>
<li><strong>关联规则挖掘</strong> 就像一位图书管理员通过分析借阅记录，发现了一个规律并写在工作日志里：“<strong>发现借《三体》的人，也很可能借《流浪地球》</strong>。” 这条记录是一个<strong>洞察（规则）</strong>，但图书馆里的书和书的描述卡片本身没有发生任何变化。</li>
<li><strong>降维</strong> 就像这位图书管理员觉得每本书的描述卡片有50个字段（特征）太啰嗦了，于是他<strong>创造了一套新的、只有3个字段的描述体系</strong>：<code>[科幻指数, 文学价值, 阅读难度]</code>。然后，他为<strong>每一本书</strong>都重新计算了这三个新指标的值，并用这张新的3字段卡片<strong>替换</strong>了原来那张50字段的旧卡片。整个图书馆的目录（数据集）变得更简洁了。</li>
</ul>
<hr>
<h3 id="关联规则挖掘-vs-降维：详细异同对比"><a href="#关联规则挖掘-vs-降维：详细异同对比" class="headerlink" title="关联规则挖掘 vs. 降维：详细异同对比"></a><strong>关联规则挖掘 vs. 降维：详细异同对比</strong></h3><table>
<thead>
<tr>
<th>特性</th>
<th><strong>关联规则挖掘 (ARM)</strong></th>
<th><strong>降维 (Dimensionality Reduction)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>根本目标</strong></td>
<td><strong>发现项与项之间的关联模式</strong></td>
<td><strong>压缩特征空间，简化数据表示</strong></td>
</tr>
<tr>
<td><strong>操作对象</strong></td>
<td>主要是<strong>数据的值</strong>（比如商品名称）</td>
<td>主要是<strong>数据的特征&#x2F;列</strong>本身</td>
</tr>
<tr>
<td><strong>输出结果</strong></td>
<td>一系列<strong>“If-Then”规则</strong>（知识、洞察）</td>
<td>一个<strong>维度更低的新数据集</strong>（数据的另一种表达）</td>
</tr>
<tr>
<td><strong>是否改变原数据</strong></td>
<td><strong>不改变</strong>，是分析和发现</td>
<td><strong>彻底改变</strong>，是转换和重构</td>
</tr>
<tr>
<td><strong>典型数据类型</strong></td>
<td><strong>交易数据、类别数据</strong>（如商品清单）</td>
<td><strong>数值型数据、连续型数据</strong>（如传感器读数）</td>
</tr>
<tr>
<td><strong>典型算法</strong></td>
<td>Apriori, FP-Growth</td>
<td>PCA, t-SNE</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<h3 id="它们的相似点在哪里？"><a href="#它们的相似点在哪里？" class="headerlink" title="它们的相似点在哪里？"></a><strong>它们的相似点在哪里？</strong></h3><p>尽管目标和操作截然不同，但它们确实有相似之处，这也是它们都被归为“无监督学习”的原因：</p>
<ol>
<li><strong>都无需标签：</strong> 它们都不需要一个预先定义好的“正确答案”来指导学习过程。</li>
<li><strong>都在探索数据内在结构：</strong><ul>
<li>ARM 探索的是<strong>项集之间的共现结构</strong>。</li>
<li>降维 探索的是<strong>特征之间的相关性结构或方差结构</strong>。</li>
<li>它们都在试图理解“数据本身告诉了我们什么”，而不是去预测一个外部变量。</li>
</ul>
</li>
</ol>
<h3 id="回到您的问题"><a href="#回到您的问题" class="headerlink" title="回到您的问题"></a><strong>回到您的问题</strong></h3><blockquote>
<p>“我挖掘了一个关联规则不就相当于提取到了一个特征来降维吗？”</p>
</blockquote>
<p>现在我们就能清晰地回答了：</p>
<ul>
<li>您挖掘的规则 <code>&#123;A&#125; -&gt; &#123;B&#125;</code> 是一个<strong>洞察</strong>，它告诉您 A 和 B 之间存在强关联。</li>
<li>您可以基于这个洞察<strong>手动创造</strong>一个新的特征，这个过程叫做<strong>特征工程（Feature Engineering）</strong>。例如，您可以增加一列叫 <code>is_A_B_buyer</code>，如果一个客户同时购买了 A 和 B，就标记为1，否则为0。</li>
<li>但是，这个<strong>手动创造新特征</strong>的动作，和降维算法（如PCA）<strong>自动地将几十个特征压缩成几个新特征</strong>，是完全不同的两件事。前者是基于一个特定模式的人工操作，后者是基于整个数据集方差分布的算法转换。</li>
</ul>
<p><strong>总结：</strong></p>
<ul>
<li><strong>关联规则挖掘告诉你一个故事</strong>（例如，“买尿布的人也爱买啤酒”）。</li>
<li><strong>降维给你提供了一张更简洁的地图</strong>（例如，用“经纬度”两个特征替代复杂的街道、门牌号等十几个特征）。</li>
</ul>
<p>一个是为了获得可以指导行动的<strong>商业洞察</strong>，另一个是为了<strong>优化数据本身</strong>，使其更易于存储、计算和可视化。</p>
<p> 是不是我可以先根据数据特征进行关联规则判断，如果两个特征有关联，我可以去掉其中一个，然后把剩下的特征来做聚类 </p>
<p>您这个想法非常有创意，而且逻辑上是自洽的！您准确地抓住了数据预处理的一个核心思想：<strong>在进行聚类或建模之前，应该先处理特征之间的冗余性，因为冗余的特征会影响算法的效果。</strong></p>
<p>这个思路的大方向是<strong>完全正确</strong>的。但是，使用“关联规则挖掘”来做这件事，属于一个<strong>“用对了逻辑，但选错了工具”</strong>的情况。</p>
<p>下面我们来详细解释为什么，并告诉您正确的工具应该是什么。</p>
<hr>
<h3 id="为什么用关联规则挖掘来处理特征冗余是“不合适”的？"><a href="#为什么用关联规则挖掘来处理特征冗余是“不合适”的？" class="headerlink" title="为什么用关联规则挖掘来处理特征冗余是“不合适”的？"></a><strong>为什么用关联规则挖掘来处理特征冗余是“不合适”的？</strong></h3><p>关联规则挖掘（ARM）和我们通常所说的特征间关联，虽然都叫“关联”，但它们处理的场景和数据类型完全不同。</p>
<ol>
<li><strong>ARM处理的是“交易数据” (Transactional Data)</strong><ul>
<li>ARM的“篮子”里装的是<strong>“项” (Items)<strong>，比如<code>&#123;牛奶, 面包, 啤酒&#125;</code>。它分析的是</strong>项与项之间的共现关系</strong>。</li>
<li>它的输入数据通常是稀疏的，每一行是一个交易，列是所有可能的商品。</li>
</ul>
</li>
<li><strong>特征关联处理的是“属性数据” (Attribute Data)</strong><ul>
<li>聚类的数据，每一行是一个样本（如一个客户），每一列是一个**“特征” (Feature)**，比如<code>年龄</code>、<code>收入</code>、<code>消费频率</code>。</li>
<li>我们在这里关心的是<strong>特征与特征（列与列）之间的关系</strong>，比如<code>年龄</code>这个特征和<code>收入</code>这个特征是不是高度相关。</li>
</ul>
</li>
</ol>
<p><strong>把特征当作“项”来跑关联规则，会遇到问题：</strong></p>
<ul>
<li><strong>数据格式不匹配：</strong> 您需要对数据进行复杂的转换（比如将连续的<code>收入</code>值离散化成“高、中、低”这样的“项”），这个过程本身就会损失大量信息。</li>
<li><strong>衡量标准不适用：</strong> ARM的支持度、置信度等指标，是为衡量“商品A出现时，商品B也出现”的场景设计的。而衡量两个特征（如<code>身高</code>和<code>体重</code>）的关系，我们有更直接、更强大的数学工具。</li>
</ul>
<hr>
<h3 id="处理特征冗余的“正确工具”是什么？"><a href="#处理特征冗余的“正确工具”是什么？" class="headerlink" title="处理特征冗余的“正确工具”是什么？"></a><strong>处理特征冗余的“正确工具”是什么？</strong></h3><p>您想实现的“如果两个特征有关联，我可以去掉其中一个”这个目标，在机器学习中通常通过以下两种标准方法实现：</p>
<h4 id="方法一：相关性分析-Correlation-Analysis-——-针对数值型特征"><a href="#方法一：相关性分析-Correlation-Analysis-——-针对数值型特征" class="headerlink" title="方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征"></a><strong>方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征</strong></h4><p>这是最直接、最常用的方法。</p>
<ul>
<li><strong>操作：</strong> 计算所有特征两两之间的<strong>相关系数</strong>（如皮尔逊相关系数）。这个系数的范围在-1到+1之间。<ul>
<li><strong>+1：</strong> 完全正相关（一个特征增加，另一个也以相同比例增加）。</li>
<li><strong>-1：</strong> 完全负相关（一个特征增加，另一个以相同比例减少）。</li>
<li><strong>0：</strong> 完全不相关。</li>
</ul>
</li>
<li><strong>判断：</strong> 您可以设定一个阈值，比如，如果两个特征之间的相关系数的<strong>绝对值大于0.8或0.9</strong>，就认为它们是高度冗余的。</li>
<li><strong>决策：</strong> 从这两个高度相关的特征中，您可以：<ol>
<li><strong>根据业务理解，保留更重要的一个</strong>，删除另一个。</li>
<li>如果无法判断，可以<strong>随机删除一个</strong>。</li>
<li>或者，将它们<strong>组合成一个新特征</strong>（比如取平均值）。</li>
</ol>
</li>
</ul>
<p><strong>这完美地、且更精确地实现了您的想法。</strong></p>
<h4 id="方法二：使用降维算法-Dimensionality-Reduction-——-更优越的自动化方案"><a href="#方法二：使用降维算法-Dimensionality-Reduction-——-更优越的自动化方案" class="headerlink" title="方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案"></a><strong>方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案</strong></h4><p>手动删除特征虽然简单，但缺点是会<strong>完全丢失被删除特征所包含的独有信息</strong>。一个更优雅、信息损失更少的方案是使用我们之前讨论过的<strong>降维</strong>，尤其是<strong>主成分分析（PCA）</strong>。</p>
<ul>
<li><strong>PCA如何解决冗余问题？</strong> PCA能够自动检测到特征之间的相关性。比如，它发现<code>身高</code>和<code>体重</code>高度相关，它不会粗暴地扔掉任何一个，而是会<strong>智能地将它们融合成一个新的、综合性的特征</strong>，比如可以理解为“体格因子”。</li>
<li><strong>优点：</strong><ul>
<li><strong>自动化：</strong> 无需手动选择阈值和要删除的特征。</li>
<li><strong>信息保留更完整：</strong> 它不是删除信息，而是重组和压缩信息，将多个相关特征的精华提取到一个新的特征中。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结：您的思路的正确升级路径"><a href="#总结：您的思路的正确升级路径" class="headerlink" title="总结：您的思路的正确升级路径"></a><strong>总结：您的思路的正确升级路径</strong></h3><p>您的想法是一个非常好的起点，现在我们可以把它升级为机器学习的标准工作流：</p>
<ol>
<li><strong>您的原始想法：</strong> <code>数据 -&gt; 用关联规则判断特征关系 -&gt; 删除冗余特征 -&gt; 聚类</code></li>
<li><strong>标准实践方法A（手动选择）：</strong> <code>数据 -&gt; 计算特征间的**相关系数矩阵** -&gt; 识别并**手动删除**高度相关的特征 -&gt; 聚类</code></li>
<li><strong>标准实践方法B（自动提取）：</strong> <code>数据 -&gt; 使用**PCA等降维技术**将原始特征转换为少数几个新的主成分 -&gt; 使用**新的主成分**进行聚类</code></li>
</ol>
<p><strong>结论：</strong> 您的核心逻辑——“消除冗余以优化聚类”——是完全正确的，这是数据科学家一直在做的事情。只是实现这个逻辑的工具，应该是<strong>相关性分析</strong>或<strong>PCA</strong>，而不是关联规则挖掘。您已经非常接近专业的数据处理思维了！</p>
<p> 那我是不是可以先使用无监督学习降维再进行无监督学习聚类 </p>
<p>是的，完全正确！您刚才描述的这个流程，<strong>不仅是可行的，而且是机器学习中非常常用、非常强大的一种标准实践（Best Practice）</strong>。</p>
<p>将降维和聚类结合使用，通常被称为“先降维，再聚类”，是无监督学习工具箱里一套威力十足的“组合拳”。</p>
<hr>
<h3 id="为什么这是一个“黄金组合”？"><a href="#为什么这是一个“黄金组合”？" class="headerlink" title="为什么这是一个“黄金组合”？"></a><strong>为什么这是一个“黄金组合”？</strong></h3><p>在很多情况下，直接在高维度的原始数据上进行聚类，效果可能并不理想。而先进行降维，则可以为后续的聚类算法提供一个更理想、更干净的“工作台”。这样做有四大核心好处：</p>
<h4 id="1-攻克“维度灾难”-Curse-of-Dimensionality"><a href="#1-攻克“维度灾难”-Curse-of-Dimensionality" class="headerlink" title="1. 攻克“维度灾难” (Curse of Dimensionality)"></a>1. <strong>攻克“维度灾难” (Curse of Dimensionality)</strong></h4><p>当数据特征（维度）非常多时，数据点会变得异常稀疏，任何两个点之间的距离都可能变得很大且相差无几。这会让依赖距离计算的聚类算法（如K-Means）难以判断样本间的真实亲疏关系，导致聚类效果下降。降维可以有效缓解这个问题。</p>
<h4 id="2-提升聚类效果与速度-Improving-Quality-and-Speed"><a href="#2-提升聚类效果与速度-Improving-Quality-and-Speed" class="headerlink" title="2. 提升聚类效果与速度 (Improving Quality and Speed)"></a>2. <strong>提升聚类效果与速度 (Improving Quality and Speed)</strong></h4><ul>
<li><strong>去除噪声：</strong> 原始特征中可能包含很多与数据内在结构无关的“噪声”。降维（尤其是PCA）擅长捕捉数据的主要变化方向（信号），而忽略次要的随机波动（噪声）。在更“纯净”的数据上聚类，效果自然更好。</li>
<li><strong>减少冗余：</strong> 降维处理了特征间的相关性，避免了聚类算法被几个高度相关的特征重复“误导”。</li>
<li><strong>加快计算：</strong> 特征数量减少了，聚类算法的计算量会大幅下降，尤其是在处理大型数据集时，可以节省大量时间和计算资源。</li>
</ul>
<h4 id="3-方便可视化与理解-Enabling-Visualization-and-Understanding"><a href="#3-方便可视化与理解-Enabling-Visualization-and-Understanding" class="headerlink" title="3. 方便可视化与理解 (Enabling Visualization and Understanding)"></a>3. <strong>方便可视化与理解 (Enabling Visualization and Understanding)</strong></h4><p>这是一个非常直观的好处。您无法画出一张100维的图，但通过降维将数据降到2维或3维后，您就可以轻松地将数据点绘制在散点图上。此时，如果数据有明显的聚类结构，你将<strong>用肉眼直接看到</strong>它们分成几堆，这对于理解数据、确定聚类数量（K值）以及验证聚类结果都非常有帮助。</p>
<hr>
<h3 id="一个典型的操作流程-A-Typical-Workflow"><a href="#一个典型的操作流程-A-Typical-Workflow" class="headerlink" title="一个典型的操作流程 (A Typical Workflow)"></a><strong>一个典型的操作流程 (A Typical Workflow)</strong></h3><p>假设您手上有包含上百个特征的客户数据，想对他们进行分群。</p>
<ol>
<li><strong>第一步：数据预处理 (Preprocessing)</strong><ul>
<li>对原始数据进行<strong>标准化（Scaling）</strong>。这是非常关键的一步，因为像PCA这样的降维算法对数据的尺度非常敏感。通常使用 <code>StandardScaler</code>。</li>
</ul>
</li>
<li><strong>第二步：应用降维 (Dimensionality Reduction)</strong><ul>
<li>使用<strong>PCA</strong>算法处理标准化后的数据。</li>
<li>您可以决定保留多少个主成分（比如，选择能解释原始数据90%或95%方差的前N个主成分）。这样，您的数据就从几百维降到了N维（N通常远小于原始维度）。</li>
</ul>
</li>
<li><strong>第三步：应用聚类 (Clustering)</strong><ul>
<li>在<strong>降维后的新数据集</strong>上运行聚类算法（如<strong>K-Means</strong>）。</li>
<li>此时算法会运行得更快，并且更容易在低维空间中找到清晰的簇边界。</li>
</ul>
</li>
<li><strong>第四步：结果分析与解读 (Analysis)</strong><ul>
<li>分析聚类结果。您可以将降到2维的数据点和聚类结果一起可视化，直观地查看分群效果。</li>
<li>要理解每个簇的业务含义，您需要将簇的标签映射回原始的高维特征空间，去分析每个群体的客户在原始特征上的具体表现。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="一个生动的比喻：雕塑家的工作"><a href="#一个生动的比喻：雕塑家的工作" class="headerlink" title="一个生动的比喻：雕塑家的工作"></a><strong>一个生动的比喻：雕塑家的工作</strong></h3><p>您可以把这个过程想象成一位雕塑家创作一件作品：</p>
<ul>
<li><strong>原始高维数据：</strong> 一块巨大、粗糙、未经雕琢的大理石原料。</li>
<li><strong>降维 (PCA)：</strong> 雕塑家首先用大锤和凿子，<strong>砍掉大块多余的石料</strong>，勾勒出作品的大致轮廓。这个过程去除了“噪声”和“冗余”，让作品的核心形态显现出来。</li>
<li><strong>聚类 (K-Means)：</strong> 在这个已经有了基本轮廓的石胚上，雕塑家再换上小刻刀，进行<strong>精雕细琢</strong>，刻画出人物的五官、衣物的褶皱等细节。这些细节就是数据中不同的“簇”。</li>
</ul>
<p>如果不先进行第一步的“塑形”，直接在巨大的原料上进行精雕细琢，不仅费时费力，而且很容易迷失在细节中，无法把握整体结构。</p>
<p><strong>总结：</strong> 您的想法完全正确，并且是解决复杂数据分析问题的标准思路。<strong>先用降维负责“清理和浓缩”数据，再用聚类负责“分组和打标”</strong>，两者相辅相成，能够达到 <code>1+1 &gt; 2</code> 的效果。</p>
<p> 关联特征分析是不是不处理特征数值的大小，只处理特征之间出现的协同性 </p>
<p>您这个问题提得非常精准！您的描述，“<strong>不处理特征数值的大小，只处理特征之间出现的协同性</strong>”，完美地概括了我们之前讨论的<strong>关联规则挖掘（Association Rule Mining）</strong>的核心特点。</p>
<p>但是，“关联特征分析”这个词可以有更广泛的含义，它包含了多种不同的技术。所以，您的这个说法<strong>是否正确，取决于我们具体指的是哪一种分析方法</strong>。</p>
<p>让我们来清晰地辨析一下：</p>
<hr>
<h3 id="情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！"><a href="#情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！" class="headerlink" title="情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！"></a><strong>情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！</strong></h3><p>关联规则挖掘（ARM）的本质就是分析**“协同性” (Co-occurrence)**，也就是“出现”与“不出现”的问题。</p>
<ul>
<li><strong>核心思想：</strong> 它处理的是**离散的、分门别类的“项” (Items)**。数据被看作是一系列“交易”，每个交易包含一个“项”的集合。</li>
<li><strong>如何对待数值：</strong> 它不关心数值本身的大小或程度。例如，在购物篮分析中：<ul>
<li>系统只关心顾客的购物篮里<strong>有没有</strong>“牛奶”和“面包”。</li>
<li>它<strong>不关心</strong>顾客是买了1升牛奶还是2升牛奶（数值大小），也不关心他买的是10元的面包还是20元的面包（数值大小）。</li>
<li>只要“牛奶”和“面包”这两个“项”同时出现在购物篮里，这次交易就被计入协同出现的次数。</li>
</ul>
</li>
</ul>
<p>所以，对于关联规则挖掘，您的总结是100%准确的。它是一个基于<strong>“是否出现”</strong>的逻辑，而不是基于<strong>“数值高低”</strong>的逻辑。</p>
<hr>
<h3 id="情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。"><a href="#情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。" class="headerlink" title="情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。"></a><strong>情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。</strong></h3><p>相关性分析（Correlation Analysis）是另一种衡量特征关联度的常用方法，它专门用来处理<strong>数值型特征</strong>，并且<strong>完全依赖于数值的大小</strong>。</p>
<ul>
<li><strong>核心思想：</strong> 它衡量的是一个数值型特征<strong>变化</strong>时，另一个数值型特征<strong>如何随之变化</strong>。</li>
<li><strong>如何对待数值：</strong> 数值的大小和变化趋势是其分析的全部基础。例如，分析<code>身高</code>和<code>体重</code>这两个特征：<ul>
<li>我们不是关心一个人“有没有身高”和“有没有体重”。</li>
<li>我们关心的是<code>身高</code><strong>数值较高</strong>的人，其<code>体重</code><strong>数值是否也倾向于较高</strong>。</li>
<li>相关系数（比如+0.8）这个结果，就是通过计算所有人的身高和体重具体数值的线性关系得出的。数值的大小和变化方向是决定性的。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结对比"><a href="#总结对比" class="headerlink" title="总结对比"></a><strong>总结对比</strong></h3><p>为了让您看得更清楚，我们用一个表格来总结：</p>
<table>
<thead>
<tr>
<th>特性</th>
<th><strong>关联规则挖掘 (ARM)</strong></th>
<th><strong>相关性分析 (Correlation Analysis)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心思想</strong></td>
<td><strong>协同性 (Co-occurrence)</strong></td>
<td><strong>相关性 (Correlation)</strong></td>
</tr>
<tr>
<td><strong>是否处理数值大小？</strong></td>
<td><strong>否</strong>，只关心“有&#x2F;无”的布尔逻辑</td>
<td><strong>是</strong>，完全依赖于数值的大小和变化趋势</td>
</tr>
<tr>
<td><strong>典型数据类型</strong></td>
<td>交易数据、类别数据（商品清单、网页浏览记录）</td>
<td>数值型数据、连续型数据（身高、体重、收入、温度）</td>
</tr>
<tr>
<td><strong>回答的问题</strong></td>
<td>“哪些东西经常<strong>一起出现</strong>？”</td>
<td>“一个数值<strong>变大</strong>时，另一个数值是否也随之<strong>变大或变小</strong>？”</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p><strong>结论：</strong></p>
<p>您的理解非常敏锐，准确地抓住了关联规则挖掘的本质。但请务必记住，“关联特征分析”是一个更宽泛的领域。</p>
<ul>
<li>如果您面对的是像购物清单一样的<strong>类别数据</strong>，想找“一起出现”的模式，那么就是用关联规则挖掘，此时<strong>不考虑数值大小</strong>。</li>
<li>如果您面对的是像客户档案里的年龄、收入这样的<strong>数值数据</strong>，想看特征间的联动关系，那么就是用相关性分析，此时<strong>数值大小是分析的核心</strong>。</li>
</ul>
<p> 聚类有哪些算法，分别适用什么情况 </p>
<p>好的，这是一个非常实际且重要的问题。聚类算法有很多种，它们基于不同的数学原理，因此各自有其独特的优势和适用场景。没有“万能”的算法，选择哪种取决于您的数据特性和分析目标。</p>
<p>下面我将介绍几种最主流、最经典的聚类算法，并详细说明它们分别适用于什么情况。</p>
<hr>
<h3 id="1-K-均值聚类-K-Means-Clustering"><a href="#1-K-均值聚类-K-Means-Clustering" class="headerlink" title="1. K-均值聚类 (K-Means Clustering)"></a><strong>1. K-均值聚类 (K-Means Clustering)</strong></h3><p>这是最著名、最简单的聚类算法，属于“原型聚类”的代表，试图找到数据的“中心点”或“原型”。</p>
<ul>
<li><strong>工作原理：</strong><ol>
<li><strong>预设 K 值：</strong> 首先，您需要<strong>手动指定</strong>要将数据分成 K 个簇。</li>
<li><strong>随机初始化：</strong> 算法随机选择 K 个数据点作为初始的“簇中心”（Centroids）。</li>
<li><strong>迭代分配与更新：</strong><ul>
<li><strong>分配：</strong> 将每个数据点分配给离它最近的那个簇中心。</li>
<li><strong>更新：</strong> 重新计算每个簇所有数据点的平均值，将这个平均值作为新的簇中心。</li>
</ul>
</li>
<li><strong>循环此过程</strong>，直到簇中心不再变化为止。</li>
</ol>
</li>
<li><strong>适用情况：</strong><ul>
<li><strong>球状&#x2F;凸状数据分布：</strong> 当您的数据簇大致呈圆形或凸形时，K-Means 效果最好。</li>
<li><strong>大数据集：</strong> 算法非常高效，计算速度快，可扩展性好，适合处理大规模数据。</li>
<li><strong>簇的数量已知或可以预估：</strong> 当您对业务有一定了解，大致知道数据应该被分成几类时。</li>
</ul>
</li>
<li><strong>不适用&#x2F;缺点：</strong><ul>
<li><strong>需要预先指定K值：</strong> K值的选择对结果影响巨大，且往往难以确定。</li>
<li><strong>对初始值敏感：</strong> 随机选择的初始簇中心可能导致不同的聚类结果。</li>
<li><strong>对异常值敏感：</strong> 异常值会极大地影响簇中心的计算，可能导致簇中心偏离。</li>
<li><strong>无法处理非球状簇：</strong> 对于条形、环形或其他不规则形状的簇，K-Means 无法正确识别。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise"><a href="#2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise" class="headerlink" title="2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"></a><strong>2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong></h3><p>DBSCAN 是“基于密度的聚类”的杰出代表，它不关心数据的“中心”，只关心数据的“密度”。</p>
<ul>
<li><strong>工作原理：</strong><ol>
<li><strong>定义邻域：</strong> 需要设定两个参数：邻域半径（<code>eps</code>）和最小点数（<code>MinPts</code>）。</li>
<li><strong>识别核心点：</strong> 如果一个点的 <code>eps</code> 半径内包含了至少 <code>MinPts</code> 个点，那么这个点就是一个“核心点”。</li>
<li><strong>扩展簇：</strong> 从任意一个核心点出发，不断寻找其密度可达（即在 <code>eps</code> 范围内）的其他核心点和边界点，将它们合并成一个簇。</li>
<li><strong>识别噪声：</strong> 那些不属于任何簇的点被标记为“噪声”或“异常点”。</li>
</ol>
</li>
<li><strong>适用情况：</strong><ul>
<li><strong>任意形状的簇：</strong> DBSCAN 的最大优势，可以发现非球状、不规则形状的簇（如月牙形、环形）。</li>
<li><strong>需要识别异常点&#x2F;噪声：</strong> 能够自动将密度稀疏的区域识别为噪声，非常适合用于异常检测。</li>
<li><strong>不需要预设簇数：</strong> 算法会自动根据数据密度发现簇的数量，非常方便。</li>
</ul>
</li>
<li><strong>不适用&#x2F;缺点：</strong><ul>
<li><strong>对参数敏感：</strong> <code>eps</code> 和 <code>MinPts</code> 这两个参数的选择至关重要，且需要经验来调整。</li>
<li><strong>对密度不均的数据集效果不佳：</strong> 如果数据中不同簇的密度差异很大，用同一套 <code>eps</code> 和 <code>MinPts</code> 参数就很难同时覆盖所有簇。</li>
<li><strong>高维数据处理困难：</strong> 在高维空间中，“密度”的定义会变得模糊（维度灾难），<code>eps</code> 参数难以设定。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3-层次聚类-Hierarchical-Clustering"><a href="#3-层次聚类-Hierarchical-Clustering" class="headerlink" title="3. 层次聚类 (Hierarchical Clustering)"></a><strong>3. 层次聚类 (Hierarchical Clustering)</strong></h3><p>层次聚类试图构建一个数据的层次结构，就像一棵树。它分为“自底向上”（凝聚型）和“自顶向下”（分裂型）两种。</p>
<ul>
<li><strong>工作原理（以凝聚型为例）：</strong><ol>
<li><strong>初始化：</strong> 将每个数据点都视为一个独立的簇。</li>
<li><strong>迭代合并：</strong> 找到所有簇中“距离”最近的两个簇，并将它们合并成一个新的簇。</li>
<li><strong>循环此过程</strong>，直到所有数据点最终被合并成一个最大的簇为止。</li>
<li><strong>结果呈现：</strong> 整个过程会形成一个树状图（Dendrogram），您可以根据需要，在任意高度“横切”一刀，来决定最终的簇数量。</li>
</ol>
</li>
<li><strong>适用情况：</strong><ul>
<li><strong>希望看到数据的层次关系：</strong> 当您不仅想知道数据分几类，还想了解类别之间的亲疏远近关系时，如生物学中的物种分类谱系。</li>
<li><strong>簇的数量不确定：</strong> 您可以通过观察树状图来决定一个合理的簇数量。</li>
<li><strong>小数据集：</strong> 对于小规模的数据，其结果直观且易于解释。</li>
</ul>
</li>
<li><strong>不适用&#x2F;缺点：</strong><ul>
<li><strong>计算复杂度高：</strong> 时间复杂度通常为 O(n³)，空间复杂度为 O(n²)，不适合处理大规模数据集。</li>
<li><strong>合并&#x2F;分裂不可逆：</strong> 一旦一个步骤的合并或分裂完成，后续就无法撤销，这可能导致次优的聚类结果。</li>
<li><strong>对距离度量敏感：</strong> 选择不同的距离计算方法（如欧氏距离、曼哈顿距离）和合并策略（如ward、complete）对结果影响很大。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="4-高斯混合模型-Gaussian-Mixture-Model-GMM"><a href="#4-高斯混合模型-Gaussian-Mixture-Model-GMM" class="headerlink" title="4. 高斯混合模型 (Gaussian Mixture Model, GMM)"></a><strong>4. 高斯混合模型 (Gaussian Mixture Model, GMM)</strong></h3><p>GMM 是一种“基于概率分布的聚类”方法。它假设所有数据点都是从若干个不同的高斯分布（正态分布）中生成的。</p>
<ul>
<li><strong>工作原理：</strong><ol>
<li><strong>假设：</strong> 假定数据由 K 个高斯分布混合而成，每个分布代表一个簇。</li>
<li><strong>EM算法：</strong> 使用期望最大化（Expectation-Maximization, EM）算法进行迭代：<ul>
<li><strong>E-步（期望）：</strong> 估计每个数据点由每个高斯分布生成的<strong>概率</strong>。</li>
<li><strong>M-步（最大化）：</strong> 根据上一步的概率，更新每个高斯分布的参数（均值、方差、权重），使其能最大化地拟合数据。</li>
</ul>
</li>
<li><strong>循环此过程</strong>，直到模型收敛。最终，每个数据点可以被分配给生成它的概率最大的那个高斯分布（簇）。</li>
</ol>
</li>
<li><strong>适用情况：</strong><ul>
<li><strong>椭圆形簇：</strong> K-Means 只能处理球形簇，而GMM可以处理更具一般性的<strong>椭圆形簇</strong>。</li>
<li><strong>软聚类 (Soft Clustering)：</strong> GMM 不会硬性地将一个点分给某一类，而是给出一个<strong>概率</strong>。这对于处理簇边界模糊的数据非常有用。</li>
<li><strong>需要生成新样本：</strong> 由于GMM是一个生成模型，一旦训练完成，您可以用它来生成与原始数据分布类似的新数据点。</li>
</ul>
</li>
<li><strong>不适用&#x2F;缺点：</strong><ul>
<li><strong>计算复杂：</strong> EM算法的计算量比K-Means大。</li>
<li><strong>需要预设簇数：</strong> 和K-Means一样，需要指定高斯分布的数量。</li>
<li><strong>对非椭圆形数据效果不佳：</strong> 其假设是高斯分布，所以无法很好地拟合不规则形状的簇。</li>
</ul>
</li>
</ul>
<h3 id="总结：如何选择？"><a href="#总结：如何选择？" class="headerlink" title="总结：如何选择？"></a><strong>总结：如何选择？</strong></h3><table>
<thead>
<tr>
<th>算法</th>
<th>核心思想</th>
<th>簇形状</th>
<th>簇数量</th>
<th>规模</th>
<th>主要优点</th>
<th>主要缺点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>K-Means</strong></td>
<td>基于中心</td>
<td><strong>球形&#x2F;凸形</strong></td>
<td><strong>需预设</strong></td>
<td>大</td>
<td>速度快，简单高效</td>
<td>对初始值&#x2F;异常值敏感，无法处理非球状</td>
</tr>
<tr>
<td><strong>DBSCAN</strong></td>
<td>基于密度</td>
<td><strong>任意形状</strong></td>
<td><strong>自动发现</strong></td>
<td>中&#x2F;大</td>
<td>能处理任意形状，能识别噪声</td>
<td>对参数和密度不均敏感</td>
</tr>
<tr>
<td><strong>层次聚类</strong></td>
<td>基于层次</td>
<td>任意形状</td>
<td>无需预设</td>
<td>小</td>
<td>提供层次关系，结果直观</td>
<td>计算复杂度高，不适合大数据</td>
</tr>
<tr>
<td><strong>GMM</strong></td>
<td>基于分布</td>
<td><strong>椭圆形</strong></td>
<td><strong>需预设</strong></td>
<td>中</td>
<td>提供属于各簇的概率（软聚类）</td>
<td>假设数据为高斯分布，计算较慢</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p><strong>一个简单的决策流程：</strong></p>
<ol>
<li><strong>数据量大，且簇的形状大致是球形？</strong> -&gt; <strong>首选 K-Means</strong>。</li>
<li><strong>不确定要分几类，簇的形状可能不规则，且想识别噪声点？</strong> -&gt; <strong>首选 DBSCAN</strong>。</li>
<li><strong>数据量不大，且想了解数据内在的层次结构？</strong> -&gt; <strong>使用层次聚类</strong>。</li>
<li><strong>簇的形状可能是椭圆形，或者希望得到一个点属于各个簇的概率？</strong> -&gt; <strong>考虑 GMM</strong>。</li>
</ol>
<p> 什么是簇的形状 </p>
<p>当然，这是一个非常好的问题！在聚类分析中，我们经常提到“簇的形状”，这听起来可能有点抽象，因为数据本身是没有物理形状的。</p>
<p>“簇的形状”其实是一个非常直观的比喻，它指的是<strong>数据点在特征空间（即图表上）的分布模式或排列形态</strong>。</p>
<p>为了方便理解，我们通常把它想象在一个二维的散点图上（例如，X轴是“收入”，Y轴是“消费频率”）。每个数据点就是图上的一个点，而一群点的聚集形态，就是我们所说的“簇的形状”。</p>
<p>下面我们来详细看一下几种典型的簇形状，以及它们如何影响算法的选择。</p>
<hr>
<h3 id="1-球形-凸形-Spherical-Convex-Shape"><a href="#1-球形-凸形-Spherical-Convex-Shape" class="headerlink" title="1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)"></a><strong>1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)</strong></h3><p>这是最简单、最理想的一种形状。</p>
<ul>
<li><strong>直观描述：</strong> 数据点紧密地聚集在一个大致<strong>圆形</strong>的区域内，密度从中心向四周均匀递减。从任何角度看，这个簇都是一个“实心”的团，没有凹陷。</li>
<li><strong>例子：</strong> 测量一大群同龄男性的身高和体重，这些数据点很可能会形成一个球状或椭圆形的簇。</li>
<li><strong>适合的算法：</strong> <strong>K-Means</strong>。因为K-Means的原理就是找到一个“中心点”，然后将离这个中心点近的所有点都归为一类。这种“离中心点近”的逻辑，天然地就定义出了一个球形的边界。</li>
</ul>
<hr>
<h3 id="2-椭圆形-Elliptical-Shape"><a href="#2-椭圆形-Elliptical-Shape" class="headerlink" title="2. 椭圆形 (Elliptical Shape)"></a><strong>2. 椭圆形 (Elliptical Shape)</strong></h3><p>这是球形的一种延伸，在现实世界中更常见。</p>
<ul>
<li><strong>直观描述：</strong> 数据点聚集在一个<strong>椭圆形</strong>的区域内。这通常意味着不同特征（坐标轴）之间的尺度不同，或者特征之间存在一定的相关性。</li>
<li><strong>例子：</strong> 客户的“年龄”和“收入”数据，很可能形成一个椭圆形，因为年龄和收入是正相关的，数据点会沿着一个方向被“拉长”。</li>
<li><strong>适合的算法：</strong> **高斯混合模型 (GMM)**。GMM假设数据是从高斯分布（正态分布）中生成的，而高斯分布的等高线图正好是椭圆形。它可以很好地拟合这种被拉伸或旋转的簇。K-Means在这种情况下效果会变差，因为它总想画一个“正圆”的边界。</li>
</ul>
<hr>
<h3 id="3-任意形状-Arbitrary-Shape"><a href="#3-任意形状-Arbitrary-Shape" class="headerlink" title="3. 任意形状 (Arbitrary Shape)"></a><strong>3. 任意形状 (Arbitrary Shape)</strong></h3><p>这是最复杂，但也最有趣的一类形状，它们不是一个“实心”的团。</p>
<ul>
<li><strong>直观描述：</strong> 数据的分布呈现出各种不规则的形态，比如条形、环形、月牙形、S形等。这些形状通常是“非凸”的，即内部有“凹陷”或“空洞”。</li>
<li><strong>例子：</strong><ul>
<li><strong>条形&#x2F;线性：</strong> 河流沿岸的村庄分布。</li>
<li><strong>环形&#x2F;同心圆：</strong> 靶心周围的弹孔分布，可能有一圈密集的9环和一圈密集的7环，但8环和靶心是空的。</li>
<li><strong>月牙形：</strong> 生物学或天文学中某些特定的数据模式。</li>
</ul>
</li>
<li><strong>适合的算法：</strong> <strong>DBSCAN</strong>。DBSCAN不关心簇的中心在哪里，只关心密度。只要点与点之间足够近，能够“连接”起来，它就能把它们都识别成一个簇。因此，它能像“贪吃蛇”一样，沿着任何形状的路径把簇找出来。</li>
</ul>
<p><em>(上图展示了不同算法在处理任意形状簇时的表现。可以看到K-Means完全失败了，而DBSCAN和层次聚类则表现得很好。)</em></p>
<hr>
<h3 id="为什么不同算法偏好不同形状？"><a href="#为什么不同算法偏好不同形状？" class="headerlink" title="为什么不同算法偏好不同形状？"></a><strong>为什么不同算法偏好不同形状？</strong></h3><p>这源于它们底层的数学逻辑：</p>
<ul>
<li><strong>K-Means的逻辑是“距离中心点近”</strong>：这个逻辑决定了它的势力范围必然是球形的。对于两个月牙形的簇，K-Means会错误地将距离各自中心点近的点划分在一起，从而把两个月牙都“切开”，并错误地组合在一起。</li>
<li><strong>DBSCAN的逻辑是“密度相连”</strong>：它的逻辑是“我的邻居是我的朋友，我邻居的邻居也是我的朋友”。这个逻辑与整体形状无关，只关心局部有多拥挤。因此，只要数据点能形成一条密度足够高的“小径”，DBSCAN就能沿着这条小径把整个簇识别出来，无论小径最终弯曲成了什么形状。</li>
</ul>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h3><table>
<thead>
<tr>
<th>形状类型</th>
<th>直观描述</th>
<th>适合的算法</th>
<th>不适合的算法</th>
</tr>
</thead>
<tbody><tr>
<td><strong>球形&#x2F;凸形</strong></td>
<td>圆形的、实心的团</td>
<td><strong>K-Means</strong>, GMM</td>
<td>DBSCAN (也能用，但有点大材小用)</td>
</tr>
<tr>
<td><strong>椭圆形</strong></td>
<td>椭圆形的、被拉伸的团</td>
<td><strong>GMM</strong>, 层次聚类</td>
<td>K-Means (效果会变差)</td>
</tr>
<tr>
<td><strong>任意形状</strong></td>
<td>条形、环形、月牙形等不规则形状</td>
<td><strong>DBSCAN</strong>, 层次聚类</td>
<td><strong>K-Means</strong> (完全不适用), GMM</td>
</tr>
</tbody></table>
<p><strong>核心要点：</strong> 在选择聚类算法前，最好先通过<strong>数据可视化</strong>（比如抽样画出特征的散点图）对数据的分布形态有一个初步的了解。看到数据的“形状”，可以极大地帮助你选择正确的“工具”（算法），从而得到有意义的分析结果。</p>
<p> 降维的算法有哪些 </p>
<p>当然。降维算法种类繁多，它们可以被划分为两大主流派系：**特征选择（Feature Selection）**和**特征提取（Feature Extraction）**。</p>
<p>下面我将为您详细介绍这两个派系下的主要算法。</p>
<hr>
<h3 id="两大降维派系"><a href="#两大降维派系" class="headerlink" title="两大降维派系"></a><strong>两大降维派系</strong></h3><p>首先，理解这两个派系的根本区别至关重要：</p>
<ol>
<li><strong>特征选择 (Feature Selection):</strong><ul>
<li><strong>思想：</strong> 像一位“评审员”，直接从原始的特征中<strong>挑选出</strong>一个最优的子集，而<strong>不改变原始特征本身</strong>。被选中的特征保持原有的物理意义。</li>
<li><strong>优点：</strong> 可解释性强，因为你清楚地知道留下了哪些原始特征。</li>
<li><strong>例子：</strong> 从100个客户特征中，挑选出最重要的10个（如“收入”、“年龄”、“消费频率”等）。</li>
</ul>
</li>
<li><strong>特征提取 (Feature Extraction):</strong><ul>
<li><strong>思想：</strong> 像一位“炼金术士”，将原始的多个特征进行<strong>融合与重组</strong>，创造出全新的、数量更少的特征。新特征是原始特征的某种组合。</li>
<li><strong>优点：</strong> 能够保留更多原始数据的信息，通常降维效果更好。</li>
<li><strong>缺点：</strong> 新生成的特征通常失去了原有的物理意义，可解释性变差。</li>
<li><strong>例子：</strong> 将“身高”、“体重”、“臂展”等多个特征融合成一个新的特征，称为“体格因子”。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="一、特征选择-Feature-Selection-的方法"><a href="#一、特征选择-Feature-Selection-的方法" class="headerlink" title="一、特征选择 (Feature Selection) 的方法"></a><strong>一、特征选择 (Feature Selection) 的方法</strong></h3><p>特征选择本身不是一个单一算法，而是一类方法的统称，主要分为三类：</p>
<h4 id="1-过滤法-Filter-Methods"><a href="#1-过滤法-Filter-Methods" class="headerlink" title="1. 过滤法 (Filter Methods)"></a><strong>1. 过滤法 (Filter Methods)</strong></h4><p>在任何模型训练之前，先对数据进行预处理，根据特征的统计属性（如方差、相关性、信息量）来“过滤”掉一部分特征。</p>
<ul>
<li><strong>方差选择法 (Variance Threshold):</strong> 优先删除那些方差很小甚至为零的特征，因为这些特征在所有样本上几乎没有变化，包含的信息量很少。</li>
<li><strong>相关系数法 (Correlation Coefficient):</strong> 计算特征之间的相关性，如果两个特征高度相关（如“房屋面积”和“房间数量”），则可以只保留其中一个。</li>
<li><strong>卡方检验 (Chi-Squared Test):</strong> 用于评估类别特征对于类别标签的相关性。</li>
<li><strong>信息增益 (Information Gain):</strong> 常用于决策树，评估一个特征对于数据集不确定性的减少程度。</li>
</ul>
<h4 id="2-包装法-Wrapper-Methods"><a href="#2-包装法-Wrapper-Methods" class="headerlink" title="2. 包装法 (Wrapper Methods)"></a><strong>2. 包装法 (Wrapper Methods)</strong></h4><p>将特征选择过程“包装”在一个特定的机器学习模型周围，通过评估模型性能来决定保留哪些特征。</p>
<ul>
<li><strong>递归特征消除 (Recursive Feature Elimination, RFE):</strong><ul>
<li><strong>思想：</strong> 首先用所有特征训练一个模型，然后移除最不重要的特征，再用剩下的特征重新训练模型……如此循环，直到剩下预设数量的特征。</li>
<li><strong>优点：</strong> 效果通常很好。</li>
<li><strong>缺点：</strong> 计算成本非常高，因为需要反复训练模型。</li>
</ul>
</li>
</ul>
<h4 id="3-嵌入法-Embedded-Methods"><a href="#3-嵌入法-Embedded-Methods" class="headerlink" title="3. 嵌入法 (Embedded Methods)"></a><strong>3. 嵌入法 (Embedded Methods)</strong></h4><p>在模型训练的过程中<strong>自动</strong>进行特征选择。</p>
<ul>
<li><strong>L1 正则化 (LASSO):</strong><ul>
<li><strong>思想：</strong> 在模型的损失函数中加入L1惩罚项，它倾向于将不重要的特征的系数直接压缩到<strong>零</strong>。训练完成后，系数不为零的特征就是被选中的特征。</li>
<li><strong>优点：</strong> 高效且常用，将特征选择与模型训练融为一体。</li>
</ul>
</li>
<li><strong>基于树模型的方法 (Tree-based Models):</strong><ul>
<li><strong>思想：</strong> 决策树、随机森林、梯度提升树等模型在构建过程中，会自然地计算每个特征的重要性（Feature Importance）。我们可以直接根据这个重要性排名来选择特征。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="二、特征提取-Feature-Extraction-的算法"><a href="#二、特征提取-Feature-Extraction-的算法" class="headerlink" title="二、特征提取 (Feature Extraction) 的算法"></a><strong>二、特征提取 (Feature Extraction) 的算法</strong></h3><p>这是“降维算法”这个词通常所指代的，主要分为线性和非线性两大类。</p>
<h4 id="A-线性降维算法"><a href="#A-线性降维算法" class="headerlink" title="A. 线性降维算法"></a><strong>A. 线性降维算法</strong></h4><p>假设数据中的主要结构可以通过一个线性子空间来描述。</p>
<ul>
<li><strong>1. 主成分分析 (Principal Component Analysis, PCA)</strong><ul>
<li><strong>核心思想：</strong> 寻找数据中<strong>方差最大</strong>的方向，将这些方向作为新的坐标轴（主成分）。它旨在最大程度地保留原始数据的“可分性”（方差）。</li>
<li><strong>适用场景：</strong> <strong>最常用、最经典</strong>的降维算法，是大多数情况下的<strong>首选基准</strong>。广泛用于数据预处理、去噪和可视化。</li>
</ul>
</li>
<li><strong>2. 线性判别分析 (Linear Discriminant Analysis, LDA)</strong><ul>
<li><strong>核心思想：</strong> <strong>（注意：这是一个有监督算法）</strong> 与PCA不同，LDA的目标是寻找一个能<strong>最大化类间距离、同时最小化类内距离</strong>的投影方向。通俗讲，就是让不同类别的数据点在投影后分得尽可能开。</li>
<li><strong>适用场景：</strong> 主要用于<strong>分类问题</strong>的预处理，当你有类别标签时，降维的目标是为了更好地分类。</li>
</ul>
</li>
</ul>
<h4 id="B-非线性降维算法（流形学习-Manifold-Learning）"><a href="#B-非线性降维算法（流形学习-Manifold-Learning）" class="headerlink" title="B. 非线性降维算法（流形学习 Manifold Learning）"></a><strong>B. 非线性降维算法（流形学习 Manifold Learning）</strong></h4><p>假设高维数据实际上分布在一个低维的、弯曲的“流形”上。目标是“展开”这个流形，找到其内在的低维结构。</p>
<ul>
<li><strong>1. t-分布随机邻域嵌入 (t-SNE)</strong><ul>
<li><strong>核心思想：</strong> 重点保持数据点的<strong>局部邻域结构</strong>。它假设高维空间中距离近的点，在低维空间中也应该距离近。</li>
<li><strong>适用场景：</strong> <strong>数据可视化</strong>的王者。非常擅长将高维数据降到2D或3D，以观察其聚类结构。<strong>注意：</strong> t-SNE主要用于可视化，降维后的点间距离不具有真实意义，不建议直接作为其他模型的输入。</li>
</ul>
</li>
<li><strong>2. 均匀流形逼近与投影 (Uniform Manifold Approximation and Projection, UMAP)</strong><ul>
<li><strong>核心思想：</strong> 与t-SNE类似，但基于更坚实的数学理论。它在保持局部结构的同时，能更好地保留数据的<strong>全局结构</strong>。</li>
<li><strong>适用场景：</strong> 现代的<strong>数据可视化</strong>首选，通常比t-SNE<strong>速度更快</strong>，且可视化效果更好。</li>
</ul>
</li>
<li><strong>3. 等距映射 (Isometric Mapping, Isomap)</strong><ul>
<li><strong>核心思想：</strong> 尝试保持点之间的<strong>测地线距离</strong>（即沿着流形表面的最短距离），而不是欧氏距离（直线距离）。</li>
<li><strong>适用场景：</strong> 处理像“瑞士卷”这样被卷曲起来的数据集效果很好。</li>
</ul>
</li>
</ul>
<h3 id="总结：如何选择？-1"><a href="#总结：如何选择？-1" class="headerlink" title="总结：如何选择？"></a><strong>总结：如何选择？</strong></h3><table>
<thead>
<tr>
<th>类别</th>
<th>算法&#x2F;方法</th>
<th>核心思想</th>
<th>优点</th>
<th>主要应用场景</th>
</tr>
</thead>
<tbody><tr>
<td><strong>特征选择</strong></td>
<td><strong>过滤&#x2F;包装&#x2F;嵌入法</strong></td>
<td>挑选原始特征子集</td>
<td>可解释性强</td>
<td>当需要理解哪些原始特征最重要时</td>
</tr>
<tr>
<td><strong>线性提取</strong></td>
<td><strong>PCA (主成分分析)</strong></td>
<td>最大化方差</td>
<td><strong>通用、高效、稳定</strong></td>
<td><strong>通用的降维首选</strong>，数据去噪</td>
</tr>
<tr>
<td><strong>线性提取</strong></td>
<td><strong>LDA (线性判别分析)</strong></td>
<td>最大化类别分离度</td>
<td>(有监督) 降维后利于分类</td>
<td>分类任务的预处理</td>
</tr>
<tr>
<td><strong>非线性提取</strong></td>
<td><strong>t-SNE &#x2F; UMAP</strong></td>
<td>保持局部邻域结构</td>
<td><strong>可视化效果极佳</strong></td>
<td><strong>高维数据可视化</strong>，探索聚类结构</td>
</tr>
<tr>
<td><strong>非线性提取</strong></td>
<td><strong>Isomap</strong></td>
<td>保持测地线距离</td>
<td>能“展开”流形</td>
<td>学术研究，处理特定流形数据</td>
</tr>
</tbody></table>
<p><strong>一个简单的实践建议：</strong></p>
<ol>
<li><strong>不确定用什么？</strong> 从 <strong>PCA</strong> 开始。</li>
<li><strong>需要很强的可解释性？</strong> 尝试<strong>特征选择</strong>方法（如L1正则化或随机森林特征重要性）。</li>
<li><strong>主要目标是可视化？</strong> 使用 <strong>UMAP</strong> 或 <strong>t-SNE</strong>。</li>
<li><strong>怀疑数据有复杂的非线性结构？</strong> 可以探索Isomap等其他流形学习方法。</li>
</ol>
<h1 id="5-什么是数据的方差-信息量"><a href="#5-什么是数据的方差-信息量" class="headerlink" title="5.什么是数据的方差&#x2F;信息量"></a>5.什么是数据的方差&#x2F;信息量</h1><p>简单来说，<strong>方差（Variance）</strong> 在这里就是用来衡量您的某一个描述符（特征）在所有14091个材料中，<strong>数值分布的“离散程度”或“差异性大小”</strong>。</p>
<p>高方差 &#x3D; 差异性大 &#x3D; 信息量丰富 低方差 &#x3D; 差异性小 &#x3D; 信息量贫乏</p>
<hr>
<h3 id="让我们用一个具体的描述符来举例"><a href="#让我们用一个具体的描述符来举例" class="headerlink" title="让我们用一个具体的描述符来举例"></a>让我们用一个具体的描述符来举例</h3><p>在您那1559个特征中，我们假设有一个特征叫做 <strong>“密度 (Density)”</strong>。现在，我们只看这一列数据，它包含了14091个不同材料的密度值。</p>
<ul>
<li><p><strong>情况一：低方差 (Low Variance)</strong> 假设您计算出的所有14091个材料的密度值都<strong>非常接近</strong>，比如都在 <code>4.5 g/cm³</code> 到 <code>4.6 g/cm³</code> 这个极小的范围内。</p>
<p>在这种情况下，“密度”这个特征的<strong>方差就非常低</strong>。它告诉我们什么信息呢？它告诉我们，在“密度”这个属性上，您所有的材料都差不多，没什么区别。因此，这个特征对于区分不同材料、或者预测它们的性质，<strong>几乎没有帮助</strong>。它的“信息量”很贫乏。</p>
</li>
<li><p><strong>情况二：高方差 (High Variance)</strong> 现在，假设您的14091个材料的密度值<strong>分布得非常广泛</strong>，从很轻的 <code>1.0 g/cm³</code> 到非常重的 <code>22.0 g/cm³</code> 都有。</p>
<p>在这种情况下，“密度”这个特征的<strong>方差就非常高</strong>。它提供了一个非常有用的信息维度来区分您的材料。一个密度为2的材料和一个密度为20的材料，显然在物理性质上有着天壤之别。因此，这个特征<strong>信息量很丰富</strong>，对于后续的机器学习建模非常有价值。</p>
</li>
</ul>
<h3 id="从单个特征的方差到“总方差”"><a href="#从单个特征的方差到“总方差”" class="headerlink" title="从单个特征的方差到“总方差”"></a>从单个特征的方差到“总方差”</h3><p>现在，请想象一下，您的1559个特征（密度、平均键长、堆积因子、平均电负性…）中的<strong>每一个</strong>，都有自己独立的方差值。</p>
<ul>
<li>有些特征（像情况二的密度）方差很高，是很好的“区分器”。</li>
<li>有些特征（像情况一的密度）方差很低，是“噪音”或者说用处不大的信息。</li>
</ul>
<p><strong>总方差 (Total Variance)</strong> 就是把这<strong>所有1559个特征各自的方差简单地加在一起</strong>。 这个“总方差”就代表了您整个数据集在所有维度上<strong>信息量的总和</strong>。这就是我们之前比喻中的“所有原材料的总营养价值”。</p>
<h3 id="PCA是如何利用“方差”的？"><a href="#PCA是如何利用“方差”的？" class="headerlink" title="PCA是如何利用“方差”的？"></a>PCA是如何利用“方差”的？</h3><p>PCA的聪明之处在于，它不仅仅是看原始的1559个方向（密度、键长等）的方差，而是要在您那个1559维的复杂空间里，<strong>寻找到全新的、最优的方向</strong>。</p>
<ol>
<li><strong>主成分1 (PC1)<strong>：PCA找到的第一个新方向（PC1），是所有可能方向中，能让您的14091个数据点投影上去后</strong>分布得最开、差异性最大、方差最高</strong>的那个方向。它捕捉了整个数据集中<strong>最主要的变化来源</strong>。</li>
<li><strong>主成分2 (PC2)<strong>：在所有与PC1垂直的方向中，PCA找到能让数据投影后方差次高的那个方向。它捕捉了数据中</strong>第二重要的变化来源</strong>。</li>
<li>…以此类推。</li>
</ol>
<p>所以，当我们说“PC1解释了30%的方差”时，我们的意思就是： <strong>“沿着PC1这个新维度观察数据，其离散程度（方差）占到了整个数据集总离散程度（总方差）的30%。”</strong></p>
<p><strong>总结：</strong> 在您的数据中，“方差”衡量了<strong>一个描述符在区分不同材料方面的能力</strong>。PCA通过寻找数据中方差最大的新方向（主成分），来智能地将最有价值的信息（差异性）浓缩到少数几个维度中，从而实现高效、有意义的降维。</p>
<h1 id="6-聚类"><a href="#6-聚类" class="headerlink" title="6. 聚类"></a>6. 聚类</h1><p>聚类就是在降维后的高维空间中识别簇并划分</p>
<h1 id="7-DBSCAN聚类方法"><a href="#7-DBSCAN聚类方法" class="headerlink" title="7. DBSCAN聚类方法"></a>7. DBSCAN聚类方法</h1><p>两个重要参数</p>
<p>**<code>eps</code><strong>邻域半径社交距离 &#x2F; 一臂之长定义了“邻居”的范围有多大。</strong><code>min_samples</code>**核心点最小样本数形成核心圈子的最少人数定义了“多密集”才算是一个簇的核心。</p>
<h3 id="K距离图（K-distance-Graph）的含义"><a href="#K距离图（K-distance-Graph）的含义" class="headerlink" title="K距离图（K-distance Graph）的含义"></a>K距离图（K-distance Graph）的含义</h3><p>首先，我们再回顾一下这张图的含义：</p>
<ul>
<li><strong>Y轴</strong>：距离值。代表一个点要“走”多远，才能找到它的第 <code>min_samples</code> 个邻居。</li>
<li><strong>X轴</strong>：数据集中的所有点。为了方便观察，我们把这些点按照Y轴的距离值<strong>从小到大</strong>重新排了序。</li>
</ul>
<p>所以，这张图的曲线实际上展示了数据集中所有点的“<strong>局部密度</strong>”分布情况：</p>
<ul>
<li><strong>曲线左侧（Y值很低）</strong>：这些点很“幸福”，不用走多远就能找到足够多的邻居。这说明它们身处<strong>人口稠密的市中心（密集区域&#x2F;簇内）</strong>。</li>
<li><strong>曲线右侧（Y值很高）</strong>：这些点很“孤独”，需要走很远才能找到足够多的邻居。这说明它们身处<strong>人烟稀少的郊区或乡村（稀疏区域&#x2F;噪声）</strong>。</li>
</ul>
<h3 id="2-“拐点”（Elbow）的特殊意义"><a href="#2-“拐点”（Elbow）的特殊意义" class="headerlink" title="2. “拐点”（Elbow）的特殊意义"></a>2. “拐点”（Elbow）的特殊意义</h3><p>现在，我们来看“拐点”。</p>
<p>拐点是曲线上斜率<strong>变化最剧烈</strong>的地方。在这个点：</p>
<ul>
<li><strong>拐点之前</strong>：曲线很平缓。这代表了所有“市中心”的居民。虽然大家住的地方有好有坏，但总体上邻里距离都差不多，都比较近。</li>
<li><strong>拐点之后</strong>：曲线突然变得非常陡峭。这代表了那些“郊区”的居民。他们的邻里距离突然变得非常大。</li>
</ul>
<p>因此，这个<strong>拐点</strong>就理所当然地成为了<strong>区分“市中心居民”和“郊区居民”的最佳分界线</strong>。它就像是这个城市的“<strong>天然边界</strong>”。</p>
<h3 id="3-为什么这个边界就是-eps？"><a href="#3-为什么这个边界就是-eps？" class="headerlink" title="3. 为什么这个边界就是 eps？"></a>3. 为什么这个边界就是 <code>eps</code>？</h3><p>现在我们回到DBSCAN算法的目标：找到由<strong>核心点</strong>组成的簇。</p>
<p><strong>核心点的定义是</strong>：在 <code>eps</code> 半径内，至少有 <code>min_samples</code> 个邻居。</p>
<ul>
<li><strong>如果我们把 <code>eps</code> 设置为拐点的Y轴值</strong>（也就是那个临界距离）：<ul>
<li>对于所有在<strong>拐点左边</strong>的点，它们找到第 <code>min_samples</code> 个邻居的实际距离都<strong>小于或等于</strong>我们设定的 <code>eps</code>。这意味着，它们都满足成为<strong>核心点</strong>的条件！</li>
<li>对于所有在<strong>拐点右边</strong>的点，它们找到第 <code>min_samples</code> 个邻居的实际距离都<strong>大于</strong>我们设定的 <code>eps</code>。这意味着，它们无法满足成为核心点的条件，因此它们理应被当作<strong>噪声点</strong>。</li>
</ul>
</li>
</ul>
<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p><strong>拐点处的Y值，是那个能够“一刀切”地将大部分核心点和大部分噪声点分开的理想距离阈值。</strong></p>
<ul>
<li>选择一个比拐点<strong>低很多</strong>的<code>eps</code>，会导致连很多“市中心”的点都因为邻域太小而被误判为噪声。</li>
<li>选择一个比拐点<strong>高很多</strong>的<code>eps</code>，会导致把很多“郊区”的噪声点也因为邻域太大而被错误地圈进了城市里。</li>
</ul>
<p>因此，拐点是平衡这两者的最佳选择，它代表了我们能容忍的、构成一个“密集区域”的<strong>最大邻居距离</strong>。</p>
<h1 id="8-PCA与UMAP对比"><a href="#8-PCA与UMAP对比" class="headerlink" title="8. PCA与UMAP对比"></a>8. PCA与UMAP对比</h1><table>
<thead>
<tr>
<th>特性</th>
<th><strong>PCA (主成分分析)</strong></th>
<th align="left"><strong>UMAP (均匀流形近似与投影)</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心原理</strong></td>
<td><strong>线性</strong> (Linear)</td>
<td align="left"><strong>非线性 &#x2F; 流形学习</strong> (Non-linear)</td>
</tr>
<tr>
<td><strong>主要目标</strong></td>
<td><strong>最大化方差</strong> (Maximize Variance)</td>
<td align="left"><strong>保持拓扑结构</strong> (Preserve Topology)</td>
</tr>
<tr>
<td></td>
<td>(保留数据整体的“伸展”方向)</td>
<td align="left">(保持数据点之间的“邻里关系”)</td>
</tr>
<tr>
<td><strong>关注点</strong></td>
<td><strong>全局结构</strong> (Global Structure)</td>
<td align="left"><strong>局部结构</strong>为主，兼顾全局结构</td>
</tr>
<tr>
<td><strong>可解释性</strong></td>
<td><strong>强</strong> (High)</td>
<td align="left"><strong>弱</strong> (Low) &#x2F; 黑盒</td>
</tr>
<tr>
<td></td>
<td>(每个主成分都是原始特征的线性组合)</td>
<td align="left">(降维后的坐标轴没有直观物理解释)</td>
</tr>
<tr>
<td><strong>计算速度</strong></td>
<td><strong>非常快</strong> (Very Fast)</td>
<td align="left"><strong>比PCA慢，但比t-SNE快得多</strong> (Slower)</td>
</tr>
<tr>
<td><strong>主要应用</strong></td>
<td><strong>数据预处理、去噪、特征工程</strong></td>
<td align="left"><strong>数据可视化、探索性数据分析</strong></td>
</tr>
</tbody></table>
<h1 id="9-KDBSCAN中的参数"><a href="#9-KDBSCAN中的参数" class="headerlink" title="9. KDBSCAN中的参数"></a>9. KDBSCAN中的参数</h1><p>社交范围和“圈子定义”</p>
<p><code>DBSCAN</code> 中这两个至关重要的参数：<code>eps</code> 和 <code>min_samples</code>。</p>
<p><code>DBSCAN</code> 是一种基于<strong>密度</strong>的聚类算法，而这两个参数正是用来定义“密度”的。您可以把它们想象成是在照片中识别“人群”的两个规则。</p>
<hr>
<h3 id="一个生动的比喻：寻找派对上的社交圈"><a href="#一个生动的比喻：寻找派对上的社交圈" class="headerlink" title="一个生动的比喻：寻找派对上的社交圈"></a>一个生动的比喻：寻找派对上的社交圈</h3><p>想象一下，您正在分析一张大型派对的俯瞰照片，照片上有很多客人（您的数据点）。您的任务是找出哪些客人形成了<strong>“社交圈”（簇）</strong>，哪些是<strong>“独行者”（噪声点）</strong>。</p>
<h4 id="1-eps-Epsilon-邻域半径"><a href="#1-eps-Epsilon-邻域半径" class="headerlink" title="1. eps (Epsilon) - 邻域半径"></a>1. <code>eps</code> (Epsilon) - 邻域半径</h4><ul>
<li><strong>定义</strong>：<code>eps</code> 是一个距离值，它定义了一个点的“邻近”范围。可以把它想象成每个人的<strong>“个人空间”或“社交距离”</strong>。</li>
<li><strong>作用</strong>：对于任何一个客人，我们以他&#x2F;她为中心，画一个半径为 <code>eps</code> 的圆。所有落在这个圆里的其他客人，都算是他&#x2F;她的“邻居”。</li>
</ul>
<p><strong>参数调整的影响：</strong></p>
<ul>
<li><strong>增加 <code>eps</code> 值 (例如，从 <code>eps=10</code> 增加到 <code>eps=20</code>)</strong><ul>
<li><strong>效果</strong>：相当于扩大了每个人的“社交距离”。原来一些距离较远的圈子现在可能会因为彼此的“社交圈”重叠而<strong>合并成一个更大的圈子</strong>。</li>
<li><strong>结果</strong>：<ul>
<li>簇的数量会<strong>减少</strong>，但每个簇的规模会<strong>变大</strong>。</li>
<li>更多的点会被纳入到簇中，被识别为<strong>噪声点的数量会减少</strong>。</li>
<li><strong>极端情况</strong>：如果 <code>eps</code> 太大，所有点可能会被合并成一个巨大的簇。</li>
</ul>
</li>
</ul>
</li>
<li><strong>降低 <code>eps</code> 值 (例如，从 <code>eps=10</code> 降低到 <code>eps=5</code>)</strong><ul>
<li><strong>效果</strong>：相当于缩小了每个人的“社交距离”，要求大家站得更近才算邻居。</li>
<li><strong>结果</strong>：<ul>
<li>一个原本较大的、松散的圈子可能会因为内部不够紧密而<strong>分裂成几个更小的、更紧密的簇</strong>。</li>
<li>簇的数量可能会<strong>增加</strong>，但每个簇的规模会<strong>变小</strong>。</li>
<li>由于标准变严，很多点找不到足够多的邻居，被识别为<strong>噪声点的数量会增加</strong>。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-min-samples-核心点最小样本数"><a href="#2-min-samples-核心点最小样本数" class="headerlink" title="2. min_samples - 核心点最小样本数"></a>2. <code>min_samples</code> - 核心点最小样本数</h4><ul>
<li><strong>定义</strong>：<code>min_samples</code> 是一个整数，它定义了要形成一个“密集区域”（即一个簇的核心）所需要的<strong>最少邻居数量</strong>。</li>
<li><strong>作用</strong>：一个点如果在他自己的 <code>eps</code> 邻域内（包括他自己）拥有至少 <code>min_samples</code> 个点，那么这个点就被称作<strong>“核心点”</strong>。簇就是由这些核心点以及它们的邻居连接而成的。可以把它理解为“<strong>几个人才能算一个真正的社交圈</strong>”。</li>
</ul>
<p><strong>参数调整的影响：</strong></p>
<ul>
<li><strong>增加 <code>min_samples</code> 值 (例如，从 <code>min_samples=40</code> 增加到 <code>min_samples=80</code>)</strong><ul>
<li><strong>效果</strong>：相当于提高了形成“社交圈”的门槛。原来三四十个人就算一个圈子，现在必须凑够八十人才行。</li>
<li><strong>结果</strong>：<ul>
<li>只有<strong>密度非常高</strong>的区域才会被识别为簇。</li>
<li>一些原本被认为是簇的稀疏区域，现在可能会因为“人数”不够而被看作是<strong>噪声</strong>。</li>
<li><strong>噪声点的数量会增加</strong>。这个参数是控制噪声的主要手段。</li>
</ul>
</li>
</ul>
</li>
<li><strong>降低 <code>min_samples</code> 值 (例如，从 <code>min_samples=40</code> 降低到 <code>min_samples=20</code>)</strong><ul>
<li><strong>效果</strong>：相当于降低了形成“社交圈”的门槛。</li>
<li><strong>结果</strong>：<ul>
<li>即使在<strong>密度较低</strong>的区域也可能形成簇。</li>
<li>簇的数量可能会<strong>增加</strong>。</li>
<li><strong>噪声点的数量会减少</strong>。</li>
<li><strong>极端情况</strong>：如果 <code>min_samples</code> 太小（如1或2），几乎所有点都会成为核心点，可能会导致噪声点也被错误地划分进簇中。</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h3 id="总结表格"><a href="#总结表格" class="headerlink" title="总结表格"></a>总结表格</h3><table>
<thead>
<tr>
<th>参数</th>
<th>增加该值的影响</th>
<th>降低该值的影响</th>
</tr>
</thead>
<tbody><tr>
<td><strong><code>eps</code></strong> (社交距离)</td>
<td>簇变大、变少；噪声点变少</td>
<td>簇变小、变多；噪声点变多</td>
</tr>
<tr>
<td><strong><code>min_samples</code></strong> (成圈人数)</td>
<td>簇变少、更紧密；噪声点变多</td>
<td>簇变多、更松散；噪声点变少</td>
</tr>
</tbody></table>
<p>导出到 Google 表格</p>
<p><strong>您的设置 <code>eps=10, min_samples=40</code> 意味着：</strong> “对于任何一个材料点，如果在10个单位的距离内，总共有至少40个其他材料点，那么这里就形成了一个高密度的核心区域。”</p>
<p>调整这两个参数通常需要一些实验，通过观察我们之前讨论过的“按簇标签着色的散点图”来判断聚类效果是否符合您的预期。</p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="10-UMAP-HDBSCAN-or-PCA-HDBSCAN"><a href="#10-UMAP-HDBSCAN-or-PCA-HDBSCAN" class="headerlink" title="10. UMAP+HDBSCAN or PCA+HDBSCAN"></a>10. UMAP+HDBSCAN or PCA+HDBSCAN</h1><p>对于你当前的数据情况——<strong>10000个样本，1500个维度</strong>——我的建议是：</p>
<p><strong>优先使用 UMAP + HDBSCAN。</strong></p>
<p>下面是详细的理由和为你量身定制的操作策略。</p>
<hr>
<h3 id="为什么优先选择-UMAP-HDBSCAN？"><a href="#为什么优先选择-UMAP-HDBSCAN？" class="headerlink" title="为什么优先选择 UMAP + HDBSCAN？"></a><strong>为什么优先选择 UMAP + HDBSCAN？</strong></h3><ol>
<li><strong>高维数据的非线性本质</strong>:<ul>
<li>在一个高达1500维的空间里，数据点之间的关系极不可能是简单的线性关系。真实世界的高维数据（如图像特征、基因表达、用户行为向量等）几乎总是分布在某个复杂的、扭曲的低维“流形”上。</li>
<li>PCA是一个<strong>线性</strong>工具，它会试图用“直线”（超平面）来切割和投影数据。这对于捕捉非线性结构是无效的，很可能会将原本分离的簇错误地重叠在一起，从而“污染”数据，让后续的HDBSCAN无法工作。</li>
<li>UMAP专门为发现这种<strong>非线性</strong>流形结构而设计。它能够“解开”高维空间中缠绕的数据结构，在低维空间中更真实地保留数据点之间的邻近关系。这正是HDBSCAN这种基于密度的聚类算法最需要的信息。</li>
</ul>
</li>
<li><strong>“维度灾难”的完美解药</strong>:<ul>
<li>在1500维下，任何基于距离的算法（包括HDBSCAN）都会因为“维度灾难”而失效——所有点到其他点的距离都趋向于相等，密度变得没有意义。因此，降维是<strong>必须</strong>的步骤。</li>
<li>UMAP在降维时，其核心目标就是保留数据的<strong>拓扑结构</strong>（哪些点是邻居，哪些簇是独立的）。这与HDBSCAN的目标（寻找密度相连的区域）完美契合。可以说，UMAP为HDBSCAN创造了最理想的工作环境。</li>
</ul>
</li>
<li><strong>计算性能完全可接受</strong>:<ul>
<li>你的数据量是10000，这对于现代的UMAP实现来说是完全可以接受的。虽然UMAP会比PCA慢，但在这个数据量级上，计算时间很可能在几分钟之内，而不是几小时或几天。为了获得质量高得多的聚类结果，这点额外的时间开销是非常值得的。</li>
</ul>
</li>
</ol>

      
       <hr><span style="font-style: italic;color: gray;"> 转载请注明来源  有问题可通过github提交issue </span>
    </div>
</article>







    




    </div>
    <div class="copyright">
        <p class="footer-entry">
    ©2024 xaz
</p>
<p class="footer-entry">Built with <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/yelog/hexo-theme-3-hexo" target="_blank">3-hexo</a> theme</p>

    </div>
    <div class="full-toc">
        <button class="full" data-title="切换全屏 快捷键 s"><span class="min "></span></button>
<a class="" id="rocket" ></a>

    </div>
</div>

</body>
<script src="/js/jquery.pjax.js?v=1.1.0" ></script>

<script src="/js/script.js?v=1.1.0" ></script>
<script>
    var img_resize = 'default';
    function initArticle() {
        /*渲染对应的表格样式*/
        
            $("#post .pjax table").addClass("green_title");
        

        /*渲染打赏样式*/
        

        /*高亮代码块行号*/
        

        /*访问数量*/
        
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js");
        

        /*代码高亮，行号对齐*/
        $('.pre-numbering').css('line-height',$('.has-numbering').css('line-height'));

        
        
    }

    /*打赏页面隐藏与展示*/
    

</script>

<!--加入行号的高亮代码块样式-->

<!--自定义样式设置-->
<style>
    
    
    .nav {
        width: 582px;
    }
    .nav.fullscreen {
        margin-left: -582px;
    }
    .nav-left {
        width: 160px;
    }
    
    
    @media screen and (max-width: 1468px) {
        .nav {
            width: 552px;
        }
        .nav.fullscreen {
            margin-left: -552px;
        }
        .nav-left {
            width: 160px;
        }
    }
    
    
    @media screen and (max-width: 1024px) {
        .nav {
            width: 552px;
            margin-left: -552px
        }
        .nav.fullscreen {
            margin-left: 0;
        }
    }
    
    @media screen and (max-width: 426px) {
        .nav {
            width: 100%;
        }
        .nav-left {
            width: 100%;
        }
    }
    
    
    .nav-right .title-list nav a .post-title, .nav-right .title-list #local-search-result a .post-title {
        color: #383636;
    }
    
    
    .nav-right .title-list nav a .post-date, .nav-right .title-list #local-search-result a .post-date {
        color: #5e5e5f;
    }
    
    
    .nav-right nav a.hover, #local-search-result a.hover{
        background-color: #e2e0e0;
    }
    
    

    /*列表样式*/
    

    /* 背景图样式 */
    
    


    /*引用块样式*/
    

    /*文章列表背景图*/
    
    .nav-right:before {
        content: ' ';
        display: block;
        position: absolute;
        left: 0;
        top: 0;
        width: 100%;
        height: 100%;
        opacity: 0.3;
        background: url("https://i.loli.net/2019/07/22/5d3521411f3f169375.png");
        background-repeat: no-repeat;
        background-position: 50% 0;
        -ms-background-size: cover;
        -o-background-size: cover;
        -moz-background-size: cover;
        -webkit-background-size: cover;
        background-size: cover;
    }
    

    
    #post .pjax article :not(pre) > code {
        color: #24292e;
        font-family: SFMono-Regular,Consolas,Liberation Mono,Menlo,Courier,monospace;
        background-color: rgba(27,31,35,.05);
        border-radius: 3px;
        font-size: 85%;
        margin: 0;
        padding: .2em .4em;
    }
    
</style>







</html>
