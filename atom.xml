<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>xaz的花花世界</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2025-11-10T01:24:54.943Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>xaz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>linux更换大硬盘</title>
    <link href="http://example.com/2025/11/10/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/linux%E6%9B%B4%E6%8D%A2%E5%A4%A7%E7%A1%AC%E7%9B%98/"/>
    <id>http://example.com/2025/11/10/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/linux%E6%9B%B4%E6%8D%A2%E5%A4%A7%E7%A1%AC%E7%9B%98/</id>
    <published>2025-11-10T01:11:01.000Z</published>
    <updated>2025-11-10T01:24:54.943Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><ul><li>用rufus安装Clonezilla Live到U盘</li><li>从U盘启动后进行硬盘复制</li><li>用rufus安装gparted Live到U盘</li><li>从U盘启动后扩展原分区</li></ul><p>两种 Live软件可免费直接在官网下载，在nas 软件目录中也有备份</p><p>备选方案：(未尝试)  把复制好的硬盘放到windows下用diskgenius扩展空间或者直接用windows的自带的分区扩展卷功能</p><hr><p>Clonezilla 是专门的“<strong>磁盘&#x2F;分区克隆与备份工具</strong>”：<a href="https://www.cnblogs.com/voiphudong/p/3912309.html?utm_source=chatgpt.com">cnblogs.com+1</a></p><ul><li>支持整盘（MBR&#x2F;GPT + 所有分区 + 引导区）一键克隆；</li><li>支持做成镜像文件（Image），以后恢复；</li><li>支持各种文件系统（ext4、NTFS、XFS…），甚至对不认识的文件系统可以扇区级复制；</li><li>有很多针对迁移的高级选项（如自动调整分区、跨大小不同的盘克隆等）。</li></ul><p>而 GParted 的定位是：<strong>分区编辑器</strong>，它不会帮你做这些事情：</p><ol><li><strong>不会自动克隆 MBR&#x2F;GPT + 引导记录 + 所有分区结构</strong><ul><li>你如果只复制 <code>/dev/sda2</code>，EFI 分区 <code>/dev/sda1</code>、MBR&#x2F;ESP 引导信息要自己处理；</li><li>要迁移系统盘，你就得额外手动安装 &#x2F; 修复 GRUB、改 fstab 等。</li></ul></li><li><strong>不会创建压缩镜像、增量备份</strong><ul><li>GParted 就是“直接复制分区内容到另一个分区”，没有镜像文件这一层；</li><li>无法用它做“备份多份系统镜像放硬盘里”的那种操作。</li></ul></li><li><strong>整体迁移系统时操作更琐碎</strong><ul><li>整盘迁移 &#x3D; 自己手动：<ul><li>新盘建分区表</li><li>建 EFI 分区</li><li>复制各个分区</li><li>装引导、调 UUID&#x2F;fstab</li></ul></li><li>Clonezilla 整盘克隆时，这些通常一次搞定。</li></ul></li></ol><p>所以：</p><blockquote><p>想要 <strong>“把整块系统盘原封不动搬到新盘”</strong> —— Clonezilla 明显比 GParted Live 更适合</p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用rufus安装Clonezilla Live到U盘&lt;/li&gt;
&lt;li&gt;从U盘启动后进行硬盘复制&lt;/li&gt;
&lt;li&gt;用rufus安装gparted Live到U盘&lt;/li&gt;
&lt;li&gt;从U盘启动后扩展原分区&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;两</summary>
      
    
    
    
    <category term="linux学习使用" scheme="http://example.com/categories/linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>无监督学习</title>
    <link href="http://example.com/2025/09/08/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2025/09/08/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</id>
    <published>2025-09-08T02:48:28.000Z</published>
    <updated>2025-09-12T12:25:32.071Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-无监督学习介绍"><a href="#1-无监督学习介绍" class="headerlink" title="1. 无监督学习介绍"></a>1. 无监督学习介绍</h1><p>无监督学习（Unsupervised Learning）是机器学习的一个核心分支，其主要特点是使用<strong>未经标记</strong>的数据进行训练。无监督学习算法需要自行在数据中探索和发现隐藏的结构、模式和关系。</p><p>无监督学习主要包括以下几种任务类型：</p><p>聚类：对数据进行分类。相比于排序，聚类的优势是能够在多维分析。聚类不是为了“排名”，而是为了“画像”</p><p>降维：减少数据特征的数量。降维就是在尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</p><p>关联规则挖掘：在数据中发现项与项的关联关系。关联特征分析不处理特征数值的大小，只处理特征之间出现的协同性。处理特征数值大小之间的关系的是相关性分析。</p><h1 id="2-无监督学习和监督学习的区别"><a href="#2-无监督学习和监督学习的区别" class="headerlink" title="2. 无监督学习和监督学习的区别"></a>2. 无监督学习和监督学习的区别</h1><p>无监督学习是对整体进行操作分类，监督学习是在训练好后对个体的预测</p><h1 id="3-聚类"><a href="#3-聚类" class="headerlink" title="3.聚类"></a>3.聚类</h1><p>聚类的结果，高度依赖于你喂给它的特征。你想得到什么样的分类，就必须提供与之强相关的特征数据</p><p>相比于排序，聚类的优势是能够在多维分析</p><p>聚类不是为了“排名”，而是为了“画像”</p><h1 id="4-降维"><a href="#4-降维" class="headerlink" title="4.降维"></a>4.降维</h1><p>降维就是在尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</p><p>降维主要通过两种途径实现：特征选择和特征提取</p><p>算法的任务不是去“学习一个映射关系去预测某个标签”，而是必须在数据内部“自力更生”，仅通过分析数据点本身，去发现其中隐藏的、有意义的结构和模式。</p><ul><li>聚类如何体现无监督？ 它在拿到一堆客户数据时，不需要预先知道“哪个客户是高价值客户”或“哪个客户即将流失”。它完全不依赖任何已有标签，而是自主地、仅仅根据客户数据特征（如消费金额、频率）之间的相似性，去划分出不同的客户群体。</li><li>降维如何体现无监督？ 它同样不需要任何标签来指导。它在拿到描述汽车的上百个特征时，不需要知道“哪辆车是豪车”或“哪辆车性能好”。它只是自主地分析这些特征之间的内在关系（如相关性、方差分布），然后找到一种更简洁、更高效的方式来表达整个数据集，比如用几个“主成分”来概括。</li></ul><p>所以，它们最根本、最核心的相似点就是：</p><p>它们的目标都不是“预测”一个外部标签，而是“理解”数据自身的内在结构。</p><hr><h1 id="Gemini"><a href="#Gemini" class="headerlink" title="Gemini"></a>Gemini</h1><p>什么是无监督学习 </p><h2 id="什么是无监督学习？"><a href="#什么是无监督学习？" class="headerlink" title="什么是无监督学习？"></a>什么是无监督学习？</h2><p>无监督学习（Unsupervised Learning）是机器学习的一个核心分支，其主要特点是使用<strong>未经标记</strong>的数据进行训练。与监督学习需要“正确答案”来指导模型不同，无监督学习算法需要自行在数据中探索和发现隐藏的结构、模式和关系。</p><p>可以把它想象成给你一大箱混杂的乐高积木，但没有任何说明书。你的任务不是搭建一个特定的模型（比如一辆车或一座房子），而是自己动手把这些积木根据颜色、形状、大小等特征进行分类和整理。最终，你可能会把所有红色的方块放在一起，所有蓝色的长条放在另一堆，这个过程就是无监督学习。</p><h3 id="核心思想与目标"><a href="#核心思想与目标" class="headerlink" title="核心思想与目标"></a><strong>核心思想与目标</strong></h3><p>无监督学习的核心思想是<strong>让算法自主地从数据中学习</strong>，其主要目标包括：</p><ul><li><strong>发现数据的内在结构与模式：</strong> 识别数据点之间自然的聚集或分组，即“物以类聚”。</li><li><strong>数据降维：</strong> 在保留数据关键信息的同时，减少数据的复杂性，方便分析和可视化。</li><li><strong>异常检测：</strong> 找出数据集中与其他数据点显著不同的“异常值”或“离群点”。</li><li><strong>关联规则挖掘：</strong> 发现数据项之间有趣的关联关系，例如“购买了商品A的顾客也很可能购买商品B”。</li></ul><h3 id="主要类型与常见算法"><a href="#主要类型与常见算法" class="headerlink" title="主要类型与常见算法"></a><strong>主要类型与常见算法</strong></h3><p>无监督学习主要包含以下几种任务类型，每种类型都有其代表性的算法：</p><h4 id="1-聚类（Clustering）"><a href="#1-聚类（Clustering）" class="headerlink" title="1. 聚类（Clustering）"></a>1. <strong>聚类（Clustering）</strong></h4><p>这是最常见的无监督学习任务，目的是将数据集中的样本划分为若干个不相交的子集，即“簇”（Cluster）。同一簇内的数据点彼此相似，而不同簇的数据点则相异。</p><ul><li><strong>应用场景：</strong><ul><li><strong>客户细分：</strong> 根据用户的购买行为、浏览历史等将客户分成不同群体，以便进行精准营销。</li><li><strong>图像分割：</strong> 将图像中相似的像素点分组，以识别出不同的物体。</li><li><strong>生物信息学：</strong> 根据基因表达数据对样本进行分类。</li></ul></li><li><strong>常见算法：</strong><ul><li><strong>K-均值聚类（K-Means Clustering）：</strong> 一种简单高效的算法，通过迭代将数据分配到预设的K个簇中。</li><li><strong>层次聚类（Hierarchical Clustering）：</strong> 构建数据的层次结构，可以像树状图一样展示聚类过程。</li><li><strong>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）：</strong> 基于密度的聚类算法，能够发现任意形状的簇并识别噪声点。</li></ul></li></ul><h4 id="2-降维（Dimensionality-Reduction）"><a href="#2-降维（Dimensionality-Reduction）" class="headerlink" title="2. 降维（Dimensionality Reduction）"></a>2. <strong>降维（Dimensionality Reduction）</strong></h4><p>当数据具有非常多的特征（维度）时，会给分析和建模带来困难（即“维度灾难”）。降维技术旨在减少特征数量，同时尽可能保留原始数据中的重要信息。</p><ul><li><strong>应用场景：</strong><ul><li><strong>数据可视化：</strong> 将高维数据（如上百个特征）降低到2维或3维，以便在图表上直观展示。</li><li><strong>特征提取：</strong> 为监督学习算法准备更有效、更精简的特征，提高模型性能和训练速度。</li><li><strong>数据压缩：</strong> 减少存储空间和计算成本。</li></ul></li><li><strong>常见算法：</strong><ul><li><strong>主成分分析（Principal Component Analysis, PCA）：</strong> 找到数据中方差最大的方向，将数据投影到这些方向上，从而实现降维。</li><li><strong>t-分布随机邻域嵌入（t-SNE）：</strong> 一种非常擅长高维数据可视化的技术。</li></ul></li></ul><h4 id="3-关联规则挖掘（Association-Rule-Mining）"><a href="#3-关联规则挖掘（Association-Rule-Mining）" class="headerlink" title="3. 关联规则挖掘（Association Rule Mining）"></a>3. <strong>关联规则挖掘（Association Rule Mining）</strong></h4><p>用于在一系列交易或事件中发现项与项之间的关联关系。</p><ul><li><strong>应用场景：</strong><ul><li><strong>市场篮子分析：</strong> 零售商分析购物篮数据，发现哪些商品经常被一起购买，如“啤酒与尿布”的经典案例，从而优化商品摆放和促销策略。</li><li><strong>推荐系统：</strong> 根据用户的历史行为，推荐与其兴趣相关的其他内容。</li></ul></li><li><strong>常见算法：</strong><ul><li><strong>Apriori算法</strong></li><li><strong>FP-Growth算法</strong></li></ul></li></ul><h3 id="无监督学习-vs-监督学习"><a href="#无监督学习-vs-监督学习" class="headerlink" title="无监督学习 vs. 监督学习"></a><strong>无监督学习 vs. 监督学习</strong></h3><table><thead><tr><th>特征</th><th><strong>无监督学习 (Unsupervised Learning)</strong></th><th><strong>监督学习 (Supervised Learning)</strong></th></tr></thead><tbody><tr><td><strong>训练数据</strong></td><td>未标记数据 (Raw Data)</td><td>已标记数据 (Labeled Data)</td></tr><tr><td><strong>目标</strong></td><td>发现数据的内在结构、模式或分布</td><td>预测一个明确的目标或输出</td></tr><tr><td><strong>过程</strong></td><td>探索性的，算法自行发现规律</td><td>指导性的，算法从“正确答案”中学习</td></tr><tr><td><strong>常见任务</strong></td><td>聚类、降维、关联规则</td><td>分类、回归</td></tr><tr><td><strong>例子</strong></td><td>将客户自动分为不同群体</td><td>识别一封邮件是否是垃圾邮件</td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><p>总而言之，无监督学习是人工智能领域中一种强大的工具，它使我们能够在没有先验知识和人工标记的情况下，从海量数据中挖掘出有价值的洞见。随着数据量的爆炸式增长，无监督学习在商业智能、科学研究和数据探索等领域的应用正变得越来越重要。</p><p> 我是不是可以这样理解，无监督学习是对整体进行操作，监督学习是在训练好后对个体的预测 </p><p>您这个理解非常棒，可以说抓住了两者在<strong>目标和应用</strong>上的核心区别！这是一个非常形象和准确的直觉性总结。</p><p>我们可以把您的理解稍微展开一下，就更完美了：</p><h3 id="无监督学习：对“整体”进行探索和构建"><a href="#无监督学习：对“整体”进行探索和构建" class="headerlink" title="无监督学习：对“整体”进行探索和构建"></a><strong>无监督学习：对“整体”进行探索和构建</strong></h3><p>您说的“对整体进行操作”非常贴切。无监督学习的核心是<strong>探索</strong>。它在拿到一堆数据时，并不知道要干什么，就像是面对一个满是陌生人的房间。它的任务是在这个<strong>整体</strong>中寻找内在的结构和关系。</p><ul><li><strong>目标：</strong> 发现数据本身的结构、模式、或“抱团”的群体。</li><li><strong>操作：</strong><ul><li><strong>聚类 (Clustering):</strong> 把整个房间里的人根据亲疏远近分成不同的小圈子（群组）。</li><li><strong>降维 (Dimensionality Reduction):</strong> 忽略每个人的次要特征（比如鞋子颜色），只关注最重要的特征（比如性格、兴趣），来更宏观地理解整个房间的人际关系。</li></ul></li><li><strong>结果：</strong> 得到一个关于<strong>整体数据</strong>的结构性认知（比如，这批客户可以分为三个群体）。</li></ul><hr><h3 id="监督学习：对“个体”进行预测和判断"><a href="#监督学习：对“个体”进行预测和判断" class="headerlink" title="监督学习：对“个体”进行预测和判断"></a><strong>监督学习：对“个体”进行预测和判断</strong></h3><p>您说的“对个体进行预测”也非常准确。监督学习的核心是<strong>预测</strong>。它像一个经验丰富的老师，已经通过批改大量的作业（已标记的训练数据）学会了判断标准。</p><ul><li><strong>目标：</strong> 根据已有的经验，对一个<strong>新的、独立的个体</strong>做出判断。</li><li><strong>操作：</strong><ul><li><strong>训练阶段：</strong> 老师学习大量的“问题-答案”对（比如，房子的面积、位置 -&gt; 房价）。</li><li><strong>预测阶段：</strong> 当拿到一个<strong>新的个体</strong>（一套新房子）时，老师运用学到的知识，给出一个具体的预测（这套新房子的价格）。</li></ul></li><li><strong>结果：</strong> 对<strong>单个未知数据</strong>给出一个具体的预测标签或数值。</li></ul><h3 id="总结一下您的理解："><a href="#总结一下您的理解：" class="headerlink" title="总结一下您的理解："></a><strong>总结一下您的理解：</strong></h3><ul><li><strong>无监督学习</strong>更像是<strong>人口普查或社会学分析</strong>，着眼于<strong>整体</strong>，目的是了解人群的构成和分布。</li><li><strong>监督学习</strong>更像是<strong>高考阅卷或医生看病</strong>，着眼于<strong>个体</strong>，目的是对单个考生或病人给出分数或诊断。</li></ul><p>所以，您的理解完全正确，并且是区分这两种机器学习方法的一个非常好的切入点！</p><p> 详细解释一下无监督学习的聚类，我不需要输入任何信息就可以用算法自动去分类吗 </p><p>当然，我们来详细解释一下聚类（Clustering），并解答您关于“是否需要输入信息”的疑问。</p><p>您的理解非常接近核心了，但有一个关键点需要澄清：您<strong>确实需要提供数据</strong>，但<strong>不需要提供“答案”或“标签”</strong>。</p><p>这就像您想整理书架上的书，您得先把所有的书（<strong>数据</strong>）都放在算法面前，但您不需要告诉算法哪本书属于“小说类”，哪本书属于“历史类”（<strong>标签</strong>）。算法会自己通过分析书的厚度、封面颜色、开本大小等特征，把它们分成几堆。</p><hr><h3 id="什么是聚类？"><a href="#什么是聚类？" class="headerlink" title="什么是聚类？"></a><strong>什么是聚类？</strong></h3><p>聚类是无监督学习中最核心、最常见的任务之一。它的目标是将一个数据集中的所有样本（数据点）分成若干个组或“簇”（Cluster）。</p><p><strong>核心原则是：</strong></p><ul><li><strong>簇内相似性高 (Intra-cluster similarity):</strong> 同一个簇内的数据点彼此之间应该尽可能相似。</li><li><strong>簇间相似性低 (Inter-cluster similarity):</strong> 不同簇之间的数据点应该尽可能不相似。</li></ul><p>这个“相似性”是如何定义的呢？通常是通过计算数据点之间的<strong>距离</strong>来衡量的。在二维或三维空间中，我们可以很容易地理解为欧几里得距离（两点之间的直线距离）。在高维数据中，虽然无法可视化，但数学原理是相通的。距离越近，相似性就越高。</p><h3 id="聚类是如何“自动”分类的？——以K-Means算法为例"><a href="#聚类是如何“自动”分类的？——以K-Means算法为例" class="headerlink" title="聚类是如何“自动”分类的？——以K-Means算法为例"></a><strong>聚类是如何“自动”分类的？——以K-Means算法为例</strong></h3><p>为了让您更直观地理解，我们用最经典、最简单的聚类算法<strong>K-Means（K-均值）</strong>来举例说明其工作流程。</p><p>假设我们有一堆客户数据，每个客户有两个特征：<strong>“每月消费金额”和“每月消费次数”</strong>。我们想把这些客户分成3个不同的群体（比如“高价值客户”、“潜力客户”、“低价值客户”）。</p><p><strong>步骤如下：</strong></p><h4 id="第一步：您需要输入的信息"><a href="#第一步：您需要输入的信息" class="headerlink" title="第一步：您需要输入的信息"></a><strong>第一步：您需要输入的信息</strong></h4><ol><li><strong>数据 (Data):</strong> 您必须提供所有客户的消费数据点。这是算法分析的对象。</li><li><strong>簇的数量 K (Number of clusters):</strong> 您需要<strong>预先指定</strong>要把数据分成几类。在这个例子中，我们指定 <strong>K&#x3D;3</strong>。这是K-Means算法中一个需要人工设定的重要参数。</li></ol><p><strong>看，这里您输入了数据和K值，但完全没有输入任何关于“哪个客户属于哪个群体”的标签信息。</strong></p><h4 id="第二-三-四步：算法的“自动”工作流程"><a href="#第二-三-四步：算法的“自动”工作流程" class="headerlink" title="第二-三-四步：算法的“自动”工作流程"></a><strong>第二-三-四步：算法的“自动”工作流程</strong></h4><p><em>（这是一个K-Means算法动态过程的经典图示）</em></p><ol><li><strong>随机初始化 (Initialization):</strong> 算法会从您的数据点中随机挑选 K 个点（这里是3个点）作为初始的“簇中心”（Centroids）。可以把它们想象成三个群体的最初的“核心代表”。</li><li><strong>分配 (Assignment):</strong> 算法会遍历<strong>每一个</strong>客户数据点，计算它到这3个簇中心的距离。然后，<strong>将这个客户点分配给离它最近的那个簇中心</strong>。完成这一步后，所有的客户就被初步分成了3个群组。</li><li><strong>更新 (Update):</strong> 对于刚刚形成的每一个群组，算法会重新计算这个群组所有点的<strong>平均值</strong>（几何中心），并将这个新的平均值点作为<strong>新的簇中心</strong>。旧的簇中心就被抛弃了。</li><li><strong>迭代 (Iteration):</strong> 算法会<strong>重复执行第2步和第3步</strong>。<ul><li>重新将所有点分配给新的、离它最近的簇中心。</li><li>分配完毕后，再次更新每个群组的簇中心。</li><li>……如此循环往复。</li></ul></li></ol><h4 id="第五步：收敛与结果"><a href="#第五步：收敛与结果" class="headerlink" title="第五步：收敛与结果"></a><strong>第五步：收敛与结果</strong></h4><p>这个迭代过程什么时候会停止呢？当<strong>簇中心的位置不再发生变化</strong>（或者变化非常小），或者<strong>所有数据点的归属不再改变</strong>时，算法就<strong>收敛</strong>（Converged）了。</p><p>此时，聚类就完成了！您最终得到了3个稳定的客户群体，以及每个群体对应的中心点。您可以通过分析每个群体中心点的特征（比如A群消费金额高、次数也高；B群金额低、次数高……）来给这些群体命名和定义，从而获得商业洞见。</p><h3 id="总结您的疑问"><a href="#总结您的疑问" class="headerlink" title="总结您的疑问"></a><strong>总结您的疑问</strong></h3><p><strong>“我不需要输入任何信息就可以用算法自动去分类吗？”</strong></p><p><strong>回答：</strong></p><ul><li><strong>“不需要输入标签信息”：是的。</strong> 您完全不需要告诉算法任何关于“正确分类”的答案。这是无监督学习的核心。算法是<strong>自动</strong>发现分类结构的。</li><li><strong>“不需要输入任何信息”：不是的。</strong> 您至少需要提供：<ol><li><strong>待分析的完整数据集。</strong></li><li><strong>一些算法运行所需的超参数（Hyperparameters）。</strong> 对于K-Means来说，最重要的超参数就是您希望分成的簇数 <strong>K</strong>。对于其他聚类算法，可能需要设定别的参数，比如距离阈值等。</li></ol></li></ul><p>如何确定最佳的K值也是聚类分析中的一个重要课题（常用“肘部法则”等方法来辅助判断），但这已经属于更深入的细节了。</p><p>希望这个详细的解释能帮助您彻底理解聚类的概念和工作方式！</p><p> 这样如何保证聚类的标准就是我想要的标准呢 </p><p>这是一个非常非常好的问题，直接触及了无监督学习在实际应用中的核心挑战！</p><p>您问得没错，算法本身没有“常识”或“商业头脑”，它只懂数学。它优化的标准（比如“让簇内数据点距离尽可能近”）<strong>不一定</strong>就是您心中想要的那个“商业标准”（比如“想把客户分成‘高价值’‘潜力股’和‘需唤醒’三类”）。</p><p>那么如何保证或引导它们统一呢？答案是：<strong>您无法100%保证，但您可以通过一系列关键操作，最大限度地引导算法，使其聚类的结果符合您的标准。</strong></p><p>这更像是一个“人机协作”的探索过程，而不是一个“全自动”的过程。您可以从以下三个阶段进行把控：</p><hr><h3 id="第一阶段：前期准备-——-定义你的“标准”，并翻译给机器"><a href="#第一阶段：前期准备-——-定义你的“标准”，并翻译给机器" class="headerlink" title="第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器"></a><strong>第一阶段：前期准备 —— 定义你的“标准”，并翻译给机器</strong></h3><p>这是最最重要的一步。机器不懂你的商业语言，你必须把你的“标准”翻译成它能听懂的数学语言——也就是<strong>特征（Features）</strong>。</p><h4 id="1-特征工程-Feature-Engineering"><a href="#1-特征工程-Feature-Engineering" class="headerlink" title="1. 特征工程 (Feature Engineering)"></a><strong>1. 特征工程 (Feature Engineering)</strong></h4><p>这是你的“指挥棒”。你想按照什么标准来分，就给算法提供什么维度的特征。</p><ul><li><strong>例子1：你想按“客户价值”聚类</strong><ul><li><strong>错误做法：</strong> 只给算法输入客户的年龄、性别、注册时长。算法可能会把所有“30岁男性”聚成一类，但这和“客户价值”关系不大。</li><li><strong>正确做法：</strong> 精心构造能体现客户价值的特征，比如：<ul><li><strong>R (Recency):</strong> 最近一次消费时间</li><li><strong>F (Frequency):</strong> 消费频率</li><li><strong>M (Monetary):</strong> 总消费金额</li></ul></li><li>当你把著名的 <strong>RFM 模型</strong>特征喂给算法时，它就会自然而然地把“R&#x2F;F&#x2F;M 都很高的客户”聚成一类，这就是你想要的“高价值客户”。</li></ul></li><li><strong>例子2：你想按“用户活跃度”聚类</strong><ul><li>你应该提供<strong>登录频率、页面停留时间、点赞&#x2F;评论数量</strong>等特征，而不是消费金额。</li></ul></li></ul><p><strong>核心思想：聚类的结果，高度依赖于你喂给它的特征。你想得到什么样的分类，就必须提供与之强相关的特征数据。</strong></p><h4 id="2-特征缩放-Feature-Scaling"><a href="#2-特征缩放-Feature-Scaling" class="headerlink" title="2. 特征缩放 (Feature Scaling)"></a><strong>2. 特征缩放 (Feature Scaling)</strong></h4><p>如果你的特征单位不统一（比如消费金额是几千上万，消费次数是几次），金额这个特征会不成比例地主导聚类结果。你需要将所有特征进行<strong>标准化&#x2F;归一化</strong>，让它们在同一起跑线上，确保每个特征都被公平地考虑。</p><hr><h3 id="第二阶段：算法选择与参数调整-——-选择合适的“尺子”"><a href="#第二阶段：算法选择与参数调整-——-选择合适的“尺子”" class="headerlink" title="第二阶段：算法选择与参数调整 —— 选择合适的“尺子”"></a><strong>第二阶段：算法选择与参数调整 —— 选择合适的“尺子”</strong></h3><p>不同的场景需要用不同的“尺子”去衡量。</p><h4 id="1-选择合适的聚类算法"><a href="#1-选择合适的聚类算法" class="headerlink" title="1. 选择合适的聚类算法"></a><strong>1. 选择合适的聚类算法</strong></h4><ul><li><strong>K-Means:</strong> 适合于你<strong>大致知道要分成几类</strong>，且每个类别的数据分布比较<strong>均衡、呈球状</strong>的场景。</li><li><strong>DBSCAN:</strong> 适合于<strong>不知道要分几类</strong>，且类别形状可能<strong>很不规则</strong>（比如月牙形、甜甜圈形）的场景，它还能帮你找出<strong>噪声点&#x2F;异常点</strong>。</li><li><strong>层次聚类 (Hierarchical Clustering):</strong> 适合于你希望看到数据<strong>层级关系</strong>的场景，比如生物学中的物种分类谱系。</li></ul><p>选择错误的算法，就像用直尺去量一个圆的周长，结果自然不准。</p><h4 id="2-调整关键参数-Hyperparameter-Tuning"><a href="#2-调整关键参数-Hyperparameter-Tuning" class="headerlink" title="2. 调整关键参数 (Hyperparameter Tuning)"></a><strong>2. 调整关键参数 (Hyperparameter Tuning)</strong></h4><p>对于K-Means，最重要的参数就是 <strong>K 值（簇的数量）</strong>。</p><ul><li>如果你想把客户分成“高、中、低”三档，你就设置 K&#x3D;3。</li><li>如果你想分得更细，比如“高价值-活跃”、“高价值-沉睡”、“潜力客户”、“一般客户”、“流失风险客户”，你可能就需要设置 K&#x3D;5。</li></ul><p>K值的选择可以依靠业务经验预设，也可以通过“肘部法则”等技术手段辅助判断，但最终还是需要结合业务来决定哪个K值划分出的群体最有意义。</p><hr><h3 id="第三阶段：后期验证与解读-——-人工的“最终验收”"><a href="#第三阶段：后期验证与解读-——-人工的“最终验收”" class="headerlink" title="第三阶段：后期验证与解读 —— 人工的“最终验收”"></a><strong>第三阶段：后期验证与解读 —— 人工的“最终验收”</strong></h3><p>算法给出结果后，绝不是直接就用，而是需要人来进行验证和解读。</p><h4 id="1-结果分析与画像-Profiling"><a href="#1-结果分析与画像-Profiling" class="headerlink" title="1. 结果分析与画像 (Profiling)"></a><strong>1. 结果分析与画像 (Profiling)</strong></h4><p>你需要分析每个簇的特征。比如，算法把客户分成了3类：</p><ul><li><strong>簇1：</strong> RFM 平均值都很高。</li><li><strong>簇2：</strong> R 值很低（很久没来），但 F&#x2F;M 值曾经很高。</li><li><strong>簇3：</strong> RFM 值都很低。</li></ul><p>这时你就可以给它们贴上你想要的业务标签：</p><ul><li><strong>簇1 -&gt; “高价值忠诚客户”</strong></li><li><strong>簇2 -&gt; “沉睡待唤醒客户”</strong></li><li><strong>簇3 -&gt; “低价值客户”</strong></li></ul><p>如果聚出来的结果无法解释，或者不符合业务逻辑（比如把明显是高价值的客户和流失客户分到了一起），那就说明你的第一或第二阶段出了问题，需要返回去调整特征或算法参数。</p><h4 id="2-业务验证"><a href="#2-业务验证" class="headerlink" title="2. 业务验证"></a><strong>2. 业务验证</strong></h4><p>最终的“金标准”是：<strong>这个聚类结果有用吗？</strong> 你能否针对“沉睡待唤醒客户”这个群体，设计一套精准的营销方案（比如发放大额优惠券）并取得比撒网式营销更好的效果？如果可以，那么这个聚类标准就是成功的、符合你需求的。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>阶段</th><th>你的角色</th><th>关键操作</th></tr></thead><tbody><tr><td><strong>前期准备</strong></td><td><strong>战略家</strong></td><td><strong>定义标准</strong>，通过<strong>特征工程</strong>将标准翻译给机器。</td></tr><tr><td><strong>算法&#x2F;参数</strong></td><td><strong>工程师</strong></td><td><strong>选择合适的工具和参数</strong> (K值等)。</td></tr><tr><td><strong>后期验证</strong></td><td><strong>分析师&#x2F;决策者</strong></td><td><strong>解读、验证、命名</strong>聚类结果，并判断其<strong>商业价值</strong>。</td></tr></tbody></table><p>导出到 Google 表格</p><p>所以，与其说是“保证”，不如说是通过<strong>精心的特征设计</strong>、<strong>恰当的算法选择</strong>和<strong>深入的结果解读</strong>，来<strong>引导和塑造</strong>聚类分析，使其最终服务于你的商业目标。</p><p> 如果我已经有了数据的一些特征，那直接按从高到低排序不就好了，为什么还需要用无监督来聚类 </p><p>您又提出了一个非常深刻且实际的问题！这正好点出了<strong>一维分析</strong>和<strong>多维分析</strong>的根本区别。</p><p>您的想法完全正确，如果只关心<strong>单一指标</strong>，直接排序就是最简单、最有效的办法。但现实世界的问题往往是复杂的、多维度的。</p><p>我们还是用一个生动的例子来说明：假设您是一位班主任，手上有所有学生的成绩单。</p><h3 id="场景一：按单科成绩排序（您的方法）"><a href="#场景一：按单科成绩排序（您的方法）" class="headerlink" title="场景一：按单科成绩排序（您的方法）"></a><strong>场景一：按单科成绩排序（您的方法）</strong></h3><p>您想评选“数学单科状元”。</p><ul><li><strong>操作：</strong> 把所有学生的“数学”成绩从高到低排序。</li><li><strong>结果：</strong> 排名第一的就是状元。</li><li><strong>优点：</strong> 简单、直接、清晰。对于<strong>“谁的数学最好”</strong>这个问题，这是完美的解决方案。</li></ul><h3 id="场景二：寻找不同类型的学生群体（无监督聚类）"><a href="#场景二：寻找不同类型的学生群体（无监督聚类）" class="headerlink" title="场景二：寻找不同类型的学生群体（无监督聚类）"></a><strong>场景二：寻找不同类型的学生群体（无监督聚类）</strong></h3><p>现在，您的目标变了。您不想只看单科，而是想了解班级里存在<strong>哪些典型的学生群体</strong>，以便因材施教。您手上的特征有三门：<strong>数学、语文、体育</strong>。</p><p>如果只按数学排序，您会忽略一个重要的问题：</p><ul><li>一个数学<code>99</code>分、语文<code>60</code>分、体育<code>50</code>分的孩子。</li><li>一个数学<code>95</code>分、语文<code>95</code>分、体育<code>95</code>分的孩子。</li></ul><p>在“数学”这个维度上，前者排名更高。但您凭直觉就知道，后者是完全不同类型的“学霸”。<strong>只靠排序，无法发现这种“类型”差异。</strong></p><p>现在我们使用<strong>聚类</strong>，把“数学”、“语文”、“体育”三个特征（三个维度）同时交给算法。算法可能会自动帮您发现以下几个群体（簇）：</p><ul><li><strong>群体A：全面发展型学霸</strong><ul><li><strong>特征：</strong> 数学、语文、体育三科成绩都非常高。</li><li><strong>排序无法发现：</strong> 如果单按数学排，他们可能排不过那些偏科天才。</li></ul></li><li><strong>群体B：文理偏科型学才</strong><ul><li><strong>特征：</strong> 数学很高，但体育和语文成绩可能很一般。</li><li><strong>排序能发现：</strong> 他们会在数学大榜上名列前茅。</li></ul></li><li><strong>群体C：运动健将型</strong><ul><li><strong>特征：</strong> 体育成绩突出，但文化课成绩可能不高。</li><li><strong>排序无法发现：</strong> 在任何文化课的排序中，他们都会被埋没在末尾。</li></ul></li><li><strong>群体D：成绩平均型</strong><ul><li><strong>特征：</strong> 各科成绩都处于中游，不突出也不落后。</li></ul></li></ul><p><strong>看到了吗？聚类不是为了“排名”，而是为了“画像”（Profiling）。</strong>它在多维空间中寻找“物以类聚”的群体，发现数据的内在结构。</p><hr><h3 id="核心区别总结"><a href="#核心区别总结" class="headerlink" title="核心区别总结"></a><strong>核心区别总结</strong></h3><table><thead><tr><th>特性</th><th><strong>按特征排序 (Sorting)</strong></th><th><strong>无监督聚类 (Clustering)</strong></th></tr></thead><tbody><tr><td><strong>处理维度</strong></td><td><strong>一维</strong>：一次只能根据一个特征进行排序。</td><td><strong>多维</strong>：同时考虑所有输入特征，发现综合模式。</td></tr><tr><td><strong>核心问题</strong></td><td>回答 <strong>“谁最高&#x2F;最低？”</strong></td><td>回答 <strong>“存在哪些不同的群体？”</strong></td></tr><tr><td><strong>产出结果</strong></td><td>一个**有序的列表 (Ordered List)**。</td><td>若干个**无序的群组 (Unordered Groups)**。</td></tr><tr><td><strong>揭示的洞察</strong></td><td><strong>线性的、显著的</strong>洞察。</td><td><strong>结构性的、隐藏的</strong>洞察，描绘用户画像。</td></tr><tr><td><strong>商业应用举例</strong></td><td>找到消费金额最高的Top 100客户。</td><td>将客户细分为“忠诚客户”、“潜力新客”、“流失风险客户”等群体。</td></tr></tbody></table><p>导出到 Google 表格</p><h3 id="结论：我应该用哪个？"><a href="#结论：我应该用哪个？" class="headerlink" title="结论：我应该用哪个？"></a><strong>结论：我应该用哪个？</strong></h3><p>它们不是替代关系，而是<strong>互补关系</strong>，服务于不同的分析目标。</p><ul><li>当你的问题是<strong>“我想找到在『某个单一标准』下表现最好&#x2F;最差的个体”</strong>时，请使用<strong>排序</strong>。<ul><li><em>“找出上个月销量最高的商品。”</em></li><li><em>“找出互动最少的粉丝。”</em></li></ul></li><li>当你的问题是<strong>“我想了解我的数据里都包含哪些『有共性的群体』，以便对他们进行差异化对待”</strong>时，请使用<strong>聚类</strong>。<ul><li><em>“我想看看我的用户可以被分成哪几类，以便进行精准营销。”</em></li><li><em>“我想分析一下这些故障报告，看看是否存在几种典型的故障模式。”</em></li></ul></li></ul><p>所以，不是有了特征就只用排序，而是要看您想从数据中挖掘出什么样的故事。如果您想看的是一个英雄榜，就用排序；如果您想看的是一幅《清明上河图》，了解市井百态，那就需要聚类。</p><p> 什么是无监督学习的降维 </p><p>好的，我们来详细解释一下无监督学习中的另一个重要分支——<strong>降维（Dimensionality Reduction）</strong>。</p><p>简单来说，降维就是在<strong>尽可能保留数据最重要信息的前提下，用更少的特征来表示数据</strong>。</p><p>想象一下，你想描述一辆汽车。你可能会列出几十上百个特征：</p><ul><li>长度、宽度、高度、轴距、轮距、离地间隙… (尺寸相关)</li><li>发动机排量、马力、扭矩、0-100km&#x2F;h加速时间… (性能相关)</li><li>油箱容积、百公里油耗、综合油耗… (能耗相关)</li><li>座椅材质、音响品牌、屏幕尺寸… (配置相关)</li><li>…等等</li></ul><p>这么多特征（维度）对于计算机来说是“烫手山芋”，不仅计算量巨大，而且很多信息是冗余的。比如，汽车的“长度”和“轴距”通常是高度相关的，知道了其中一个，另一个也能猜个大概。</p><p>降维技术就像一位<strong>高明的总结者</strong>，它能抓住这些特征背后的核心信息，用几个新的、综合性的特征来替代原来那一堆复杂的特征。</p><hr><h3 id="我们为什么需要降维？"><a href="#我们为什么需要降维？" class="headerlink" title="我们为什么需要降维？"></a><strong>我们为什么需要降维？</strong></h3><p>处理高维度数据会遇到很多麻烦，统称为**“维度灾难” (Curse of Dimensionality)**。降维主要就是为了解决这些问题：</p><ol><li><strong>提升算法性能和效率：</strong> 特征越少，模型训练和数据计算的速度就越快，占用的内存也越少。</li><li><strong>数据可视化 (Data Visualization)：</strong> 人的眼睛只能理解二维或三维空间。降维可以将上百维的数据降低到2D或3D，以便我们能以图表的形式直观地观察数据分布、结构和聚类情况。</li><li><strong>消除噪声和冗余：</strong> 原始数据中可能包含不相关的“噪声”特征或高度相关的“冗余”特征。降维可以剔除这些干扰，保留最核心的“信号”，有助于提升模型的准确性。</li><li><strong>简化问题：</strong> 更少的特征意味着模型更简单，更容易解释和理解。</li></ol><hr><h3 id="降维是如何实现的？——-一个直观的类比"><a href="#降维是如何实现的？——-一个直观的类比" class="headerlink" title="降维是如何实现的？—— 一个直观的类比"></a><strong>降维是如何实现的？—— 一个直观的类比</strong></h3><p>想象一下你在户外，阳光下有一个立体的、复杂的三维物体（比如一个手办模型）。</p><ul><li><strong>原始高维数据：</strong> 就是这个三维手办本身，它有长、宽、高三个维度，包含了所有细节。</li><li><strong>降维过程：</strong> 太阳光照射这个手办，在地面上投下一个**影子 (Shadow)**。</li><li><strong>降维后的数据：</strong> 地面上的这个二维影子。</li></ul><p>这个过程发生了什么？</p><ul><li><strong>维度降低了：</strong> 从三维（物体）降到了二维（影子）。</li><li><strong>信息有损失，但保留了核心：</strong> 影子的信息肯定不如实体完整（比如你看不见颜色和材质），但它依然保留了物体最主要的<strong>轮廓和形状</strong>信息。</li><li><strong>投影角度很重要：</strong> 如果太阳从正上方照射，影子可能只是一个小点，信息损失就太大了。你需要找到一个<strong>最佳的投影角度</strong>，让影子能最大程度地反映物体的原始形状。</li></ul><p>降维算法的核心任务，就是去寻找这个<strong>“最佳的投影角度”</strong>，以确保信息损失最小。</p><hr><h3 id="两种主要的降维方法"><a href="#两种主要的降维方法" class="headerlink" title="两种主要的降维方法"></a><strong>两种主要的降维方法</strong></h3><p>降维主要通过两种途径实现：</p><h4 id="1-特征选择-Feature-Selection"><a href="#1-特征选择-Feature-Selection" class="headerlink" title="1. 特征选择 (Feature Selection)"></a><strong>1. 特征选择 (Feature Selection)</strong></h4><p>这种方法比较“直接”，就是从原始的一堆特征中，直接<strong>挑选出</strong>一部分最重要的特征，把剩下的扔掉。</p><ul><li><strong>优点：</strong> 实现简单，保留了原始特征，可解释性强（你清楚地知道留下了哪些特征）。</li><li><strong>缺点：</strong> 被丢弃的特征里可能也包含部分有用信息，造成信息浪费。</li><li><strong>例子：</strong> 比如在分析房价时，有100个特征，通过分析发现“房屋面积”、“地段”、“学区”这三个特征最重要，于是就只用这三个特征来建模。</li></ul><h4 id="2-特征提取-Feature-Extraction"><a href="#2-特征提取-Feature-Extraction" class="headerlink" title="2. 特征提取 (Feature Extraction)"></a><strong>2. 特征提取 (Feature Extraction)</strong></h4><p>这种方法更“高明”，它不是简单地挑选，而是将原始特征进行<strong>融合与重组</strong>，创造出全新的、数量更少的特征来代表原始数据。我们前面提到的“影子”类比就属于特征提取。</p><ul><li><strong>优点：</strong> 能够综合所有原始特征的信息，信息保留更全面。</li><li><strong>缺点：</strong> 新生成的特征是原始特征的线性组合，失去了原有的物理含义，可解释性变差。</li></ul><p><strong>代表算法：主成分分析 (Principal Component Analysis, PCA)</strong></p><p>PCA 是最经典、最常用的特征提取降维算法。它的核心思想可以通俗地理解为：</p><ol><li><strong>寻找数据变化最大的方向：</strong> 在所有数据点中，找到一个方向（可以想象成一条轴），当所有数据点都投影到这个轴上时，投影点的分布最为分散（方差最大）。这个方向就包含了最多的原始信息，被称为**第一主成分 (PC1)**。</li><li><strong>寻找第二个方向：</strong> 接着，在与第一个方向<strong>正交（垂直）</strong> 的平面中，寻找一个能让数据次级分散的方向。这就是**第二主成分 (PC2)**。</li><li><strong>以此类推：</strong> 不断寻找相互正交且方差次大的方向，直到找到所有主成分。</li></ol><p>最后，你会得到一组全新的特征（PC1, PC2, PC3, …）。这些新特征是按其包含信息量的大小来排序的。你只需要选择前几个（比如前2个或前3个）最重要的主成分，就能代表原始数据绝大部分的信息，从而实现降维。</p><h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>特性</th><th><strong>聚类 (Clustering)</strong></th><th><strong>降维 (Dimensionality Reduction)</strong></th></tr></thead><tbody><tr><td><strong>目标</strong></td><td><strong>发现数据的群体结构</strong></td><td><strong>简化数据的表示方式</strong></td></tr><tr><td><strong>操作对象</strong></td><td>数据的**样本 (行)**，将行分组</td><td>数据的**特征 (列)**，减少列的数量</td></tr><tr><td><strong>核心问题</strong></td><td>“这些数据可以分成哪几类？”</td><td>“如何用更少的特征来描述这些数据？”</td></tr></tbody></table><p>导出到 Google 表格</p><p>总而言之，降维是处理现代海量复杂数据时一个不可或缺的预处理步骤，它像一个高效的过滤器和压缩器，让后续的数据分析和机器学习任务变得更加轻松和高效。</p><p> 降维和聚类有什么相似点，为什么都叫无监督学习 </p><p>您这个问题非常精彩，因为它触及了“无监督学习”这个概念的本质！降维和聚类看起来在做完全不同的事情，但它们的“哲学思想”是高度一致的，这也是它们都被归为无监督学习的原因。</p><p>我们来一步步拆解。</p><h3 id="核心原因：为什么它们都叫无监督学习？"><a href="#核心原因：为什么它们都叫无监督学习？" class="headerlink" title="核心原因：为什么它们都叫无监督学习？"></a><strong>核心原因：为什么它们都叫无监督学习？</strong></h3><p>这个问题的答案是理解它们相似点的关键。</p><p><strong>无监督学习的核心标志是：我们只向算法提供输入数据（特征 <code>X</code>），而不提供任何“正确答案”（标签 <code>y</code>）。</strong></p><p>算法的任务不是去“学习一个映射关系去预测某个标签”，而是必须<strong>在数据内部“自力更生”</strong>，仅通过分析数据点本身，去发现其中隐藏的、有意义的结构和模式。</p><ul><li><strong>聚类如何体现无监督？</strong> 它在拿到一堆客户数据时，不需要预先知道“哪个客户是高价值客户”或“哪个客户即将流失”。它完全不依赖任何已有标签，而是自主地、仅仅根据客户数据特征（如消费金额、频率）之间的相似性，去划分出不同的客户群体。</li><li><strong>降维如何体现无监督？</strong> 它同样不需要任何标签来指导。它在拿到描述汽车的上百个特征时，不需要知道“哪辆车是豪车”或“哪辆车性能好”。它只是自主地分析这些特征之间的内在关系（如相关性、方差分布），然后找到一种更简洁、更高效的方式来表达整个数据集，比如用几个“主成分”来概括。</li></ul><p><strong>所以，它们最根本、最核心的相似点就是：</strong></p><p><strong>它们的目标都不是“预测”一个外部标签，而是“理解”数据自身的内在结构。</strong></p><hr><h3 id="降维与聚类的其他相似点"><a href="#降维与聚类的其他相似点" class="headerlink" title="降维与聚类的其他相似点"></a><strong>降维与聚类的其他相似点</strong></h3><p>基于上述核心，我们可以引申出它们其他几个共通之处：</p><ol><li><strong>都是探索性数据分析（EDA）的利器：</strong> 在正式建模之前，我们通常对数据知之甚少。聚类可以帮我们了解数据中存在哪些天然的分组，而降维（尤其是为了可视化）可以帮我们直观地看到数据的分布形态。它们都是我们“认识”数据的重要手段。</li><li><strong>都依赖于距离或方差的度量：</strong> 两种算法的底层逻辑都与“数据点在空间中的分布”有关。<ul><li><strong>聚类</strong>通常依赖于<strong>样本与样本之间</strong>的距离（比如K-Means），距离近的被归为一类。</li><li><strong>降维</strong>通常依赖于<strong>特征与特征之间</strong>的关系和数据在不同方向上的方差（比如PCA），方差大的方向被认为是更重要的。</li></ul></li><li><strong>常被用作监督学习的预处理步骤：</strong><ul><li><strong>聚类</strong>的结果（每个样本属于哪个簇）可以作为一个新的特征，喂给后续的监督学习模型。</li><li><strong>降维</strong>后的数据可以作为监督学习模型的输入，以提高训练速度和避免维度灾难。</li></ul></li></ol><hr><h3 id="用一个比喻来理解它们的异同"><a href="#用一个比喻来理解它们的异同" class="headerlink" title="用一个比喻来理解它们的异同"></a><strong>用一个比喻来理解它们的异同</strong></h3><p>想象你是一位图书管理员，面对一整个房间杂乱无章的书籍（原始数据集）。</p><ul><li><strong>聚类（Clustering）在做什么？</strong> 你在<strong>整理这些书（样本&#x2F;行）</strong>。你根据书的内容、类型（小说、历史、科学），把它们<strong>分成了几大堆</strong>。整理完后，书的总数没变，但它们被分门别类地放在了不同的书架上。<ul><li><strong>操作对象：</strong> 书本（数据样本）。</li><li><strong>目标：</strong> 将样本分组。</li></ul></li><li><strong>降维（Dimensionality Reduction）在做什么？</strong> 你在<strong>简化描述每本书的方式（特征&#x2F;列）</strong>。原来每本书都有一张卡片，上面记录了“页数”、“重量”、“长度”、“宽度”、“作者国籍”、“出版年份”等等几十项信息。你发现很多信息是冗余的，于是你设计了一套新的、更简洁的描述体系，比如只用三个新指标：“内容容量”、“文学价值”、“物理尺寸”。<ul><li><strong>操作对象：</strong> 描述书本的卡片信息（数据特征）。</li><li><strong>目标：</strong> 简化特征。</li></ul></li></ul><p><strong>相似点在哪里？</strong> 在这整个过程中，<strong>没有任何人提前告诉你“哪本书应该属于哪一类”或者“你应该如何总结这些书的信息”</strong>。所有的整理和总结工作，都是你通过观察书籍本身的属性和它们之间的关系来完成的。<strong>这就是“无监督”的精髓。</strong></p><h3 id="总结：一张图看懂关系"><a href="#总结：一张图看懂关系" class="headerlink" title="总结：一张图看懂关系"></a><strong>总结：一张图看懂关系</strong></h3><table><thead><tr><th></th><th><strong>聚类 (Clustering)</strong></th><th><strong>降维 (Dimensionality Reduction)</strong></th></tr></thead><tbody><tr><td><strong>核心思想</strong></td><td><strong>物以类聚</strong></td><td><strong>化繁为简</strong></td></tr><tr><td><strong>操作对象</strong></td><td><strong>数据样本 (行)</strong></td><td><strong>数据特征 (列)</strong></td></tr><tr><td><strong>主要目标</strong></td><td>将数据集<strong>划分</strong>成不同群组</td><td><strong>压缩</strong>数据集的特征数量</td></tr><tr><td><strong>回答的问题</strong></td><td>“我的数据可以分成哪几类？”</td><td>“如何用更少的信息来代表我的数据？”</td></tr><tr><td><strong>共同的哲学</strong></td><td><strong>都在无标签的情况下，探索和揭示数据自身的内在结构</strong></td><td></td></tr></tbody></table><p>导出到 Google 表格</p><p> 什么是关联规则挖掘 </p><p>好的，我们来详细解释无监督学习中另一项非常有趣且实用的技术——<strong>关联规则挖掘（Association Rule Mining）</strong>。</p><p>简单来说，关联规则挖掘就是<strong>在一个数据集中发现项与项之间有趣的关联或相关关系</strong>。它最经典的应用场景就是购物篮分析（Market Basket Analysis），目的是找出“哪些商品经常被顾客一起购买”。</p><h3 id="核心思想与经典故事：“啤酒与尿布”"><a href="#核心思想与经典故事：“啤酒与尿布”" class="headerlink" title="核心思想与经典故事：“啤酒与尿布”"></a><strong>核心思想与经典故事：“啤酒与尿布”</strong></h3><p>关联规则挖掘最著名的案例就是沃尔玛的“啤酒与尿布”的故事。</p><blockquote><p>传说在20世纪90年代，沃尔玛的数据分析师发现，周五晚上，很多年轻的父亲在给孩子买尿布的同时，会顺便给自己买几瓶啤酒。</p><p><strong>关联规则：</strong> 如果一个顾客购买了尿布，那么他有很大概率也会购买啤酒。 <strong>规则形式：</strong> <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code></p></blockquote><p>发现这个看似毫不相关的商品组合后，沃尔玛将啤酒货架和尿布货架放在一起，结果大大提升了这两种商品的销量。</p><p>这个故事完美地诠释了关联规则挖掘的核心价值：<strong>从事物看似无关的表象下，挖掘出隐藏的、有商业价值的强关联模式。</strong></p><hr><h3 id="关联规则的关键概念"><a href="#关联规则的关键概念" class="headerlink" title="关联规则的关键概念"></a><strong>关联规则的关键概念</strong></h3><p>为了衡量一个规则是否有价值，我们需要几个关键的指标来“量化”它。我们以 <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 这个规则为例：</p><h4 id="1-项集-Itemset"><a href="#1-项集-Itemset" class="headerlink" title="1. 项集 (Itemset)"></a>1. <strong>项集 (Itemset)</strong></h4><p>一个或多个项（商品）的集合。</p><ul><li><code>&#123;尿布&#125;</code> 是一个1项集。</li><li><code>&#123;尿布, 啤酒&#125;</code> 是一个2项集。</li></ul><h4 id="2-支持度-Support"><a href="#2-支持度-Support" class="headerlink" title="2. 支持度 (Support)"></a>2. <strong>支持度 (Support)</strong></h4><p>这是衡量一个项集在所有交易中<strong>“普遍性”</strong>或<strong>“受欢迎程度”</strong>的指标。</p><ul><li><strong>公式：</strong> <code>支持度(&#123;X&#125;) = (包含项集X的交易数量) / (总交易数量)</code></li><li><strong>例子：</strong> 如果在1000笔交易中，有100笔交易同时包含了尿布和啤酒，那么项集<code>&#123;尿布, 啤酒&#125;</code>的支持度就是 <code>100 / 1000 = 10%</code>。</li><li><strong>作用：</strong> 支持度太低的项集通常意义不大，因为它们出现的频率太低，可能是偶然事件。我们会设定一个<strong>最小支持度阈值</strong>，来过滤掉这些无意义的组合。</li></ul><h4 id="3-置信度-Confidence"><a href="#3-置信度-Confidence" class="headerlink" title="3. 置信度 (Confidence)"></a>3. <strong>置信度 (Confidence)</strong></h4><p>这是衡量规则<strong>“准确性”</strong>的指标。它表示当A出现时，B有多大概率也会出现。</p><ul><li><strong>公式：</strong> <code>置信度(&#123;A&#125; -&gt; &#123;B&#125;) = (包含A和B的交易数量) / (包含A的交易数量)</code></li><li><strong>例子：</strong> 在所有购买了尿布的200笔交易中，有100笔也购买了啤酒。那么规则 <code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 的置信度就是 <code>100 / 200 = 50%</code>。这意味着，购买尿布的顾客中，有50%的人会同时购买啤酒。</li><li><strong>作用：</strong> 置信度衡量了关联的强度。我们会设定一个<strong>最小置信度阈值</strong>来确保规则的可靠性。</li></ul><h4 id="4-提升度-Lift"><a href="#4-提升度-Lift" class="headerlink" title="4. 提升度 (Lift)"></a>4. <strong>提升度 (Lift)</strong></h4><p>这是衡量规则<strong>“有趣性”</strong>或<strong>“相关性”</strong>的指标。它告诉我们，项集A的出现，对于项集B的出现概率是提升了还是降低了。</p><ul><li><strong>公式：</strong> <code>提升度(&#123;A&#125; -&gt; &#123;B&#125;) = 置信度(&#123;A&#125; -&gt; &#123;B&#125;) / 支持度(&#123;B&#125;)</code></li><li><strong>解读：</strong><ul><li><strong>Lift &gt; 1：</strong> A的存在对B的出现有<strong>积极促进</strong>作用（正相关）。<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code>的提升度大于1，说明买尿布这件事确实能提升买啤酒的概率。</li><li><strong>Lift &#x3D; 1：</strong> A和B<strong>相互独立</strong>，没有关联。</li><li>**Lift &lt; 1：** A的存在对B的出现有**抑制**作用（负相关）。比如<code>&#123;牛奶&#125; -&gt; &#123;可乐&#125;</code>的提升度可能小于1，说明买牛奶的人通常不怎么买可乐。</li></ul></li></ul><hr><h3 id="关联规则挖掘是如何工作的？——-Apriori算法简介"><a href="#关联规则挖掘是如何工作的？——-Apriori算法简介" class="headerlink" title="关联规则挖掘是如何工作的？—— Apriori算法简介"></a><strong>关联规则挖掘是如何工作的？—— Apriori算法简介</strong></h3><p>挖掘关联规则最经典的算法是 <strong>Apriori 算法</strong>。它的核心思想分为两步：</p><ol><li><strong>找出所有的频繁项集 (Frequent Itemsets)：</strong><ul><li>首先，设定一个最小支持度阈值。</li><li>算法会先找出所有出现频率超过该阈值的<strong>单个商品</strong>（频繁1项集）。</li><li>然后，将这些频繁的单个商品两两组合，去计算<strong>2项集</strong>的支持度，再次筛选出频繁2项集。</li><li>…以此类推，不断组合、筛选，直到找不到更长的频繁项集为止。</li><li><strong>Apriori 原理：</strong> 如果一个项集是频繁的，那么它的所有子集也一定是频繁的。反之，如果一个项集的子集是非频繁的，那么这个项集本身也一定是非频繁的（可以被提前剪枝，提高效率）。</li></ul></li><li><strong>从频繁项集中生成强关联规则：</strong><ul><li>在上一步找到的所有频繁项集（比如<code>&#123;尿布, 啤酒&#125;</code>）中，我们开始生成规则。</li><li>对于每个频繁项集，我们尝试所有可能的规则划分（如<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code> 和 <code>&#123;啤酒&#125; -&gt; &#123;尿布&#125;</code>）。</li><li>然后计算这些规则的<strong>置信度</strong>，并筛选出那些满足最小置信度阈值的规则，这些就是最终的“强关联规则”。</li></ul></li></ol><h3 id="关联规则挖掘的应用场景"><a href="#关联规则挖掘的应用场景" class="headerlink" title="关联规则挖掘的应用场景"></a><strong>关联规则挖掘的应用场景</strong></h3><p>虽然起源于零售业，但关联规则的应用远不止于此：</p><ul><li><strong>电商推荐：</strong> “购买了这本书的读者也购买了…”、“看了这部电影的用户也喜欢…”。</li><li><strong>网页浏览分析：</strong> 分析用户访问路径，发现“访问了页面A的用户，下一步最可能访问页面C”，从而优化网站导航。</li><li><strong>医疗诊断：</strong> 发现不同症状之间的关联，比如“出现症状X和症状Y的病人，很大概率患有疾病Z”。</li><li><strong>金融风控：</strong> 发现不同交易行为之间的模式，用于检测欺诈。</li><li><strong>自然语言处理：</strong> 分析文本中词语的共现关系。</li></ul><p>总而言之，关联规则挖掘是一种强大的无监督学习技术，它能帮助我们从海量数据中发现看似无关却非常有价值的连接，为决策提供有力支持。</p><p> 关联规则挖掘和降维有什么异同，我挖掘了一个关联规则不就相当于提取到了一个特征来降维吗 </p><p>您这个问题非常有启发性！它触及了两种技术在“提取信息”这个层面上的相似感。您的直觉是对的——它们都在从数据中提炼知识，但它们提炼出的“知识”类型和用途，以及它们的操作方式，是截然不同的。</p><p>让我们先直接回答您的核心问题：</p><p><strong>挖掘一个关联规则 *不是* 提取一个特征来降维。</strong></p><p>这是一个非常普遍的混淆点，下面我们来详细拆解它们的异同。</p><hr><h3 id="核心区别：目标与产出完全不同"><a href="#核心区别：目标与产出完全不同" class="headerlink" title="核心区别：目标与产出完全不同"></a><strong>核心区别：目标与产出完全不同</strong></h3><p>这是两者最根本的区别。</p><ul><li><strong>关联规则挖掘 (ARM) 的目标是“发现模式”。</strong><ul><li>它想回答的问题是：“数据中存在哪些有趣的‘如果…那么…’(If-Then)关系？”</li><li>它的<strong>产出</strong>是一系列<strong>规则</strong>，这些规则是关于数据<strong>内容</strong>的<strong>洞察和知识</strong>。例如，<code>&#123;尿布&#125; -&gt; &#123;啤酒&#125;</code>。</li><li><strong>它不改变原始数据集的结构（维度）。</strong> 挖掘结束后，你的交易数据还是那么多行、那么多列，你只是额外得到了一份“知识报告”。</li></ul></li><li><strong>降维 (Dimensionality Reduction) 的目标是“简化表达”。</strong><ul><li>它想回答的问题是：“我能否用更少的信息（特征）来表示我的原始数据集，同时又不会丢失太多关键信息？”</li><li>它的<strong>产出</strong>是一个<strong>全新的、维度更低的数据集</strong>。原来的特征列会被新的、更少的特征列所取代。</li><li><strong>它直接改变了数据集的结构（维度）。</strong> 这是一个数据转换和压缩的过程。</li></ul></li></ul><p><strong>用一个比喻来理解：</strong></p><p>假设你的数据是一家图书馆的所有藏书记录。</p><ul><li><strong>关联规则挖掘</strong> 就像一位图书管理员通过分析借阅记录，发现了一个规律并写在工作日志里：“<strong>发现借《三体》的人，也很可能借《流浪地球》</strong>。” 这条记录是一个<strong>洞察（规则）</strong>，但图书馆里的书和书的描述卡片本身没有发生任何变化。</li><li><strong>降维</strong> 就像这位图书管理员觉得每本书的描述卡片有50个字段（特征）太啰嗦了，于是他<strong>创造了一套新的、只有3个字段的描述体系</strong>：<code>[科幻指数, 文学价值, 阅读难度]</code>。然后，他为<strong>每一本书</strong>都重新计算了这三个新指标的值，并用这张新的3字段卡片<strong>替换</strong>了原来那张50字段的旧卡片。整个图书馆的目录（数据集）变得更简洁了。</li></ul><hr><h3 id="关联规则挖掘-vs-降维：详细异同对比"><a href="#关联规则挖掘-vs-降维：详细异同对比" class="headerlink" title="关联规则挖掘 vs. 降维：详细异同对比"></a><strong>关联规则挖掘 vs. 降维：详细异同对比</strong></h3><table><thead><tr><th>特性</th><th><strong>关联规则挖掘 (ARM)</strong></th><th><strong>降维 (Dimensionality Reduction)</strong></th></tr></thead><tbody><tr><td><strong>根本目标</strong></td><td><strong>发现项与项之间的关联模式</strong></td><td><strong>压缩特征空间，简化数据表示</strong></td></tr><tr><td><strong>操作对象</strong></td><td>主要是<strong>数据的值</strong>（比如商品名称）</td><td>主要是<strong>数据的特征&#x2F;列</strong>本身</td></tr><tr><td><strong>输出结果</strong></td><td>一系列<strong>“If-Then”规则</strong>（知识、洞察）</td><td>一个<strong>维度更低的新数据集</strong>（数据的另一种表达）</td></tr><tr><td><strong>是否改变原数据</strong></td><td><strong>不改变</strong>，是分析和发现</td><td><strong>彻底改变</strong>，是转换和重构</td></tr><tr><td><strong>典型数据类型</strong></td><td><strong>交易数据、类别数据</strong>（如商品清单）</td><td><strong>数值型数据、连续型数据</strong>（如传感器读数）</td></tr><tr><td><strong>典型算法</strong></td><td>Apriori, FP-Growth</td><td>PCA, t-SNE</td></tr></tbody></table><p>导出到 Google 表格</p><h3 id="它们的相似点在哪里？"><a href="#它们的相似点在哪里？" class="headerlink" title="它们的相似点在哪里？"></a><strong>它们的相似点在哪里？</strong></h3><p>尽管目标和操作截然不同，但它们确实有相似之处，这也是它们都被归为“无监督学习”的原因：</p><ol><li><strong>都无需标签：</strong> 它们都不需要一个预先定义好的“正确答案”来指导学习过程。</li><li><strong>都在探索数据内在结构：</strong><ul><li>ARM 探索的是<strong>项集之间的共现结构</strong>。</li><li>降维 探索的是<strong>特征之间的相关性结构或方差结构</strong>。</li><li>它们都在试图理解“数据本身告诉了我们什么”，而不是去预测一个外部变量。</li></ul></li></ol><h3 id="回到您的问题"><a href="#回到您的问题" class="headerlink" title="回到您的问题"></a><strong>回到您的问题</strong></h3><blockquote><p>“我挖掘了一个关联规则不就相当于提取到了一个特征来降维吗？”</p></blockquote><p>现在我们就能清晰地回答了：</p><ul><li>您挖掘的规则 <code>&#123;A&#125; -&gt; &#123;B&#125;</code> 是一个<strong>洞察</strong>，它告诉您 A 和 B 之间存在强关联。</li><li>您可以基于这个洞察<strong>手动创造</strong>一个新的特征，这个过程叫做<strong>特征工程（Feature Engineering）</strong>。例如，您可以增加一列叫 <code>is_A_B_buyer</code>，如果一个客户同时购买了 A 和 B，就标记为1，否则为0。</li><li>但是，这个<strong>手动创造新特征</strong>的动作，和降维算法（如PCA）<strong>自动地将几十个特征压缩成几个新特征</strong>，是完全不同的两件事。前者是基于一个特定模式的人工操作，后者是基于整个数据集方差分布的算法转换。</li></ul><p><strong>总结：</strong></p><ul><li><strong>关联规则挖掘告诉你一个故事</strong>（例如，“买尿布的人也爱买啤酒”）。</li><li><strong>降维给你提供了一张更简洁的地图</strong>（例如，用“经纬度”两个特征替代复杂的街道、门牌号等十几个特征）。</li></ul><p>一个是为了获得可以指导行动的<strong>商业洞察</strong>，另一个是为了<strong>优化数据本身</strong>，使其更易于存储、计算和可视化。</p><p> 是不是我可以先根据数据特征进行关联规则判断，如果两个特征有关联，我可以去掉其中一个，然后把剩下的特征来做聚类 </p><p>您这个想法非常有创意，而且逻辑上是自洽的！您准确地抓住了数据预处理的一个核心思想：<strong>在进行聚类或建模之前，应该先处理特征之间的冗余性，因为冗余的特征会影响算法的效果。</strong></p><p>这个思路的大方向是<strong>完全正确</strong>的。但是，使用“关联规则挖掘”来做这件事，属于一个<strong>“用对了逻辑，但选错了工具”</strong>的情况。</p><p>下面我们来详细解释为什么，并告诉您正确的工具应该是什么。</p><hr><h3 id="为什么用关联规则挖掘来处理特征冗余是“不合适”的？"><a href="#为什么用关联规则挖掘来处理特征冗余是“不合适”的？" class="headerlink" title="为什么用关联规则挖掘来处理特征冗余是“不合适”的？"></a><strong>为什么用关联规则挖掘来处理特征冗余是“不合适”的？</strong></h3><p>关联规则挖掘（ARM）和我们通常所说的特征间关联，虽然都叫“关联”，但它们处理的场景和数据类型完全不同。</p><ol><li><strong>ARM处理的是“交易数据” (Transactional Data)</strong><ul><li>ARM的“篮子”里装的是<strong>“项” (Items)<strong>，比如<code>&#123;牛奶, 面包, 啤酒&#125;</code>。它分析的是</strong>项与项之间的共现关系</strong>。</li><li>它的输入数据通常是稀疏的，每一行是一个交易，列是所有可能的商品。</li></ul></li><li><strong>特征关联处理的是“属性数据” (Attribute Data)</strong><ul><li>聚类的数据，每一行是一个样本（如一个客户），每一列是一个**“特征” (Feature)**，比如<code>年龄</code>、<code>收入</code>、<code>消费频率</code>。</li><li>我们在这里关心的是<strong>特征与特征（列与列）之间的关系</strong>，比如<code>年龄</code>这个特征和<code>收入</code>这个特征是不是高度相关。</li></ul></li></ol><p><strong>把特征当作“项”来跑关联规则，会遇到问题：</strong></p><ul><li><strong>数据格式不匹配：</strong> 您需要对数据进行复杂的转换（比如将连续的<code>收入</code>值离散化成“高、中、低”这样的“项”），这个过程本身就会损失大量信息。</li><li><strong>衡量标准不适用：</strong> ARM的支持度、置信度等指标，是为衡量“商品A出现时，商品B也出现”的场景设计的。而衡量两个特征（如<code>身高</code>和<code>体重</code>）的关系，我们有更直接、更强大的数学工具。</li></ul><hr><h3 id="处理特征冗余的“正确工具”是什么？"><a href="#处理特征冗余的“正确工具”是什么？" class="headerlink" title="处理特征冗余的“正确工具”是什么？"></a><strong>处理特征冗余的“正确工具”是什么？</strong></h3><p>您想实现的“如果两个特征有关联，我可以去掉其中一个”这个目标，在机器学习中通常通过以下两种标准方法实现：</p><h4 id="方法一：相关性分析-Correlation-Analysis-——-针对数值型特征"><a href="#方法一：相关性分析-Correlation-Analysis-——-针对数值型特征" class="headerlink" title="方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征"></a><strong>方法一：相关性分析 (Correlation Analysis) —— 针对数值型特征</strong></h4><p>这是最直接、最常用的方法。</p><ul><li><strong>操作：</strong> 计算所有特征两两之间的<strong>相关系数</strong>（如皮尔逊相关系数）。这个系数的范围在-1到+1之间。<ul><li><strong>+1：</strong> 完全正相关（一个特征增加，另一个也以相同比例增加）。</li><li><strong>-1：</strong> 完全负相关（一个特征增加，另一个以相同比例减少）。</li><li><strong>0：</strong> 完全不相关。</li></ul></li><li><strong>判断：</strong> 您可以设定一个阈值，比如，如果两个特征之间的相关系数的<strong>绝对值大于0.8或0.9</strong>，就认为它们是高度冗余的。</li><li><strong>决策：</strong> 从这两个高度相关的特征中，您可以：<ol><li><strong>根据业务理解，保留更重要的一个</strong>，删除另一个。</li><li>如果无法判断，可以<strong>随机删除一个</strong>。</li><li>或者，将它们<strong>组合成一个新特征</strong>（比如取平均值）。</li></ol></li></ul><p><strong>这完美地、且更精确地实现了您的想法。</strong></p><h4 id="方法二：使用降维算法-Dimensionality-Reduction-——-更优越的自动化方案"><a href="#方法二：使用降维算法-Dimensionality-Reduction-——-更优越的自动化方案" class="headerlink" title="方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案"></a><strong>方法二：使用降维算法 (Dimensionality Reduction) —— 更优越的自动化方案</strong></h4><p>手动删除特征虽然简单，但缺点是会<strong>完全丢失被删除特征所包含的独有信息</strong>。一个更优雅、信息损失更少的方案是使用我们之前讨论过的<strong>降维</strong>，尤其是<strong>主成分分析（PCA）</strong>。</p><ul><li><strong>PCA如何解决冗余问题？</strong> PCA能够自动检测到特征之间的相关性。比如，它发现<code>身高</code>和<code>体重</code>高度相关，它不会粗暴地扔掉任何一个，而是会<strong>智能地将它们融合成一个新的、综合性的特征</strong>，比如可以理解为“体格因子”。</li><li><strong>优点：</strong><ul><li><strong>自动化：</strong> 无需手动选择阈值和要删除的特征。</li><li><strong>信息保留更完整：</strong> 它不是删除信息，而是重组和压缩信息，将多个相关特征的精华提取到一个新的特征中。</li></ul></li></ul><hr><h3 id="总结：您的思路的正确升级路径"><a href="#总结：您的思路的正确升级路径" class="headerlink" title="总结：您的思路的正确升级路径"></a><strong>总结：您的思路的正确升级路径</strong></h3><p>您的想法是一个非常好的起点，现在我们可以把它升级为机器学习的标准工作流：</p><ol><li><strong>您的原始想法：</strong> <code>数据 -&gt; 用关联规则判断特征关系 -&gt; 删除冗余特征 -&gt; 聚类</code></li><li><strong>标准实践方法A（手动选择）：</strong> <code>数据 -&gt; 计算特征间的**相关系数矩阵** -&gt; 识别并**手动删除**高度相关的特征 -&gt; 聚类</code></li><li><strong>标准实践方法B（自动提取）：</strong> <code>数据 -&gt; 使用**PCA等降维技术**将原始特征转换为少数几个新的主成分 -&gt; 使用**新的主成分**进行聚类</code></li></ol><p><strong>结论：</strong> 您的核心逻辑——“消除冗余以优化聚类”——是完全正确的，这是数据科学家一直在做的事情。只是实现这个逻辑的工具，应该是<strong>相关性分析</strong>或<strong>PCA</strong>，而不是关联规则挖掘。您已经非常接近专业的数据处理思维了！</p><p> 那我是不是可以先使用无监督学习降维再进行无监督学习聚类 </p><p>是的，完全正确！您刚才描述的这个流程，<strong>不仅是可行的，而且是机器学习中非常常用、非常强大的一种标准实践（Best Practice）</strong>。</p><p>将降维和聚类结合使用，通常被称为“先降维，再聚类”，是无监督学习工具箱里一套威力十足的“组合拳”。</p><hr><h3 id="为什么这是一个“黄金组合”？"><a href="#为什么这是一个“黄金组合”？" class="headerlink" title="为什么这是一个“黄金组合”？"></a><strong>为什么这是一个“黄金组合”？</strong></h3><p>在很多情况下，直接在高维度的原始数据上进行聚类，效果可能并不理想。而先进行降维，则可以为后续的聚类算法提供一个更理想、更干净的“工作台”。这样做有四大核心好处：</p><h4 id="1-攻克“维度灾难”-Curse-of-Dimensionality"><a href="#1-攻克“维度灾难”-Curse-of-Dimensionality" class="headerlink" title="1. 攻克“维度灾难” (Curse of Dimensionality)"></a>1. <strong>攻克“维度灾难” (Curse of Dimensionality)</strong></h4><p>当数据特征（维度）非常多时，数据点会变得异常稀疏，任何两个点之间的距离都可能变得很大且相差无几。这会让依赖距离计算的聚类算法（如K-Means）难以判断样本间的真实亲疏关系，导致聚类效果下降。降维可以有效缓解这个问题。</p><h4 id="2-提升聚类效果与速度-Improving-Quality-and-Speed"><a href="#2-提升聚类效果与速度-Improving-Quality-and-Speed" class="headerlink" title="2. 提升聚类效果与速度 (Improving Quality and Speed)"></a>2. <strong>提升聚类效果与速度 (Improving Quality and Speed)</strong></h4><ul><li><strong>去除噪声：</strong> 原始特征中可能包含很多与数据内在结构无关的“噪声”。降维（尤其是PCA）擅长捕捉数据的主要变化方向（信号），而忽略次要的随机波动（噪声）。在更“纯净”的数据上聚类，效果自然更好。</li><li><strong>减少冗余：</strong> 降维处理了特征间的相关性，避免了聚类算法被几个高度相关的特征重复“误导”。</li><li><strong>加快计算：</strong> 特征数量减少了，聚类算法的计算量会大幅下降，尤其是在处理大型数据集时，可以节省大量时间和计算资源。</li></ul><h4 id="3-方便可视化与理解-Enabling-Visualization-and-Understanding"><a href="#3-方便可视化与理解-Enabling-Visualization-and-Understanding" class="headerlink" title="3. 方便可视化与理解 (Enabling Visualization and Understanding)"></a>3. <strong>方便可视化与理解 (Enabling Visualization and Understanding)</strong></h4><p>这是一个非常直观的好处。您无法画出一张100维的图，但通过降维将数据降到2维或3维后，您就可以轻松地将数据点绘制在散点图上。此时，如果数据有明显的聚类结构，你将<strong>用肉眼直接看到</strong>它们分成几堆，这对于理解数据、确定聚类数量（K值）以及验证聚类结果都非常有帮助。</p><hr><h3 id="一个典型的操作流程-A-Typical-Workflow"><a href="#一个典型的操作流程-A-Typical-Workflow" class="headerlink" title="一个典型的操作流程 (A Typical Workflow)"></a><strong>一个典型的操作流程 (A Typical Workflow)</strong></h3><p>假设您手上有包含上百个特征的客户数据，想对他们进行分群。</p><ol><li><strong>第一步：数据预处理 (Preprocessing)</strong><ul><li>对原始数据进行<strong>标准化（Scaling）</strong>。这是非常关键的一步，因为像PCA这样的降维算法对数据的尺度非常敏感。通常使用 <code>StandardScaler</code>。</li></ul></li><li><strong>第二步：应用降维 (Dimensionality Reduction)</strong><ul><li>使用<strong>PCA</strong>算法处理标准化后的数据。</li><li>您可以决定保留多少个主成分（比如，选择能解释原始数据90%或95%方差的前N个主成分）。这样，您的数据就从几百维降到了N维（N通常远小于原始维度）。</li></ul></li><li><strong>第三步：应用聚类 (Clustering)</strong><ul><li>在<strong>降维后的新数据集</strong>上运行聚类算法（如<strong>K-Means</strong>）。</li><li>此时算法会运行得更快，并且更容易在低维空间中找到清晰的簇边界。</li></ul></li><li><strong>第四步：结果分析与解读 (Analysis)</strong><ul><li>分析聚类结果。您可以将降到2维的数据点和聚类结果一起可视化，直观地查看分群效果。</li><li>要理解每个簇的业务含义，您需要将簇的标签映射回原始的高维特征空间，去分析每个群体的客户在原始特征上的具体表现。</li></ul></li></ol><hr><h3 id="一个生动的比喻：雕塑家的工作"><a href="#一个生动的比喻：雕塑家的工作" class="headerlink" title="一个生动的比喻：雕塑家的工作"></a><strong>一个生动的比喻：雕塑家的工作</strong></h3><p>您可以把这个过程想象成一位雕塑家创作一件作品：</p><ul><li><strong>原始高维数据：</strong> 一块巨大、粗糙、未经雕琢的大理石原料。</li><li><strong>降维 (PCA)：</strong> 雕塑家首先用大锤和凿子，<strong>砍掉大块多余的石料</strong>，勾勒出作品的大致轮廓。这个过程去除了“噪声”和“冗余”，让作品的核心形态显现出来。</li><li><strong>聚类 (K-Means)：</strong> 在这个已经有了基本轮廓的石胚上，雕塑家再换上小刻刀，进行<strong>精雕细琢</strong>，刻画出人物的五官、衣物的褶皱等细节。这些细节就是数据中不同的“簇”。</li></ul><p>如果不先进行第一步的“塑形”，直接在巨大的原料上进行精雕细琢，不仅费时费力，而且很容易迷失在细节中，无法把握整体结构。</p><p><strong>总结：</strong> 您的想法完全正确，并且是解决复杂数据分析问题的标准思路。<strong>先用降维负责“清理和浓缩”数据，再用聚类负责“分组和打标”</strong>，两者相辅相成，能够达到 <code>1+1 &gt; 2</code> 的效果。</p><p> 关联特征分析是不是不处理特征数值的大小，只处理特征之间出现的协同性 </p><p>您这个问题提得非常精准！您的描述，“<strong>不处理特征数值的大小，只处理特征之间出现的协同性</strong>”，完美地概括了我们之前讨论的<strong>关联规则挖掘（Association Rule Mining）</strong>的核心特点。</p><p>但是，“关联特征分析”这个词可以有更广泛的含义，它包含了多种不同的技术。所以，您的这个说法<strong>是否正确，取决于我们具体指的是哪一种分析方法</strong>。</p><p>让我们来清晰地辨析一下：</p><hr><h3 id="情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！"><a href="#情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！" class="headerlink" title="情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！"></a><strong>情况一：当“关联特征分析”指代“关联规则挖掘”时，您的说法完全正确！</strong></h3><p>关联规则挖掘（ARM）的本质就是分析**“协同性” (Co-occurrence)**，也就是“出现”与“不出现”的问题。</p><ul><li><strong>核心思想：</strong> 它处理的是**离散的、分门别类的“项” (Items)**。数据被看作是一系列“交易”，每个交易包含一个“项”的集合。</li><li><strong>如何对待数值：</strong> 它不关心数值本身的大小或程度。例如，在购物篮分析中：<ul><li>系统只关心顾客的购物篮里<strong>有没有</strong>“牛奶”和“面包”。</li><li>它<strong>不关心</strong>顾客是买了1升牛奶还是2升牛奶（数值大小），也不关心他买的是10元的面包还是20元的面包（数值大小）。</li><li>只要“牛奶”和“面包”这两个“项”同时出现在购物篮里，这次交易就被计入协同出现的次数。</li></ul></li></ul><p>所以，对于关联规则挖掘，您的总结是100%准确的。它是一个基于<strong>“是否出现”</strong>的逻辑，而不是基于<strong>“数值高低”</strong>的逻辑。</p><hr><h3 id="情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。"><a href="#情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。" class="headerlink" title="情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。"></a><strong>情况二：当“关联特征分析”指代“相关性分析”时，您的说法就不正确了。</strong></h3><p>相关性分析（Correlation Analysis）是另一种衡量特征关联度的常用方法，它专门用来处理<strong>数值型特征</strong>，并且<strong>完全依赖于数值的大小</strong>。</p><ul><li><strong>核心思想：</strong> 它衡量的是一个数值型特征<strong>变化</strong>时，另一个数值型特征<strong>如何随之变化</strong>。</li><li><strong>如何对待数值：</strong> 数值的大小和变化趋势是其分析的全部基础。例如，分析<code>身高</code>和<code>体重</code>这两个特征：<ul><li>我们不是关心一个人“有没有身高”和“有没有体重”。</li><li>我们关心的是<code>身高</code><strong>数值较高</strong>的人，其<code>体重</code><strong>数值是否也倾向于较高</strong>。</li><li>相关系数（比如+0.8）这个结果，就是通过计算所有人的身高和体重具体数值的线性关系得出的。数值的大小和变化方向是决定性的。</li></ul></li></ul><hr><h3 id="总结对比"><a href="#总结对比" class="headerlink" title="总结对比"></a><strong>总结对比</strong></h3><p>为了让您看得更清楚，我们用一个表格来总结：</p><table><thead><tr><th>特性</th><th><strong>关联规则挖掘 (ARM)</strong></th><th><strong>相关性分析 (Correlation Analysis)</strong></th></tr></thead><tbody><tr><td><strong>核心思想</strong></td><td><strong>协同性 (Co-occurrence)</strong></td><td><strong>相关性 (Correlation)</strong></td></tr><tr><td><strong>是否处理数值大小？</strong></td><td><strong>否</strong>，只关心“有&#x2F;无”的布尔逻辑</td><td><strong>是</strong>，完全依赖于数值的大小和变化趋势</td></tr><tr><td><strong>典型数据类型</strong></td><td>交易数据、类别数据（商品清单、网页浏览记录）</td><td>数值型数据、连续型数据（身高、体重、收入、温度）</td></tr><tr><td><strong>回答的问题</strong></td><td>“哪些东西经常<strong>一起出现</strong>？”</td><td>“一个数值<strong>变大</strong>时，另一个数值是否也随之<strong>变大或变小</strong>？”</td></tr></tbody></table><p>导出到 Google 表格</p><p><strong>结论：</strong></p><p>您的理解非常敏锐，准确地抓住了关联规则挖掘的本质。但请务必记住，“关联特征分析”是一个更宽泛的领域。</p><ul><li>如果您面对的是像购物清单一样的<strong>类别数据</strong>，想找“一起出现”的模式，那么就是用关联规则挖掘，此时<strong>不考虑数值大小</strong>。</li><li>如果您面对的是像客户档案里的年龄、收入这样的<strong>数值数据</strong>，想看特征间的联动关系，那么就是用相关性分析，此时<strong>数值大小是分析的核心</strong>。</li></ul><p> 聚类有哪些算法，分别适用什么情况 </p><p>好的，这是一个非常实际且重要的问题。聚类算法有很多种，它们基于不同的数学原理，因此各自有其独特的优势和适用场景。没有“万能”的算法，选择哪种取决于您的数据特性和分析目标。</p><p>下面我将介绍几种最主流、最经典的聚类算法，并详细说明它们分别适用于什么情况。</p><hr><h3 id="1-K-均值聚类-K-Means-Clustering"><a href="#1-K-均值聚类-K-Means-Clustering" class="headerlink" title="1. K-均值聚类 (K-Means Clustering)"></a><strong>1. K-均值聚类 (K-Means Clustering)</strong></h3><p>这是最著名、最简单的聚类算法，属于“原型聚类”的代表，试图找到数据的“中心点”或“原型”。</p><ul><li><strong>工作原理：</strong><ol><li><strong>预设 K 值：</strong> 首先，您需要<strong>手动指定</strong>要将数据分成 K 个簇。</li><li><strong>随机初始化：</strong> 算法随机选择 K 个数据点作为初始的“簇中心”（Centroids）。</li><li><strong>迭代分配与更新：</strong><ul><li><strong>分配：</strong> 将每个数据点分配给离它最近的那个簇中心。</li><li><strong>更新：</strong> 重新计算每个簇所有数据点的平均值，将这个平均值作为新的簇中心。</li></ul></li><li><strong>循环此过程</strong>，直到簇中心不再变化为止。</li></ol></li><li><strong>适用情况：</strong><ul><li><strong>球状&#x2F;凸状数据分布：</strong> 当您的数据簇大致呈圆形或凸形时，K-Means 效果最好。</li><li><strong>大数据集：</strong> 算法非常高效，计算速度快，可扩展性好，适合处理大规模数据。</li><li><strong>簇的数量已知或可以预估：</strong> 当您对业务有一定了解，大致知道数据应该被分成几类时。</li></ul></li><li><strong>不适用&#x2F;缺点：</strong><ul><li><strong>需要预先指定K值：</strong> K值的选择对结果影响巨大，且往往难以确定。</li><li><strong>对初始值敏感：</strong> 随机选择的初始簇中心可能导致不同的聚类结果。</li><li><strong>对异常值敏感：</strong> 异常值会极大地影响簇中心的计算，可能导致簇中心偏离。</li><li><strong>无法处理非球状簇：</strong> 对于条形、环形或其他不规则形状的簇，K-Means 无法正确识别。</li></ul></li></ul><hr><h3 id="2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise"><a href="#2-DBSCAN-Density-Based-Spatial-Clustering-of-Applications-with-Noise" class="headerlink" title="2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"></a><strong>2. DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</strong></h3><p>DBSCAN 是“基于密度的聚类”的杰出代表，它不关心数据的“中心”，只关心数据的“密度”。</p><ul><li><strong>工作原理：</strong><ol><li><strong>定义邻域：</strong> 需要设定两个参数：邻域半径（<code>eps</code>）和最小点数（<code>MinPts</code>）。</li><li><strong>识别核心点：</strong> 如果一个点的 <code>eps</code> 半径内包含了至少 <code>MinPts</code> 个点，那么这个点就是一个“核心点”。</li><li><strong>扩展簇：</strong> 从任意一个核心点出发，不断寻找其密度可达（即在 <code>eps</code> 范围内）的其他核心点和边界点，将它们合并成一个簇。</li><li><strong>识别噪声：</strong> 那些不属于任何簇的点被标记为“噪声”或“异常点”。</li></ol></li><li><strong>适用情况：</strong><ul><li><strong>任意形状的簇：</strong> DBSCAN 的最大优势，可以发现非球状、不规则形状的簇（如月牙形、环形）。</li><li><strong>需要识别异常点&#x2F;噪声：</strong> 能够自动将密度稀疏的区域识别为噪声，非常适合用于异常检测。</li><li><strong>不需要预设簇数：</strong> 算法会自动根据数据密度发现簇的数量，非常方便。</li></ul></li><li><strong>不适用&#x2F;缺点：</strong><ul><li><strong>对参数敏感：</strong> <code>eps</code> 和 <code>MinPts</code> 这两个参数的选择至关重要，且需要经验来调整。</li><li><strong>对密度不均的数据集效果不佳：</strong> 如果数据中不同簇的密度差异很大，用同一套 <code>eps</code> 和 <code>MinPts</code> 参数就很难同时覆盖所有簇。</li><li><strong>高维数据处理困难：</strong> 在高维空间中，“密度”的定义会变得模糊（维度灾难），<code>eps</code> 参数难以设定。</li></ul></li></ul><hr><h3 id="3-层次聚类-Hierarchical-Clustering"><a href="#3-层次聚类-Hierarchical-Clustering" class="headerlink" title="3. 层次聚类 (Hierarchical Clustering)"></a><strong>3. 层次聚类 (Hierarchical Clustering)</strong></h3><p>层次聚类试图构建一个数据的层次结构，就像一棵树。它分为“自底向上”（凝聚型）和“自顶向下”（分裂型）两种。</p><ul><li><strong>工作原理（以凝聚型为例）：</strong><ol><li><strong>初始化：</strong> 将每个数据点都视为一个独立的簇。</li><li><strong>迭代合并：</strong> 找到所有簇中“距离”最近的两个簇，并将它们合并成一个新的簇。</li><li><strong>循环此过程</strong>，直到所有数据点最终被合并成一个最大的簇为止。</li><li><strong>结果呈现：</strong> 整个过程会形成一个树状图（Dendrogram），您可以根据需要，在任意高度“横切”一刀，来决定最终的簇数量。</li></ol></li><li><strong>适用情况：</strong><ul><li><strong>希望看到数据的层次关系：</strong> 当您不仅想知道数据分几类，还想了解类别之间的亲疏远近关系时，如生物学中的物种分类谱系。</li><li><strong>簇的数量不确定：</strong> 您可以通过观察树状图来决定一个合理的簇数量。</li><li><strong>小数据集：</strong> 对于小规模的数据，其结果直观且易于解释。</li></ul></li><li><strong>不适用&#x2F;缺点：</strong><ul><li><strong>计算复杂度高：</strong> 时间复杂度通常为 O(n³)，空间复杂度为 O(n²)，不适合处理大规模数据集。</li><li><strong>合并&#x2F;分裂不可逆：</strong> 一旦一个步骤的合并或分裂完成，后续就无法撤销，这可能导致次优的聚类结果。</li><li><strong>对距离度量敏感：</strong> 选择不同的距离计算方法（如欧氏距离、曼哈顿距离）和合并策略（如ward、complete）对结果影响很大。</li></ul></li></ul><hr><h3 id="4-高斯混合模型-Gaussian-Mixture-Model-GMM"><a href="#4-高斯混合模型-Gaussian-Mixture-Model-GMM" class="headerlink" title="4. 高斯混合模型 (Gaussian Mixture Model, GMM)"></a><strong>4. 高斯混合模型 (Gaussian Mixture Model, GMM)</strong></h3><p>GMM 是一种“基于概率分布的聚类”方法。它假设所有数据点都是从若干个不同的高斯分布（正态分布）中生成的。</p><ul><li><strong>工作原理：</strong><ol><li><strong>假设：</strong> 假定数据由 K 个高斯分布混合而成，每个分布代表一个簇。</li><li><strong>EM算法：</strong> 使用期望最大化（Expectation-Maximization, EM）算法进行迭代：<ul><li><strong>E-步（期望）：</strong> 估计每个数据点由每个高斯分布生成的<strong>概率</strong>。</li><li><strong>M-步（最大化）：</strong> 根据上一步的概率，更新每个高斯分布的参数（均值、方差、权重），使其能最大化地拟合数据。</li></ul></li><li><strong>循环此过程</strong>，直到模型收敛。最终，每个数据点可以被分配给生成它的概率最大的那个高斯分布（簇）。</li></ol></li><li><strong>适用情况：</strong><ul><li><strong>椭圆形簇：</strong> K-Means 只能处理球形簇，而GMM可以处理更具一般性的<strong>椭圆形簇</strong>。</li><li><strong>软聚类 (Soft Clustering)：</strong> GMM 不会硬性地将一个点分给某一类，而是给出一个<strong>概率</strong>。这对于处理簇边界模糊的数据非常有用。</li><li><strong>需要生成新样本：</strong> 由于GMM是一个生成模型，一旦训练完成，您可以用它来生成与原始数据分布类似的新数据点。</li></ul></li><li><strong>不适用&#x2F;缺点：</strong><ul><li><strong>计算复杂：</strong> EM算法的计算量比K-Means大。</li><li><strong>需要预设簇数：</strong> 和K-Means一样，需要指定高斯分布的数量。</li><li><strong>对非椭圆形数据效果不佳：</strong> 其假设是高斯分布，所以无法很好地拟合不规则形状的簇。</li></ul></li></ul><h3 id="总结：如何选择？"><a href="#总结：如何选择？" class="headerlink" title="总结：如何选择？"></a><strong>总结：如何选择？</strong></h3><table><thead><tr><th>算法</th><th>核心思想</th><th>簇形状</th><th>簇数量</th><th>规模</th><th>主要优点</th><th>主要缺点</th></tr></thead><tbody><tr><td><strong>K-Means</strong></td><td>基于中心</td><td><strong>球形&#x2F;凸形</strong></td><td><strong>需预设</strong></td><td>大</td><td>速度快，简单高效</td><td>对初始值&#x2F;异常值敏感，无法处理非球状</td></tr><tr><td><strong>DBSCAN</strong></td><td>基于密度</td><td><strong>任意形状</strong></td><td><strong>自动发现</strong></td><td>中&#x2F;大</td><td>能处理任意形状，能识别噪声</td><td>对参数和密度不均敏感</td></tr><tr><td><strong>层次聚类</strong></td><td>基于层次</td><td>任意形状</td><td>无需预设</td><td>小</td><td>提供层次关系，结果直观</td><td>计算复杂度高，不适合大数据</td></tr><tr><td><strong>GMM</strong></td><td>基于分布</td><td><strong>椭圆形</strong></td><td><strong>需预设</strong></td><td>中</td><td>提供属于各簇的概率（软聚类）</td><td>假设数据为高斯分布，计算较慢</td></tr></tbody></table><p>导出到 Google 表格</p><p><strong>一个简单的决策流程：</strong></p><ol><li><strong>数据量大，且簇的形状大致是球形？</strong> -&gt; <strong>首选 K-Means</strong>。</li><li><strong>不确定要分几类，簇的形状可能不规则，且想识别噪声点？</strong> -&gt; <strong>首选 DBSCAN</strong>。</li><li><strong>数据量不大，且想了解数据内在的层次结构？</strong> -&gt; <strong>使用层次聚类</strong>。</li><li><strong>簇的形状可能是椭圆形，或者希望得到一个点属于各个簇的概率？</strong> -&gt; <strong>考虑 GMM</strong>。</li></ol><p> 什么是簇的形状 </p><p>当然，这是一个非常好的问题！在聚类分析中，我们经常提到“簇的形状”，这听起来可能有点抽象，因为数据本身是没有物理形状的。</p><p>“簇的形状”其实是一个非常直观的比喻，它指的是<strong>数据点在特征空间（即图表上）的分布模式或排列形态</strong>。</p><p>为了方便理解，我们通常把它想象在一个二维的散点图上（例如，X轴是“收入”，Y轴是“消费频率”）。每个数据点就是图上的一个点，而一群点的聚集形态，就是我们所说的“簇的形状”。</p><p>下面我们来详细看一下几种典型的簇形状，以及它们如何影响算法的选择。</p><hr><h3 id="1-球形-凸形-Spherical-Convex-Shape"><a href="#1-球形-凸形-Spherical-Convex-Shape" class="headerlink" title="1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)"></a><strong>1. 球形&#x2F;凸形 (Spherical &#x2F; Convex Shape)</strong></h3><p>这是最简单、最理想的一种形状。</p><ul><li><strong>直观描述：</strong> 数据点紧密地聚集在一个大致<strong>圆形</strong>的区域内，密度从中心向四周均匀递减。从任何角度看，这个簇都是一个“实心”的团，没有凹陷。</li><li><strong>例子：</strong> 测量一大群同龄男性的身高和体重，这些数据点很可能会形成一个球状或椭圆形的簇。</li><li><strong>适合的算法：</strong> <strong>K-Means</strong>。因为K-Means的原理就是找到一个“中心点”，然后将离这个中心点近的所有点都归为一类。这种“离中心点近”的逻辑，天然地就定义出了一个球形的边界。</li></ul><hr><h3 id="2-椭圆形-Elliptical-Shape"><a href="#2-椭圆形-Elliptical-Shape" class="headerlink" title="2. 椭圆形 (Elliptical Shape)"></a><strong>2. 椭圆形 (Elliptical Shape)</strong></h3><p>这是球形的一种延伸，在现实世界中更常见。</p><ul><li><strong>直观描述：</strong> 数据点聚集在一个<strong>椭圆形</strong>的区域内。这通常意味着不同特征（坐标轴）之间的尺度不同，或者特征之间存在一定的相关性。</li><li><strong>例子：</strong> 客户的“年龄”和“收入”数据，很可能形成一个椭圆形，因为年龄和收入是正相关的，数据点会沿着一个方向被“拉长”。</li><li><strong>适合的算法：</strong> **高斯混合模型 (GMM)**。GMM假设数据是从高斯分布（正态分布）中生成的，而高斯分布的等高线图正好是椭圆形。它可以很好地拟合这种被拉伸或旋转的簇。K-Means在这种情况下效果会变差，因为它总想画一个“正圆”的边界。</li></ul><hr><h3 id="3-任意形状-Arbitrary-Shape"><a href="#3-任意形状-Arbitrary-Shape" class="headerlink" title="3. 任意形状 (Arbitrary Shape)"></a><strong>3. 任意形状 (Arbitrary Shape)</strong></h3><p>这是最复杂，但也最有趣的一类形状，它们不是一个“实心”的团。</p><ul><li><strong>直观描述：</strong> 数据的分布呈现出各种不规则的形态，比如条形、环形、月牙形、S形等。这些形状通常是“非凸”的，即内部有“凹陷”或“空洞”。</li><li><strong>例子：</strong><ul><li><strong>条形&#x2F;线性：</strong> 河流沿岸的村庄分布。</li><li><strong>环形&#x2F;同心圆：</strong> 靶心周围的弹孔分布，可能有一圈密集的9环和一圈密集的7环，但8环和靶心是空的。</li><li><strong>月牙形：</strong> 生物学或天文学中某些特定的数据模式。</li></ul></li><li><strong>适合的算法：</strong> <strong>DBSCAN</strong>。DBSCAN不关心簇的中心在哪里，只关心密度。只要点与点之间足够近，能够“连接”起来，它就能把它们都识别成一个簇。因此，它能像“贪吃蛇”一样，沿着任何形状的路径把簇找出来。</li></ul><p><em>(上图展示了不同算法在处理任意形状簇时的表现。可以看到K-Means完全失败了，而DBSCAN和层次聚类则表现得很好。)</em></p><hr><h3 id="为什么不同算法偏好不同形状？"><a href="#为什么不同算法偏好不同形状？" class="headerlink" title="为什么不同算法偏好不同形状？"></a><strong>为什么不同算法偏好不同形状？</strong></h3><p>这源于它们底层的数学逻辑：</p><ul><li><strong>K-Means的逻辑是“距离中心点近”</strong>：这个逻辑决定了它的势力范围必然是球形的。对于两个月牙形的簇，K-Means会错误地将距离各自中心点近的点划分在一起，从而把两个月牙都“切开”，并错误地组合在一起。</li><li><strong>DBSCAN的逻辑是“密度相连”</strong>：它的逻辑是“我的邻居是我的朋友，我邻居的邻居也是我的朋友”。这个逻辑与整体形状无关，只关心局部有多拥挤。因此，只要数据点能形成一条密度足够高的“小径”，DBSCAN就能沿着这条小径把整个簇识别出来，无论小径最终弯曲成了什么形状。</li></ul><h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>形状类型</th><th>直观描述</th><th>适合的算法</th><th>不适合的算法</th></tr></thead><tbody><tr><td><strong>球形&#x2F;凸形</strong></td><td>圆形的、实心的团</td><td><strong>K-Means</strong>, GMM</td><td>DBSCAN (也能用，但有点大材小用)</td></tr><tr><td><strong>椭圆形</strong></td><td>椭圆形的、被拉伸的团</td><td><strong>GMM</strong>, 层次聚类</td><td>K-Means (效果会变差)</td></tr><tr><td><strong>任意形状</strong></td><td>条形、环形、月牙形等不规则形状</td><td><strong>DBSCAN</strong>, 层次聚类</td><td><strong>K-Means</strong> (完全不适用), GMM</td></tr></tbody></table><p><strong>核心要点：</strong> 在选择聚类算法前，最好先通过<strong>数据可视化</strong>（比如抽样画出特征的散点图）对数据的分布形态有一个初步的了解。看到数据的“形状”，可以极大地帮助你选择正确的“工具”（算法），从而得到有意义的分析结果。</p><p> 降维的算法有哪些 </p><p>当然。降维算法种类繁多，它们可以被划分为两大主流派系：**特征选择（Feature Selection）**和**特征提取（Feature Extraction）**。</p><p>下面我将为您详细介绍这两个派系下的主要算法。</p><hr><h3 id="两大降维派系"><a href="#两大降维派系" class="headerlink" title="两大降维派系"></a><strong>两大降维派系</strong></h3><p>首先，理解这两个派系的根本区别至关重要：</p><ol><li><strong>特征选择 (Feature Selection):</strong><ul><li><strong>思想：</strong> 像一位“评审员”，直接从原始的特征中<strong>挑选出</strong>一个最优的子集，而<strong>不改变原始特征本身</strong>。被选中的特征保持原有的物理意义。</li><li><strong>优点：</strong> 可解释性强，因为你清楚地知道留下了哪些原始特征。</li><li><strong>例子：</strong> 从100个客户特征中，挑选出最重要的10个（如“收入”、“年龄”、“消费频率”等）。</li></ul></li><li><strong>特征提取 (Feature Extraction):</strong><ul><li><strong>思想：</strong> 像一位“炼金术士”，将原始的多个特征进行<strong>融合与重组</strong>，创造出全新的、数量更少的特征。新特征是原始特征的某种组合。</li><li><strong>优点：</strong> 能够保留更多原始数据的信息，通常降维效果更好。</li><li><strong>缺点：</strong> 新生成的特征通常失去了原有的物理意义，可解释性变差。</li><li><strong>例子：</strong> 将“身高”、“体重”、“臂展”等多个特征融合成一个新的特征，称为“体格因子”。</li></ul></li></ol><hr><h3 id="一、特征选择-Feature-Selection-的方法"><a href="#一、特征选择-Feature-Selection-的方法" class="headerlink" title="一、特征选择 (Feature Selection) 的方法"></a><strong>一、特征选择 (Feature Selection) 的方法</strong></h3><p>特征选择本身不是一个单一算法，而是一类方法的统称，主要分为三类：</p><h4 id="1-过滤法-Filter-Methods"><a href="#1-过滤法-Filter-Methods" class="headerlink" title="1. 过滤法 (Filter Methods)"></a><strong>1. 过滤法 (Filter Methods)</strong></h4><p>在任何模型训练之前，先对数据进行预处理，根据特征的统计属性（如方差、相关性、信息量）来“过滤”掉一部分特征。</p><ul><li><strong>方差选择法 (Variance Threshold):</strong> 优先删除那些方差很小甚至为零的特征，因为这些特征在所有样本上几乎没有变化，包含的信息量很少。</li><li><strong>相关系数法 (Correlation Coefficient):</strong> 计算特征之间的相关性，如果两个特征高度相关（如“房屋面积”和“房间数量”），则可以只保留其中一个。</li><li><strong>卡方检验 (Chi-Squared Test):</strong> 用于评估类别特征对于类别标签的相关性。</li><li><strong>信息增益 (Information Gain):</strong> 常用于决策树，评估一个特征对于数据集不确定性的减少程度。</li></ul><h4 id="2-包装法-Wrapper-Methods"><a href="#2-包装法-Wrapper-Methods" class="headerlink" title="2. 包装法 (Wrapper Methods)"></a><strong>2. 包装法 (Wrapper Methods)</strong></h4><p>将特征选择过程“包装”在一个特定的机器学习模型周围，通过评估模型性能来决定保留哪些特征。</p><ul><li><strong>递归特征消除 (Recursive Feature Elimination, RFE):</strong><ul><li><strong>思想：</strong> 首先用所有特征训练一个模型，然后移除最不重要的特征，再用剩下的特征重新训练模型……如此循环，直到剩下预设数量的特征。</li><li><strong>优点：</strong> 效果通常很好。</li><li><strong>缺点：</strong> 计算成本非常高，因为需要反复训练模型。</li></ul></li></ul><h4 id="3-嵌入法-Embedded-Methods"><a href="#3-嵌入法-Embedded-Methods" class="headerlink" title="3. 嵌入法 (Embedded Methods)"></a><strong>3. 嵌入法 (Embedded Methods)</strong></h4><p>在模型训练的过程中<strong>自动</strong>进行特征选择。</p><ul><li><strong>L1 正则化 (LASSO):</strong><ul><li><strong>思想：</strong> 在模型的损失函数中加入L1惩罚项，它倾向于将不重要的特征的系数直接压缩到<strong>零</strong>。训练完成后，系数不为零的特征就是被选中的特征。</li><li><strong>优点：</strong> 高效且常用，将特征选择与模型训练融为一体。</li></ul></li><li><strong>基于树模型的方法 (Tree-based Models):</strong><ul><li><strong>思想：</strong> 决策树、随机森林、梯度提升树等模型在构建过程中，会自然地计算每个特征的重要性（Feature Importance）。我们可以直接根据这个重要性排名来选择特征。</li></ul></li></ul><hr><h3 id="二、特征提取-Feature-Extraction-的算法"><a href="#二、特征提取-Feature-Extraction-的算法" class="headerlink" title="二、特征提取 (Feature Extraction) 的算法"></a><strong>二、特征提取 (Feature Extraction) 的算法</strong></h3><p>这是“降维算法”这个词通常所指代的，主要分为线性和非线性两大类。</p><h4 id="A-线性降维算法"><a href="#A-线性降维算法" class="headerlink" title="A. 线性降维算法"></a><strong>A. 线性降维算法</strong></h4><p>假设数据中的主要结构可以通过一个线性子空间来描述。</p><ul><li><strong>1. 主成分分析 (Principal Component Analysis, PCA)</strong><ul><li><strong>核心思想：</strong> 寻找数据中<strong>方差最大</strong>的方向，将这些方向作为新的坐标轴（主成分）。它旨在最大程度地保留原始数据的“可分性”（方差）。</li><li><strong>适用场景：</strong> <strong>最常用、最经典</strong>的降维算法，是大多数情况下的<strong>首选基准</strong>。广泛用于数据预处理、去噪和可视化。</li></ul></li><li><strong>2. 线性判别分析 (Linear Discriminant Analysis, LDA)</strong><ul><li><strong>核心思想：</strong> <strong>（注意：这是一个有监督算法）</strong> 与PCA不同，LDA的目标是寻找一个能<strong>最大化类间距离、同时最小化类内距离</strong>的投影方向。通俗讲，就是让不同类别的数据点在投影后分得尽可能开。</li><li><strong>适用场景：</strong> 主要用于<strong>分类问题</strong>的预处理，当你有类别标签时，降维的目标是为了更好地分类。</li></ul></li></ul><h4 id="B-非线性降维算法（流形学习-Manifold-Learning）"><a href="#B-非线性降维算法（流形学习-Manifold-Learning）" class="headerlink" title="B. 非线性降维算法（流形学习 Manifold Learning）"></a><strong>B. 非线性降维算法（流形学习 Manifold Learning）</strong></h4><p>假设高维数据实际上分布在一个低维的、弯曲的“流形”上。目标是“展开”这个流形，找到其内在的低维结构。</p><ul><li><strong>1. t-分布随机邻域嵌入 (t-SNE)</strong><ul><li><strong>核心思想：</strong> 重点保持数据点的<strong>局部邻域结构</strong>。它假设高维空间中距离近的点，在低维空间中也应该距离近。</li><li><strong>适用场景：</strong> <strong>数据可视化</strong>的王者。非常擅长将高维数据降到2D或3D，以观察其聚类结构。<strong>注意：</strong> t-SNE主要用于可视化，降维后的点间距离不具有真实意义，不建议直接作为其他模型的输入。</li></ul></li><li><strong>2. 均匀流形逼近与投影 (Uniform Manifold Approximation and Projection, UMAP)</strong><ul><li><strong>核心思想：</strong> 与t-SNE类似，但基于更坚实的数学理论。它在保持局部结构的同时，能更好地保留数据的<strong>全局结构</strong>。</li><li><strong>适用场景：</strong> 现代的<strong>数据可视化</strong>首选，通常比t-SNE<strong>速度更快</strong>，且可视化效果更好。</li></ul></li><li><strong>3. 等距映射 (Isometric Mapping, Isomap)</strong><ul><li><strong>核心思想：</strong> 尝试保持点之间的<strong>测地线距离</strong>（即沿着流形表面的最短距离），而不是欧氏距离（直线距离）。</li><li><strong>适用场景：</strong> 处理像“瑞士卷”这样被卷曲起来的数据集效果很好。</li></ul></li></ul><h3 id="总结：如何选择？-1"><a href="#总结：如何选择？-1" class="headerlink" title="总结：如何选择？"></a><strong>总结：如何选择？</strong></h3><table><thead><tr><th>类别</th><th>算法&#x2F;方法</th><th>核心思想</th><th>优点</th><th>主要应用场景</th></tr></thead><tbody><tr><td><strong>特征选择</strong></td><td><strong>过滤&#x2F;包装&#x2F;嵌入法</strong></td><td>挑选原始特征子集</td><td>可解释性强</td><td>当需要理解哪些原始特征最重要时</td></tr><tr><td><strong>线性提取</strong></td><td><strong>PCA (主成分分析)</strong></td><td>最大化方差</td><td><strong>通用、高效、稳定</strong></td><td><strong>通用的降维首选</strong>，数据去噪</td></tr><tr><td><strong>线性提取</strong></td><td><strong>LDA (线性判别分析)</strong></td><td>最大化类别分离度</td><td>(有监督) 降维后利于分类</td><td>分类任务的预处理</td></tr><tr><td><strong>非线性提取</strong></td><td><strong>t-SNE &#x2F; UMAP</strong></td><td>保持局部邻域结构</td><td><strong>可视化效果极佳</strong></td><td><strong>高维数据可视化</strong>，探索聚类结构</td></tr><tr><td><strong>非线性提取</strong></td><td><strong>Isomap</strong></td><td>保持测地线距离</td><td>能“展开”流形</td><td>学术研究，处理特定流形数据</td></tr></tbody></table><p><strong>一个简单的实践建议：</strong></p><ol><li><strong>不确定用什么？</strong> 从 <strong>PCA</strong> 开始。</li><li><strong>需要很强的可解释性？</strong> 尝试<strong>特征选择</strong>方法（如L1正则化或随机森林特征重要性）。</li><li><strong>主要目标是可视化？</strong> 使用 <strong>UMAP</strong> 或 <strong>t-SNE</strong>。</li><li><strong>怀疑数据有复杂的非线性结构？</strong> 可以探索Isomap等其他流形学习方法。</li></ol><h1 id="5-什么是数据的方差-信息量"><a href="#5-什么是数据的方差-信息量" class="headerlink" title="5.什么是数据的方差&#x2F;信息量"></a>5.什么是数据的方差&#x2F;信息量</h1><p>简单来说，<strong>方差（Variance）</strong> 在这里就是用来衡量您的某一个描述符（特征）在所有14091个材料中，<strong>数值分布的“离散程度”或“差异性大小”</strong>。</p><p>高方差 &#x3D; 差异性大 &#x3D; 信息量丰富 低方差 &#x3D; 差异性小 &#x3D; 信息量贫乏</p><hr><h3 id="让我们用一个具体的描述符来举例"><a href="#让我们用一个具体的描述符来举例" class="headerlink" title="让我们用一个具体的描述符来举例"></a>让我们用一个具体的描述符来举例</h3><p>在您那1559个特征中，我们假设有一个特征叫做 <strong>“密度 (Density)”</strong>。现在，我们只看这一列数据，它包含了14091个不同材料的密度值。</p><ul><li><p><strong>情况一：低方差 (Low Variance)</strong> 假设您计算出的所有14091个材料的密度值都<strong>非常接近</strong>，比如都在 <code>4.5 g/cm³</code> 到 <code>4.6 g/cm³</code> 这个极小的范围内。</p><p>在这种情况下，“密度”这个特征的<strong>方差就非常低</strong>。它告诉我们什么信息呢？它告诉我们，在“密度”这个属性上，您所有的材料都差不多，没什么区别。因此，这个特征对于区分不同材料、或者预测它们的性质，<strong>几乎没有帮助</strong>。它的“信息量”很贫乏。</p></li><li><p><strong>情况二：高方差 (High Variance)</strong> 现在，假设您的14091个材料的密度值<strong>分布得非常广泛</strong>，从很轻的 <code>1.0 g/cm³</code> 到非常重的 <code>22.0 g/cm³</code> 都有。</p><p>在这种情况下，“密度”这个特征的<strong>方差就非常高</strong>。它提供了一个非常有用的信息维度来区分您的材料。一个密度为2的材料和一个密度为20的材料，显然在物理性质上有着天壤之别。因此，这个特征<strong>信息量很丰富</strong>，对于后续的机器学习建模非常有价值。</p></li></ul><h3 id="从单个特征的方差到“总方差”"><a href="#从单个特征的方差到“总方差”" class="headerlink" title="从单个特征的方差到“总方差”"></a>从单个特征的方差到“总方差”</h3><p>现在，请想象一下，您的1559个特征（密度、平均键长、堆积因子、平均电负性…）中的<strong>每一个</strong>，都有自己独立的方差值。</p><ul><li>有些特征（像情况二的密度）方差很高，是很好的“区分器”。</li><li>有些特征（像情况一的密度）方差很低，是“噪音”或者说用处不大的信息。</li></ul><p><strong>总方差 (Total Variance)</strong> 就是把这<strong>所有1559个特征各自的方差简单地加在一起</strong>。 这个“总方差”就代表了您整个数据集在所有维度上<strong>信息量的总和</strong>。这就是我们之前比喻中的“所有原材料的总营养价值”。</p><h3 id="PCA是如何利用“方差”的？"><a href="#PCA是如何利用“方差”的？" class="headerlink" title="PCA是如何利用“方差”的？"></a>PCA是如何利用“方差”的？</h3><p>PCA的聪明之处在于，它不仅仅是看原始的1559个方向（密度、键长等）的方差，而是要在您那个1559维的复杂空间里，<strong>寻找到全新的、最优的方向</strong>。</p><ol><li><strong>主成分1 (PC1)<strong>：PCA找到的第一个新方向（PC1），是所有可能方向中，能让您的14091个数据点投影上去后</strong>分布得最开、差异性最大、方差最高</strong>的那个方向。它捕捉了整个数据集中<strong>最主要的变化来源</strong>。</li><li><strong>主成分2 (PC2)<strong>：在所有与PC1垂直的方向中，PCA找到能让数据投影后方差次高的那个方向。它捕捉了数据中</strong>第二重要的变化来源</strong>。</li><li>…以此类推。</li></ol><p>所以，当我们说“PC1解释了30%的方差”时，我们的意思就是： <strong>“沿着PC1这个新维度观察数据，其离散程度（方差）占到了整个数据集总离散程度（总方差）的30%。”</strong></p><p><strong>总结：</strong> 在您的数据中，“方差”衡量了<strong>一个描述符在区分不同材料方面的能力</strong>。PCA通过寻找数据中方差最大的新方向（主成分），来智能地将最有价值的信息（差异性）浓缩到少数几个维度中，从而实现高效、有意义的降维。</p><h1 id="6-聚类"><a href="#6-聚类" class="headerlink" title="6. 聚类"></a>6. 聚类</h1><p>聚类就是在降维后的高维空间中识别簇并划分</p><h1 id="7-DBSCAN聚类方法"><a href="#7-DBSCAN聚类方法" class="headerlink" title="7. DBSCAN聚类方法"></a>7. DBSCAN聚类方法</h1><p>两个重要参数</p><p>**<code>eps</code><strong>邻域半径社交距离 &#x2F; 一臂之长定义了“邻居”的范围有多大。</strong><code>min_samples</code>**核心点最小样本数形成核心圈子的最少人数定义了“多密集”才算是一个簇的核心。</p><h3 id="K距离图（K-distance-Graph）的含义"><a href="#K距离图（K-distance-Graph）的含义" class="headerlink" title="K距离图（K-distance Graph）的含义"></a>K距离图（K-distance Graph）的含义</h3><p>首先，我们再回顾一下这张图的含义：</p><ul><li><strong>Y轴</strong>：距离值。代表一个点要“走”多远，才能找到它的第 <code>min_samples</code> 个邻居。</li><li><strong>X轴</strong>：数据集中的所有点。为了方便观察，我们把这些点按照Y轴的距离值<strong>从小到大</strong>重新排了序。</li></ul><p>所以，这张图的曲线实际上展示了数据集中所有点的“<strong>局部密度</strong>”分布情况：</p><ul><li><strong>曲线左侧（Y值很低）</strong>：这些点很“幸福”，不用走多远就能找到足够多的邻居。这说明它们身处<strong>人口稠密的市中心（密集区域&#x2F;簇内）</strong>。</li><li><strong>曲线右侧（Y值很高）</strong>：这些点很“孤独”，需要走很远才能找到足够多的邻居。这说明它们身处<strong>人烟稀少的郊区或乡村（稀疏区域&#x2F;噪声）</strong>。</li></ul><h3 id="2-“拐点”（Elbow）的特殊意义"><a href="#2-“拐点”（Elbow）的特殊意义" class="headerlink" title="2. “拐点”（Elbow）的特殊意义"></a>2. “拐点”（Elbow）的特殊意义</h3><p>现在，我们来看“拐点”。</p><p>拐点是曲线上斜率<strong>变化最剧烈</strong>的地方。在这个点：</p><ul><li><strong>拐点之前</strong>：曲线很平缓。这代表了所有“市中心”的居民。虽然大家住的地方有好有坏，但总体上邻里距离都差不多，都比较近。</li><li><strong>拐点之后</strong>：曲线突然变得非常陡峭。这代表了那些“郊区”的居民。他们的邻里距离突然变得非常大。</li></ul><p>因此，这个<strong>拐点</strong>就理所当然地成为了<strong>区分“市中心居民”和“郊区居民”的最佳分界线</strong>。它就像是这个城市的“<strong>天然边界</strong>”。</p><h3 id="3-为什么这个边界就是-eps？"><a href="#3-为什么这个边界就是-eps？" class="headerlink" title="3. 为什么这个边界就是 eps？"></a>3. 为什么这个边界就是 <code>eps</code>？</h3><p>现在我们回到DBSCAN算法的目标：找到由<strong>核心点</strong>组成的簇。</p><p><strong>核心点的定义是</strong>：在 <code>eps</code> 半径内，至少有 <code>min_samples</code> 个邻居。</p><ul><li><strong>如果我们把 <code>eps</code> 设置为拐点的Y轴值</strong>（也就是那个临界距离）：<ul><li>对于所有在<strong>拐点左边</strong>的点，它们找到第 <code>min_samples</code> 个邻居的实际距离都<strong>小于或等于</strong>我们设定的 <code>eps</code>。这意味着，它们都满足成为<strong>核心点</strong>的条件！</li><li>对于所有在<strong>拐点右边</strong>的点，它们找到第 <code>min_samples</code> 个邻居的实际距离都<strong>大于</strong>我们设定的 <code>eps</code>。这意味着，它们无法满足成为核心点的条件，因此它们理应被当作<strong>噪声点</strong>。</li></ul></li></ul><h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p><strong>拐点处的Y值，是那个能够“一刀切”地将大部分核心点和大部分噪声点分开的理想距离阈值。</strong></p><ul><li>选择一个比拐点<strong>低很多</strong>的<code>eps</code>，会导致连很多“市中心”的点都因为邻域太小而被误判为噪声。</li><li>选择一个比拐点<strong>高很多</strong>的<code>eps</code>，会导致把很多“郊区”的噪声点也因为邻域太大而被错误地圈进了城市里。</li></ul><p>因此，拐点是平衡这两者的最佳选择，它代表了我们能容忍的、构成一个“密集区域”的<strong>最大邻居距离</strong>。</p><h1 id="8-PCA与UMAP对比"><a href="#8-PCA与UMAP对比" class="headerlink" title="8. PCA与UMAP对比"></a>8. PCA与UMAP对比</h1><table><thead><tr><th>特性</th><th><strong>PCA (主成分分析)</strong></th><th align="left"><strong>UMAP (均匀流形近似与投影)</strong></th></tr></thead><tbody><tr><td><strong>核心原理</strong></td><td><strong>线性</strong> (Linear)</td><td align="left"><strong>非线性 &#x2F; 流形学习</strong> (Non-linear)</td></tr><tr><td><strong>主要目标</strong></td><td><strong>最大化方差</strong> (Maximize Variance)</td><td align="left"><strong>保持拓扑结构</strong> (Preserve Topology)</td></tr><tr><td></td><td>(保留数据整体的“伸展”方向)</td><td align="left">(保持数据点之间的“邻里关系”)</td></tr><tr><td><strong>关注点</strong></td><td><strong>全局结构</strong> (Global Structure)</td><td align="left"><strong>局部结构</strong>为主，兼顾全局结构</td></tr><tr><td><strong>可解释性</strong></td><td><strong>强</strong> (High)</td><td align="left"><strong>弱</strong> (Low) &#x2F; 黑盒</td></tr><tr><td></td><td>(每个主成分都是原始特征的线性组合)</td><td align="left">(降维后的坐标轴没有直观物理解释)</td></tr><tr><td><strong>计算速度</strong></td><td><strong>非常快</strong> (Very Fast)</td><td align="left"><strong>比PCA慢，但比t-SNE快得多</strong> (Slower)</td></tr><tr><td><strong>主要应用</strong></td><td><strong>数据预处理、去噪、特征工程</strong></td><td align="left"><strong>数据可视化、探索性数据分析</strong></td></tr></tbody></table><h1 id="9-KDBSCAN中的参数"><a href="#9-KDBSCAN中的参数" class="headerlink" title="9. KDBSCAN中的参数"></a>9. KDBSCAN中的参数</h1><p>社交范围和“圈子定义”</p><p><code>DBSCAN</code> 中这两个至关重要的参数：<code>eps</code> 和 <code>min_samples</code>。</p><p><code>DBSCAN</code> 是一种基于<strong>密度</strong>的聚类算法，而这两个参数正是用来定义“密度”的。您可以把它们想象成是在照片中识别“人群”的两个规则。</p><hr><h3 id="一个生动的比喻：寻找派对上的社交圈"><a href="#一个生动的比喻：寻找派对上的社交圈" class="headerlink" title="一个生动的比喻：寻找派对上的社交圈"></a>一个生动的比喻：寻找派对上的社交圈</h3><p>想象一下，您正在分析一张大型派对的俯瞰照片，照片上有很多客人（您的数据点）。您的任务是找出哪些客人形成了<strong>“社交圈”（簇）</strong>，哪些是<strong>“独行者”（噪声点）</strong>。</p><h4 id="1-eps-Epsilon-邻域半径"><a href="#1-eps-Epsilon-邻域半径" class="headerlink" title="1. eps (Epsilon) - 邻域半径"></a>1. <code>eps</code> (Epsilon) - 邻域半径</h4><ul><li><strong>定义</strong>：<code>eps</code> 是一个距离值，它定义了一个点的“邻近”范围。可以把它想象成每个人的<strong>“个人空间”或“社交距离”</strong>。</li><li><strong>作用</strong>：对于任何一个客人，我们以他&#x2F;她为中心，画一个半径为 <code>eps</code> 的圆。所有落在这个圆里的其他客人，都算是他&#x2F;她的“邻居”。</li></ul><p><strong>参数调整的影响：</strong></p><ul><li><strong>增加 <code>eps</code> 值 (例如，从 <code>eps=10</code> 增加到 <code>eps=20</code>)</strong><ul><li><strong>效果</strong>：相当于扩大了每个人的“社交距离”。原来一些距离较远的圈子现在可能会因为彼此的“社交圈”重叠而<strong>合并成一个更大的圈子</strong>。</li><li><strong>结果</strong>：<ul><li>簇的数量会<strong>减少</strong>，但每个簇的规模会<strong>变大</strong>。</li><li>更多的点会被纳入到簇中，被识别为<strong>噪声点的数量会减少</strong>。</li><li><strong>极端情况</strong>：如果 <code>eps</code> 太大，所有点可能会被合并成一个巨大的簇。</li></ul></li></ul></li><li><strong>降低 <code>eps</code> 值 (例如，从 <code>eps=10</code> 降低到 <code>eps=5</code>)</strong><ul><li><strong>效果</strong>：相当于缩小了每个人的“社交距离”，要求大家站得更近才算邻居。</li><li><strong>结果</strong>：<ul><li>一个原本较大的、松散的圈子可能会因为内部不够紧密而<strong>分裂成几个更小的、更紧密的簇</strong>。</li><li>簇的数量可能会<strong>增加</strong>，但每个簇的规模会<strong>变小</strong>。</li><li>由于标准变严，很多点找不到足够多的邻居，被识别为<strong>噪声点的数量会增加</strong>。</li></ul></li></ul></li></ul><h4 id="2-min-samples-核心点最小样本数"><a href="#2-min-samples-核心点最小样本数" class="headerlink" title="2. min_samples - 核心点最小样本数"></a>2. <code>min_samples</code> - 核心点最小样本数</h4><ul><li><strong>定义</strong>：<code>min_samples</code> 是一个整数，它定义了要形成一个“密集区域”（即一个簇的核心）所需要的<strong>最少邻居数量</strong>。</li><li><strong>作用</strong>：一个点如果在他自己的 <code>eps</code> 邻域内（包括他自己）拥有至少 <code>min_samples</code> 个点，那么这个点就被称作<strong>“核心点”</strong>。簇就是由这些核心点以及它们的邻居连接而成的。可以把它理解为“<strong>几个人才能算一个真正的社交圈</strong>”。</li></ul><p><strong>参数调整的影响：</strong></p><ul><li><strong>增加 <code>min_samples</code> 值 (例如，从 <code>min_samples=40</code> 增加到 <code>min_samples=80</code>)</strong><ul><li><strong>效果</strong>：相当于提高了形成“社交圈”的门槛。原来三四十个人就算一个圈子，现在必须凑够八十人才行。</li><li><strong>结果</strong>：<ul><li>只有<strong>密度非常高</strong>的区域才会被识别为簇。</li><li>一些原本被认为是簇的稀疏区域，现在可能会因为“人数”不够而被看作是<strong>噪声</strong>。</li><li><strong>噪声点的数量会增加</strong>。这个参数是控制噪声的主要手段。</li></ul></li></ul></li><li><strong>降低 <code>min_samples</code> 值 (例如，从 <code>min_samples=40</code> 降低到 <code>min_samples=20</code>)</strong><ul><li><strong>效果</strong>：相当于降低了形成“社交圈”的门槛。</li><li><strong>结果</strong>：<ul><li>即使在<strong>密度较低</strong>的区域也可能形成簇。</li><li>簇的数量可能会<strong>增加</strong>。</li><li><strong>噪声点的数量会减少</strong>。</li><li><strong>极端情况</strong>：如果 <code>min_samples</code> 太小（如1或2），几乎所有点都会成为核心点，可能会导致噪声点也被错误地划分进簇中。</li></ul></li></ul></li></ul><hr><h3 id="总结表格"><a href="#总结表格" class="headerlink" title="总结表格"></a>总结表格</h3><table><thead><tr><th>参数</th><th>增加该值的影响</th><th>降低该值的影响</th></tr></thead><tbody><tr><td><strong><code>eps</code></strong> (社交距离)</td><td>簇变大、变少；噪声点变少</td><td>簇变小、变多；噪声点变多</td></tr><tr><td><strong><code>min_samples</code></strong> (成圈人数)</td><td>簇变少、更紧密；噪声点变多</td><td>簇变多、更松散；噪声点变少</td></tr></tbody></table><p>导出到 Google 表格</p><p><strong>您的设置 <code>eps=10, min_samples=40</code> 意味着：</strong> “对于任何一个材料点，如果在10个单位的距离内，总共有至少40个其他材料点，那么这里就形成了一个高密度的核心区域。”</p><p>调整这两个参数通常需要一些实验，通过观察我们之前讨论过的“按簇标签着色的散点图”来判断聚类效果是否符合您的预期。</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="10-UMAP-HDBSCAN-or-PCA-HDBSCAN"><a href="#10-UMAP-HDBSCAN-or-PCA-HDBSCAN" class="headerlink" title="10. UMAP+HDBSCAN or PCA+HDBSCAN"></a>10. UMAP+HDBSCAN or PCA+HDBSCAN</h1><p>对于你当前的数据情况——<strong>10000个样本，1500个维度</strong>——我的建议是：</p><p><strong>优先使用 UMAP + HDBSCAN。</strong></p><p>下面是详细的理由和为你量身定制的操作策略。</p><hr><h3 id="为什么优先选择-UMAP-HDBSCAN？"><a href="#为什么优先选择-UMAP-HDBSCAN？" class="headerlink" title="为什么优先选择 UMAP + HDBSCAN？"></a><strong>为什么优先选择 UMAP + HDBSCAN？</strong></h3><ol><li><strong>高维数据的非线性本质</strong>:<ul><li>在一个高达1500维的空间里，数据点之间的关系极不可能是简单的线性关系。真实世界的高维数据（如图像特征、基因表达、用户行为向量等）几乎总是分布在某个复杂的、扭曲的低维“流形”上。</li><li>PCA是一个<strong>线性</strong>工具，它会试图用“直线”（超平面）来切割和投影数据。这对于捕捉非线性结构是无效的，很可能会将原本分离的簇错误地重叠在一起，从而“污染”数据，让后续的HDBSCAN无法工作。</li><li>UMAP专门为发现这种<strong>非线性</strong>流形结构而设计。它能够“解开”高维空间中缠绕的数据结构，在低维空间中更真实地保留数据点之间的邻近关系。这正是HDBSCAN这种基于密度的聚类算法最需要的信息。</li></ul></li><li><strong>“维度灾难”的完美解药</strong>:<ul><li>在1500维下，任何基于距离的算法（包括HDBSCAN）都会因为“维度灾难”而失效——所有点到其他点的距离都趋向于相等，密度变得没有意义。因此，降维是<strong>必须</strong>的步骤。</li><li>UMAP在降维时，其核心目标就是保留数据的<strong>拓扑结构</strong>（哪些点是邻居，哪些簇是独立的）。这与HDBSCAN的目标（寻找密度相连的区域）完美契合。可以说，UMAP为HDBSCAN创造了最理想的工作环境。</li></ul></li><li><strong>计算性能完全可接受</strong>:<ul><li>你的数据量是10000，这对于现代的UMAP实现来说是完全可以接受的。虽然UMAP会比PCA慢，但在这个数据量级上，计算时间很可能在几分钟之内，而不是几小时或几天。为了获得质量高得多的聚类结果，这点额外的时间开销是非常值得的。</li></ul></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-无监督学习介绍&quot;&gt;&lt;a href=&quot;#1-无监督学习介绍&quot; class=&quot;headerlink&quot; title=&quot;1. 无监督学习介绍&quot;&gt;&lt;/a&gt;1. 无监督学习介绍&lt;/h1&gt;&lt;p&gt;无监督学习（Unsupervised Learning</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>OER台阶图</title>
    <link href="http://example.com/2025/07/18/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/OER%E5%92%8CHER/"/>
    <id>http://example.com/2025/07/18/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/OER%E5%92%8CHER/</id>
    <published>2025-07-18T12:55:56.000Z</published>
    <updated>2025-12-02T12:09:06.682Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-关于催化理论知识"><a href="#1-关于催化理论知识" class="headerlink" title="1. 关于催化理论知识"></a>1. 关于催化理论知识</h1><blockquote><ol><li>Bifunctional CoNx embedded graphene electrocatalysts for OER and ORR: A theoretical evaluation</li><li><a href="https://www.bilibili.com/video/BV1Q7411k7NU/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">VASP电催化，OER&#x2F;ORR计算实例【电催化课程2】_哔哩哔哩_bilibili</a></li><li><a href="https://www.bigbrosci.com/2018/11/06/ex68/">Ex68 频率，零点能，吉布斯自由能的计算 | Learn VASP The Hard Way (bigbrosci.com)</a></li><li><a href="https://blog.shishiruqi.com/2019/04/21/freenergy/">校正分子和吸附分子自由能 | 世事如棋 (shishiruqi.com)</a></li><li><a href="https://www.bilibili.com/video/BV1aH4y1278C/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">催化OER计算：PH值对反应自由能影响 | 【华算科技朱老师讲DFT-VASP】_哔哩哔哩_bilibili</a></li></ol></blockquote><h2 id="1-ORR-and-OER"><a href="#1-ORR-and-OER" class="headerlink" title="1. ORR and OER"></a>1. ORR and OER</h2><ul><li>ORR : oxygen reduction reaction 氧还原反应</li><li>OER : oxygen evolution reaction  氧析出反应</li></ul><p>用于金属-空气电池</p><h3 id="1-OER-：氧气析出过程-氢气脱离过程"><a href="#1-OER-：氧气析出过程-氢气脱离过程" class="headerlink" title="1. OER ：氧气析出过程&#x2F;氢气脱离过程"></a>1. OER ：氧气析出过程&#x2F;氢气脱离过程</h3><blockquote><p>H2O   to OH and H+ and e-</p><p>其中，H+ and e-的能量用1&#x2F;2 H2表示</p><p>H2O是液态水分子的能量，等于饱和蒸汽压下的水蒸汽能量（常温下 ~0.035 bar) 用vaspkit-502-298.15-0.035-1计算</p><p>OH to O</p><p>O and H2O to OOH</p><p>OOH to O2</p><p>氧气自由能不能直接计算，需要用2G(H2O)-2G(H2)-G(O2)&#x3D;-4.92 eV计算，以保证总包的反应能量。同理，CO N2也需要能量反推</p><p>O2 脱吸附</p></blockquote><p>其中，前四步是电子过程，也是通常台阶图包括的</p><hr><h2 id="3-计算过程"><a href="#3-计算过程" class="headerlink" title="3. 计算过程"></a>3. 计算过程</h2><ol><li><p>计算H2O H2 O2 的自由能 (需要修正能量 H2O用饱和蒸汽压的方法;注意算频率的一步只能得到修正的能量，不能用最后的总能当作最后的总能，总能看正常优化、自洽后的总能；氧气自由能不能直接计算(自旋多重度），需要用2G(H2O)-2G(H2)-G(O2)&#x3D;-4.92 eV，注意添加自旋极化 ISYM&#x3D;0)</p><p>氧气有自旋，电子占据问题，高斯软件的高斯基组可以计算能量，但是vasp的平面波无法计算<a href="https://www.bilibili.com/video/BV1pB4y1A7Ae/?spm_id_from=333.788&vd_source=3fde33be3f37f952ed046ee6301aa448">DFT学习11-VASP和高斯计算有何不同_哔哩哔哩_bilibili</a></p><p>自旋极化计算需要把ISYM 设置为0（<a href="https://www.bigbrosci.com/2017/10/30/ex08/">Ex08 氧原子能量的正确计算 | Learn VASP The Hard Way (bigbrosci.com)</a>）</p></li><li><p>优化基底 (固定c轴的优化)</p></li><li><p>优化基团+基底 （ISIF&#x3D;2 即固定晶格，催化是化学吸附，不需要考虑范德瓦尔斯修正(VDW)，但需要考虑零点振动能的能量修正）</p><p><strong>进行频率计算校正能量时，由于只能使用NPAR&#x3D;1,所以用结构优化的k点数量算频率很不划算，很慢。测试发现用Gamma点计算后修正的G值的量变化只有几个meV。可以直接用Gamma点计算。</strong></p></li><li><p>对基团+基底进行自由能校正(需要固定住基底原子，selective dynamics ,只放开吸附的分子做计算)</p></li><li><p>进行分电势的台阶图计算</p></li><li><p>过电势由能量变化最大的一步减去1.23eV&#x2F;e 得到</p></li></ol><h2 id="2-需要注意的计算细节"><a href="#2-需要注意的计算细节" class="headerlink" title="2.需要注意的计算细节"></a>2.需要注意的计算细节</h2><ul><li><p>优化二维材料需要用固定一个晶格的优化，而不是全部打开</p><p>OPTCELL</p></li></ul><pre><code class="shell"># 固定c轴进行优化，不希望改变和c轴的夹角100110000</code></pre><ul><li><p>磁性元素ISPIN &#x3D;2</p></li><li><p>优化吸附物的时候用固定晶格的优化就可以了</p></li><li><p>第一性原理计算不能直接计算双原子分子的能量，有较大误差</p></li></ul><hr><h3 id="1-自由能的计算（计算频率以修正自由能）："><a href="#1-自由能的计算（计算频率以修正自由能）：" class="headerlink" title="1.自由能的计算（计算频率以修正自由能）："></a>1.自由能的计算（计算频率以修正自由能）：</h3><p>通常的计算中只考虑了晶体的势能而没有考虑零点振动能及温度变化导致的电子能对能量的贡献。通过计算气相分子的频率得到振动相关的配分函数，进而可以计算振动对能量的贡献。</p><p>计算溶液分子的自由能可以通过饱和蒸汽压的方法shi’xian</p><ol><li>计算基团的频率振动</li></ol><pre><code class="shell">EDIFF = 1E-7IBRION = 5 #有限差分海森矩阵POTIM =0.015  #步长NFREE =2 #中心差分的算法NSW = 1ISIF =2 ISYM = 0固定基底的原子(by vaspkit) selected dynamics</code></pre><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240913211253982.png" alt="image-20240913211253982"></p><p>计算完成后查看频率</p><p><code> grep cm OUTCAR</code></p><p>校正自由能使用 vaspkit 501</p><hr><h1 id="2-碱性-酸性环境下的催化"><a href="#2-碱性-酸性环境下的催化" class="headerlink" title="2. 碱性&#x2F;酸性环境下的催化"></a>2. 碱性&#x2F;酸性环境下的催化</h1><blockquote><p><a href="https://www.bilibili.com/video/av67721985/?vd_source=3fde33be3f37f952ed046ee6301aa448">超详细电催化台阶图计算步骤，细节拉满【电催化课程1】_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1BK4y1d73H/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">DFT16-碱性条件下OER的计算_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV1aH4y1278C/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">催化OER计算：PH值对反应自由能影响 | 【华算科技朱老师讲DFT-VASP】_哔哩哔哩_bilibili</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240726091418857.png" alt="image-20240726091418857"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240725163001124.png" alt="image-20240725163001124"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240926102947433.png" alt="image-20240926102947433"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240926103042908.png" alt="image-20240926103042908"></p><blockquote><p><strong>Selectively nitrogen-doped carbon materials as  superior metal-free catalysts for oxygen reduction</strong></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240726142836671.png" alt="image-20240726142836671"></p><h1 id="3-HER-析氢反应"><a href="#3-HER-析氢反应" class="headerlink" title="3. HER 析氢反应"></a>3. HER 析氢反应</h1><blockquote><p><a href="https://www.bilibili.com/video/BV1Fe411F7yK/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">析氢反应（HER）计算_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV19CgoeKEz8/?spm_id_from=333.337.search-card.all.click&vd_source=3fde33be3f37f952ed046ee6301aa448">吸附与催化：HER反应计算_哔哩哔哩_bilibili</a></p><p><a href="https://www.bilibili.com/video/BV17K411K7oH/?vd_source=3fde33be3f37f952ed046ee6301aa448">老司机公开课3 | 计算电催化专题 2小时干货拉满_哔哩哔哩_bilibili</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240919202401459.png" alt="image-20240919202401459"> </p><p>酸性条件下的HER，1.需要吸热的部分加个电压就克服了；2.并不是放热越大越好，因为第二步脱氢的时候也会越大，所以绝对值越小越好</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920110028493.png" alt="image-20240920110028493"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920110053716.png" alt="image-20240920110053716"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920110558249.png" alt="image-20240920110558249"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920110632396.png" alt="image-20240920110632396"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920110618106.png" alt="image-20240920110618106"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920112512564.png" alt="image-20240920112512564"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920112427558.png" alt="image-20240920112427558"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920112434745.png" alt="image-20240920112434745"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920112455211.png" alt="image-20240920112455211"></p><blockquote><p><a href="https://www.researchgate.net/publication/352363498_Macroporous_Array_Induced_Multiscale_Modulation_at_the_SurfaceInterface_of_CoOH2NiMo_Self-Supporting_Electrode_for_Effective_Overall_Water_Splitting?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">Macroporous Array Induced Multiscale Modulation at the Surface&#x2F;Interface of Co(OH)2&#x2F;NiMo Self‐Supporting Electrode for Effective Overall Water Splitting</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920141930381.png" alt="image-20240920141930381"></p><blockquote><p><a href="https://www.researchgate.net/publication/342964851_Trends_in_Alkaline_Hydrogen_Evolution_Activity_on_Cobalt_Phosphide_Electrocatalysts_Doped_with_Transition_Metals?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">Trends in Alkaline Hydrogen Evolution Activity on Cobalt Phosphide Electrocatalysts Doped with Transition Metals</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920142618174.png" alt="image-20240920142618174"></p><blockquote><p><a href="https://www.researchgate.net/publication/357180559_Review_of_the_Hydrogen_Evolution_Reaction-A_Basic_Approach?_tp=eyJjb250ZXh0Ijp7ImZpcnN0UGFnZSI6Il9kaXJlY3QiLCJwYWdlIjoiX2RpcmVjdCJ9fQ">Review of the Hydrogen Evolution Reaction—A Basic Approach</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920142752619.png" alt="image-20240920142752619"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920142919588.png" alt="image-20240920142919588"></p><blockquote><p><a href="https://www.sciencedirect.com/science/article/pii/S2211285520312180">https://www.sciencedirect.com/science/article/pii/S2211285520312180</a></p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920143350032.png" alt="image-20240920143350032"></p><p><a href="https://pubs.rsc.org/en/content/articlepdf/2023/nr/d3nr01940c">https://pubs.rsc.org/en/content/articlepdf/2023/nr/d3nr01940c</a></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240920144432901.png" alt="image-20240920144432901"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-关于催化理论知识&quot;&gt;&lt;a href=&quot;#1-关于催化理论知识&quot; class=&quot;headerlink&quot; title=&quot;1. 关于催化理论知识&quot;&gt;&lt;/a&gt;1. 关于催化理论知识&lt;/h1&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Bifu</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>编译ACNN</title>
    <link href="http://example.com/2025/05/27/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BC%96%E8%AF%91ACNN/"/>
    <id>http://example.com/2025/05/27/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BC%96%E8%AF%91ACNN/</id>
    <published>2025-05-27T01:30:30.000Z</published>
    <updated>2025-11-24T06:29:00.843Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-编译可使用ACNN势的lammps"><a href="#1-编译可使用ACNN势的lammps" class="headerlink" title="1. 编译可使用ACNN势的lammps"></a>1. 编译可使用ACNN势的lammps</h1><p>需要注意的点</p><ul><li><p>修改torchdemo-v3 里的prefix.cmake  修改里面openblas 和 torch的位置 </p></li><li><p>需要使用oneapi 来编译，因为需要用到mkl库</p><p>可以检查mpicxx -show  如果输出是icx 说明使用的是oneapi的编译器，如果输出是g++ ，说明使用的是openmpi   （好像必须要有oneapi的库，用g++也行，保险起见用oneapi)</p></li><li><p>在interface 处sh bulid**.sh build</p></li><li><p>编译完成后，需要给出torch 和openblas的库文件位置以及source onempi 才能运行程序</p></li></ul><h1 id="2-关于传入的元素"><a href="#2-关于传入的元素" class="headerlink" title="2. 关于传入的元素"></a>2. 关于传入的元素</h1><p>lammps元素类型输入到ACNN中时会先做原子序数排序，ACNN中会有多个元素的通道（取决于训练用到的元素种类），这些通道应该也是按照元素周期表进行排序的。这样实现match。<br>因此训练时的元素顺序和后续不匹配没有关系，因为训练后的权重会按照元素周期表进行排序</p><p>例1:in.lmp中的元素顺序 Na Y Si  O 和训练时用到的顺序（Na Si O Y）不一致,没有关系，因为都是先排序之后再传入的<br>例2:in.lmp中的元素顺序 Na La Si O 和训练时用到的类型（Na Si O Y）不一致,程序仍然能够进行下去，但是完全是错误的，因为模型里没有La的信息，本质上用到的还是Y的信息<br>因此需要保证：<br>1.结构data.nso中的元素类型和in.lmp中的元素类型相符，和元素的质量的顺序相符，确保没有错误。<br>2.结构中的元素类型在训练model时的元素类型相同，顺序不一定相同，保证不会索引到其他的元素类型上<br>3.元素的质量一定要对，写错了质量也能运行（因为相互作用势没有显性包括质量这一参数）</p><p>可以看源码 interface&#x2F;lammps&#x2F;pair_acnn.cpp 325行部分</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-编译可使用ACNN势的lammps&quot;&gt;&lt;a href=&quot;#1-编译可使用ACNN势的lammps&quot; class=&quot;headerlink&quot; title=&quot;1. 编译可使用ACNN势的lammps&quot;&gt;&lt;/a&gt;1. 编译可使用ACNN势的l</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>机器学习与主动学习</title>
    <link href="http://example.com/2025/05/25/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2025/05/25/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2025-05-25T01:23:40.000Z</published>
    <updated>2025-12-02T12:09:54.282Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="0-deepmd"><a href="#0-deepmd" class="headerlink" title="0. deepmd"></a>0. deepmd</h1><p>使用案例路径：海康&gt;A&gt;A-6工作&gt;deepkit使用案例</p><h1 id="1-机器学习"><a href="#1-机器学习" class="headerlink" title="1.机器学习"></a>1.机器学习</h1><h2 id="1-安装-V3-版本"><a href="#1-安装-V3-版本" class="headerlink" title="1.安装:V3 版本"></a>1.安装:V3 版本</h2><p><strong>1.git clone -b v3 <a href="https://gitlab.com/Herbbb/torchdemo.git">https://gitlab.com/Herbbb/torchdemo.git</a></strong> ****</p><p><strong>2. source</strong> <strong>env_torchdemo</strong> <strong>(optional)</strong></p><p><strong>3.安装各种库,修改cmakelist&#x2F;prifix.~前几行</strong>  </p><p>  **completer:**<strong>编译器</strong> </p><p><strong>4.makedir build</strong> </p><p>  <strong>cd build</strong> </p><p>  <strong>cmake</strong>  **..&#x2F;**<strong>cmakelist</strong></p><p>  <strong>make</strong> </p><p><strong>5.编译和lammps的接口</strong></p><p>把lammps的文件包复制过来，然后sh ~.sh build 18 (18是主节点核数，可以通过lscps 查看cpus的值获得)</p><h2 id="2-训练势基本流程"><a href="#2-训练势基本流程" class="headerlink" title="2.训练势基本流程"></a>2.训练势基本流程</h2><p>lammps元素类型输入到ACNN中时会先做原子序数排序，ACNN中会有多个元素的通道（取决于训练用到的元素种类），这些通道应该也是按照元素周期表进行排序的。这样实现match。<br>因此训练时的元素顺序和后续不匹配没有关系，因为训练后的权重会按照元素周期表进行排序</p><p>例1:in.lmp中的元素顺序 Na Y Si  O 和训练时用到的顺序（Na Si O Y）不一致,没有关系，因为都是先排序之后再传入的<br>例2:in.lmp中的元素顺序 Na La Si O 和训练时用到的类型（Na Si O Y）不一致,程序仍然能够进行下去，但是完全是错误的，因为模型里没有La的信息，本质上用到的还是Y的信息<br>因此需要保证：<br>1.结构data.nso中的元素类型和in.lmp中的元素类型相符，和元素的质量的顺序相符，确保没有错误。<br>2.结构中的元素类型在训练model时的元素类型相同，顺序不一定相同，保证不会索引到其他的元素类型上<br>3.元素的质量一定要对，写错了质量也能运行（因为相互作用势没有显性包括质量这一参数）</p><p><strong>1.从OUTCAR构建训练集，xsf格式，可直接在xcrystal 打开</strong> </p><p>   <strong>sampleing.py</strong></p><ol start="2"><li><p><strong>修改inv2</strong></p><p><strong>traindatapath  &#x3D;  .&#x2F;train-set</strong>*</p><p><strong>batchsize    &#x3D;  1  和原子数相乘超过200</strong></p><p><strong>device     &#x3D;  cpu</strong> </p><p><strong>printstep    &#x3D;  100</strong></p><p><strong>hasattention  &#x3D;  false 加不加注意力</strong></p></li><li><p><strong>训练</strong></p><p><strong>export OMP_NUM_THREADS&#x3D;12 多线程</strong></p><p>   <strong>.&#x2F;<code>main.mp</code> –train inv2</strong>   (是main.mp而不是lam.mpi)</p></li><li><p><strong>测试（修改inv2的测试部分）</strong></p><p><strong>.&#x2F;main.mp –eval inv2   保证能量相差几个meV,力相差几十meV,输出的单位是eV</strong></p><p><strong><code>注意：线程和核，48核是虚拟核，实际只有24物理核，提交一个6线程任务就会占据6个核</code></strong></p><p><strong><code>训练机器学习势需要测试进程数，并不是越多越好，比如4远远好于48</code></strong></p><p><strong>注意二:楼下集群做训练的速度太慢,最好不要用楼下集群训练</strong></p></li><li><p><code>计算 （记得训练用的什么样的inv2计算就要用什么样子的inv2)</code>。否则会报错，无法运行</p></li><li><p>可以在训练过程中增加训练集，只要注意名字别和最开始的训练集重复</p></li></ol><h2 id="3-机器学习取样脚本"><a href="#3-机器学习取样脚本" class="headerlink" title="3.机器学习取样脚本"></a>3.机器学习取样脚本</h2><h3 id="1-按照目录来提取"><a href="#1-按照目录来提取" class="headerlink" title="1. 按照目录来提取"></a>1. 按照目录来提取</h3><pre><code class="python">import osimport sysimport timeimport scipy.stats as stimport numpy as npfrom sklearn.neighbors import KernelDensityfrom matplotlib import pyplotdef timer(func):    def wrapper(*args, **kwargs):        start_time = time.time()        result = func(*args, **kwargs)        end_time = time.time()        print(f&quot;Time elapsed: &#123;end_time - start_time:.4f&#125; seconds&quot;)        return result    return wrapperdef find_files(directory):    &quot;&quot;&quot;    递归查找一个目录下所有叫OUTCAR的文件，并返回这些文件的完整路径    &quot;&quot;&quot;    outcar_files = []  # 保存所有叫OUTCAR的文件的路径    for root, dirs, files in os.walk(directory):        for file in files:            if file == &quot;OUTCAR&quot;:                outcar_files.append(os.path.join(root, file))        for dir in dirs:            dir_outcar_files = find_files(os.path.join(root, dir))            outcar_files.extend(dir_outcar_files)        # 将子目录添加到 dirs 中，以便递归处理        dirs[:] = [d for d in dirs if os.path.isdir(os.path.join(root, d))]    outcar_files = list(set(outcar_files))    return outcar_files@timerdef read_vasp_xdatcar(file_path):    f = open(file=file_path)    f.readline()    f.readline()    lat = []    pos = []    # lattice    for _ in range(3):        lat.append(f.readline())    ele = f.readline()    npele = f.readline()    cnt = 0    tmp = None    while True:        line = f.readline()        if &quot;configuration&quot; in line:            cnt += 1            if tmp is not None:                pos.append(tmp)            tmp = []            continue        if not line:            pos.append(tmp)            break        tmp.append(line)    for conf_i in range(len(pos)):        for atom_i in range(len(pos[conf_i])):            pos[conf_i][atom_i] = pos[conf_i][atom_i].split()        pos[conf_i] = np.array(pos[conf_i]).astype(float)    ele = ele.split()    npele = npele.split()    npele = [int(i) for i in npele]    for i in range(len(lat)):        lat[i] = lat[i].split()    lat = np.array(lat).astype(float)    return cnt, ele, npele, lat, posdef topn(arr, n):    indices = np.argpartition(arr, -n)[-n:]    values = arr[indices]    sort_indices = np.argsort(values)[::-1]    return values[sort_indices], indices[sort_indices]def split_alp_num(string):    for i in range(len(string)):        if string[i].isdigit():            return [string[:i]], int(string[i:])def write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)        for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            file.write(f&#39;&#123;ele_i[j]:2&#125;&#39;)            # coo            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # force            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)    passdef direct2pos(lat, pos, direction=True):    &quot;&quot;&quot;    lat = [[Ax, Ay, Az],           [Bx, By, Bz],           [Cx, Cy, Cz]]    Pos = [n, 3]    :return: direct = [n, 3]    if direction=True    &quot;&quot;&quot;    if direction:        return pos @ lat    else:        return pos @ np.linalg.inv(lat)class MyDistrib(st.rv_continuous):    def __init__(self, prdf):        super().__init__()        self.prdf = prdf    def _pdf(self, x, *args):        return self.prdf.evaluate(x)class RejectSamp:    def __init__(self, ocr_name: str):        self.ocr_name = ocr_name  # &#123;XDATCAR path: OUTCAR path&#125;        self.rng_min = 0        self.rng_max = None        self.rng_grid = 10000        self.rng = None        self.rng_intv = None        self.pdf = None        self.sum_intv = 10        self.envelope = None        self.samples = None        pass    def sampling(self, num):        nframe, natom, ele, ene, lat, pos, foc = self._anls_ocr()        print(nframe)        print(natom)        # print(ele)        print(ene.shape)        print(lat.shape)        print(pos.shape)        print(foc.shape)        self.rng_max = nframe        self.rng = np.linspace(self.rng_min, self.rng_max, self.rng_grid)        self.rng_intv = self.rng[1] - self.rng[0]        sd = np.sum((pos[1:] - pos[:-1]) ** 2, (1, 2))        nintv = sd.size // self.sum_intv        sum_sd = sd[:nintv * self.sum_intv].reshape(nintv, self.sum_intv).sum(1)        sum_sd_body = np.copy(sum_sd)        sum_sd_body[:sum_sd.size // 20] = sum_sd.min()        topn_val, topn_idx = topn(sum_sd_body, 100)        x_train = topn_idx * self.sum_intv        distrib = KernelDensity(kernel=&quot;gaussian&quot;, bandwidth=int(0.003 * nframe)).fit(x_train[..., None])        self.pdf = lambda x: np.exp(distrib.score_samples(np.reshape(x, [-1, 1])))        self.envelope = self.pdf(self.rng).max()        cnt = 0        iters = 0        res = np.array([])        while cnt &lt; num:            u = np.random.uniform(self.rng_min, self.rng_max, num - cnt)            y = np.random.uniform(0, self.envelope, num - cnt)            pd = self.pdf(u)            res = np.append(res, u[pd &gt; y])            res = res.astype(int)            res = np.unique(res)            cnt = len(res)            iters += 1            print(f&quot;&#123;iters:&lt;3&#125;&#123;cnt:6&#125;&quot;)        print(&quot;Total iters: &quot;, iters)        self.samples = res        return self.samples.size, natom, ele, ene[self.samples], \            lat[self.samples], pos[self.samples], foc[self.samples]    def plot(self):        fig, ax = pyplot.subplots(figsize=(10, 6))        ax.hist(self.samples, bins=int(self.samples.size / 8), density=True, alpha=0.5)        ax.plot(self.rng, self.pdf(self.rng), &#39;r-&#39;, lw=1)        ax.plot(self.samples, -0.01 * self.envelope - 0.03 * self.envelope * np.random.random(self.samples.size), &quot;+k&quot;)        ax.set_ylim(-self.envelope * 0.05)        ax.set_title(self.ocr_name + &quot;  PDF&quot;)        return fig    @timer    def _anls_ocr(self):        pos_foc = []        ene = []        lat = []        ions_per = []        types = []        print(self.ocr_name)        with open(self.ocr_name, &#39;r&#39;) as file:            while True:                line = file.readline()                # eof break                if not line:                    break                # ions per type                #if &quot;POSCAR&quot; in line:                #    ions_per.append(line)                #    continue                if &quot;ions per type&quot; in line:                    ions_per.append(line)                if &quot;VRHFIN&quot; in line:                    types.append(line)                # lat                if &quot;BASIS-vectors&quot; in line:                    file.readline()                    file.readline()                    file.readline()                    file.readline()                    for i in range(3):                        line = file.readline()                        lat.append(line)                    continue                # pos &amp; force                if &quot;TOTAL-FORCE (eV/Angst)&quot; in line:                    line = file.readline()                    while True:                        line = file.readline()                        if not line:                            break                        if &quot;------&quot; in line:                            break                        pos_foc.append(line)                    continue                # ene                if &quot;energy  without entropy&quot; in line:                    ene.append(line)        ene = np.array([i.split(&#39;=&#39;)[1].split()[0] for i in ene]).astype(np.float64)        # lat = np.array([i.split() for i in lat]).astype(np.float64)[:, :3].reshape(-1, 3, 3)        lat = np.array([[float(i[4:][:12]), float(i[4:][12:25]), float(i[4:][25:38])] for i in lat]).astype(np.float64)[:, :3].reshape(-1, 3, 3)        # ele pretreatment        types = [i.split(&#39;=&#39;)[1].split(&#39;:&#39;)[0].rstrip() for i in types]        ions_per = ions_per[0].split()[-4:]        ele = sum([[i] * int(j) for i, j in zip(types, ions_per)], [])        pos_foc = np.array([i.split() for i in pos_foc]).astype(np.float64)        nframe = len(ene)        natom = len(ele)        pos = pos_foc[:, :3][:nframe * natom].reshape(nframe, natom, 3)[:nframe]        foc = pos_foc[:, 3:][:nframe * natom].reshape(nframe, natom, 3)[:nframe]        lat = lat[:nframe]        return nframe, natom, ele, ene, lat, pos, focif __name__ == &#39;__main__&#39;:    # ocr_files = find_files(&quot;/data/ljx/third/lizhao/LYSO/LYSO-src/old&quot;)#训练结构路径    ocr_files = [&quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/1fs/2&quot;,                 &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/1fs/4&quot;,                 &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/2fs/2&quot;,                 &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/2fs/4&quot;]    #ocr_files = [&quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/triple/1&quot;,    #             &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/triple/2&quot;] #训练集输出路径      out_path = &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/&quot;    if not os.path.exists(out_path):        os.mkdir(out_path)    out_data_path = os.path.join(out_path, &quot;eval-set&quot;)    if not os.path.exists(out_data_path):        os.mkdir(out_data_path)#每个数据取多少点，大概相乘在10000附近    smp_per_ocr = 1500    # mode = &#39;rand&#39;    mode = &#39;kde&#39;    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #    # Be careful when changing the code below    print(f&quot;total trajectories: &#123;len(ocr_files)&#125;&quot;)    if not os.path.exists(out_data_path):        os.mkdir(out_data_path)    xsf_tot_cnt = 0    if mode == &#39;kde&#39;:        for ocr_i in range(len(ocr_files)):            r = RejectSamp(ocr_files[ocr_i])            n_frame, n_atom, ele, ene, lat, pos, foc = r.sampling(smp_per_ocr)            fig = r.plot()            fig_path = os.path.join(out_path, str(ocr_i))            fig.savefig(fig_path)            for j in range(n_frame):                path_j = os.path.join(out_data_path, f&#39;structure&#123;str(smp_per_ocr * ocr_i + j).zfill(5)&#125;.xsf&#39;)                ene_j = ene[j]                lat_j = lat[j]                ele_j = ele                coo_j = pos[j]                foc_j = foc[j]                write2my(path_j, ene_j, lat_j, ele_j, coo_j, foc_j)                xsf_tot_cnt += 1</code></pre><h3 id="2-从单个目录下递归直接提取全部"><a href="#2-从单个目录下递归直接提取全部" class="headerlink" title="2. 从单个目录下递归直接提取全部"></a>2. 从单个目录下递归直接提取全部</h3><p><code>注意root_dir和write_dir的位置，其中root_dir放到要提取的当前目录最好，并且使用相对路径，这样提取的文件名称比较短</code></p><p><code>而write_dir则需要新建一个目录来存储提取的结构</code></p><pre><code class="python">import osfrom ase.io import readfrom ase import Atomsdef write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i, vir_i=None):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        if vir_i is not None:            file.write(f&quot;VIRIAL\n&quot;)            for i in vir_i:                file.write(f&#39;&#123;i:20.8f&#125;&#39;)        file.write(&quot;\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)        for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            file.write(f&#39;&#123;ele_i[j]:2&#125;&#39;)            # coo            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # force            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)    passdef read_from_ase_atoms(atoms: Atoms):    ene = atoms.get_potential_energy()    lat = atoms.get_cell()    pos = atoms.get_positions()    foc = atoms.get_forces()    try:        sts = atoms.get_stress()        xx, yy, zz, yz, xz, xy = - sts * atoms.get_volume()        vir = np.array(            [[xx, xy, xz],             [xy, yy, yz],             [xz, yz, zz]]).reshape(-1)    except:        vir = None    ele = atoms.get_chemical_symbols()    return ene, lat, pos, foc, vir, eledef find_files(directory, filename):    result = []    for root, dirs, files in os.walk(directory):        for file in files:            if file == filename:                result.append(os.path.join(root, file))    return resultif __name__ == &#39;__main__&#39;:    root_dir = &quot;./&quot;     outcar_dir = find_files(root_dir, &quot;OUTCAR&quot;)    write_dir = &quot;/work/home/xieyu/workplace/liz/2-Li-Y-Si-O-600K/B-升温/data_xsf&quot;    for i in outcar_dir:        xsf_head = &#39;_&#39;.join(i.split(&#39;.&#39;)[-1].split(&#39;/&#39;))        print(xsf_head)        datas = read(i, index=&quot;:&quot;)        for n, i in enumerate(datas):            ene, lat, pos, foc, vir, ele = read_from_ase_atoms(i)            write2my(                os.path.join(write_dir, f&quot;&#123;xsf_head&#125;_struct&#123;str(n).zfill(6)&#125;.xsf&quot;),                ene_i=ene,                lat_i=lat,                ele_i=ele,                coo_i=pos,                foc_i=foc,                vir_i=vir)</code></pre><h2 id="4-遇到的报错、问题"><a href="#4-遇到的报错、问题" class="headerlink" title="4.遇到的报错、问题"></a>4.遇到的报错、问题</h2><ol><li><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230518110402035.png" alt="image-20230518110402035"></p><p><strong>晶胞中某种原子的个数少，导致小的截断半径探索不到足够的数据，需要增加截断半径</strong></p><p><strong>解决方法：增加rcut和acut</strong></p></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230518110457046.png" alt="image-20230518110457046"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230518110502110.png" alt="image-20230518110502110"></p><ol start="2"><li><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230518110515573.png" alt="image-20230518110515573"></p><p><strong>找师弟</strong></p></li><li><p><strong>取样脚本报错，原因是中文名称路径</strong></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230518110605312.png" alt="image-20230518110605312"></p></li></ol><h2 id="5-机器学习训练增加对压力的训练"><a href="#5-机器学习训练增加对压力的训练" class="headerlink" title="5.机器学习训练增加对压力的训练"></a>5.机器学习训练增加对压力的训练</h2><ol><li>在从OUTCAR中提取出的</li><li>在inv2中vpref的值不能为0，可以设置为与epref相等。</li></ol><h2 id="6-inv2中的元素类型的顺序"><a href="#6-inv2中的元素类型的顺序" class="headerlink" title="6. inv2中的元素类型的顺序"></a>6. inv2中的元素类型的顺序</h2><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20241030141805731.png" alt="image-20241030141805731"></p><p><strong>pair_coeff应该和初始的data.nso或者初始的POSCAR的原子顺序相同，而不是按照原子序数的大小变化</strong></p><h1 id="2-主动学习机器学习"><a href="#2-主动学习机器学习" class="headerlink" title="2. 主动学习机器学习"></a>2. 主动学习机器学习</h1><p>如果给主动学习一个很大的初始训练集，会降低主动学习的速度，原因是主动学习需要遍历很多的结构</p><p>初始训练集太少会导致一些较少的原子检测不到，会报错</p><h2 id="1-注意事项"><a href="#1-注意事项" class="headerlink" title="1.注意事项"></a>1.注意事项</h2><h3 id="1-老版本"><a href="#1-老版本" class="headerlink" title="1. 老版本"></a>1. 老版本</h3><h4 id="1-关于显卡"><a href="#1-关于显卡" class="headerlink" title="1 关于显卡"></a>1 关于显卡</h4><p>gtop查看显卡信息，<code>注意别把显存提爆</code>  占用显存大小和胞的大小有关，和数据集数量无关</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231022221759671.png" alt="image-20231022221759671"></p><h4 id="2-关于主动学习"><a href="#2-关于主动学习" class="headerlink" title="2 关于主动学习"></a>2 关于主动学习</h4><p>对每个主动学习任务，需要在集群处建立新的文件夹，来进行自洽计算，避免多个任务进行时发生‘哈希碰撞’</p><p>修改下面文件中的路径</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231023155810672.png" alt="image-20231023155810672"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231023155824015.png" alt="image-20231023155824015"></p><ol start="2"><li><p>cal.slurm里提任务的名称要不同，因为是依据任务名称来回收任务的</p></li><li><p>记得激活python3环境（不然不写新找到的数据集，现在修复了）</p></li><li><p>修改使用哪个卡<img src="/./../../%E5%9B%BE%E7%89%87/image-20231118163008730.png" alt="image-20231118163008730"></p><p>可以把多个任务提交到一块卡上面</p><p>如果是单独只训练需要把restart修改为false</p><p>不能把poscar放到fp里面，否则会反复lable 一个结构</p><p>任务输出的log文件要统一命名   3-md-log（lammps分子动力学输出）</p><p>2-train-log（训练输出）</p><p>4-select-log （选择输出）</p><p>vasp.log vasp输出</p><p>训练的圈数不需要太多 3000-5000</p><p>初始训练集的数目100左右</p><p>acnn的路径（sub-train.sh sub-select.sh)要正确</p><p>inv2 里rcut等</p><p>注意sub-md.sh和sub.slurm里面的任务名称不但不同任务要不同，而且要与原来集群上的任务名称不同</p><p>waiting remote jobs: 1  含义是有一个等待回收的任务</p><p>lr是学习率，学习率的值减小，说明模型的变化量减小</p></li><li><p>inv2中 select_thr 控制挑选结构的下限阈值。目的是挑选出结构差异性大的结构，增大sub-select.log中target的值（一个矩阵的行列式，不同行差异性越大行列式越大）。初期因为跑的少，很多相似结构，所以阈值要高点，不能什么都加进去，后期跑的多了，可以把阈值调低点，增加多样性，</p><p>现在推荐把select_thr设置为1.2</p></li></ol><h4 id="3-需要测试的事项"><a href="#3-需要测试的事项" class="headerlink" title="3.需要测试的事项"></a>3.需要测试的事项</h4><ol><li>初始数据集的大小多少更合适</li><li>训练的圈数需要随着主动学习次数的增加而变化，多少圈比较合适</li><li>小包跑多少算合理了</li><li>主动学习过程中，哪些是占资源多的，哪些是花费时间长的</li><li>nohup .&#x2F;sub-train.sh &gt; train.log 2&gt;&amp;1 &amp;</li><li>inv2 里卡的名称</li><li>in.lmp 里质量、名称</li><li>给一个data.nlso文件</li></ol><h4 id="4-训练方式-增强采样"><a href="#4-训练方式-增强采样" class="headerlink" title="4.训练方式-增强采样"></a>4.训练方式-增强采样</h4><p><code>先用小胞，小圈数  训练到一定程度后（小胞可以跑足够长时间），用大胞、大圈数</code>   目的是增加训练集</p><p><code>可以循环着提供大胞的扩胞方式</code></p><p>inv2 nbatch和savestep要联动修改</p><p>增加胞的大小应该可以减小代数</p><p>top -u lijx</p><p>跑升温过程训练，这样得到的势理论上可以跑不同的温度</p><p>增强采样：大晶胞、训练圈数增加、广的温度采样范围、长的模拟时间</p><h4 id="5-关于续算"><a href="#5-关于续算" class="headerlink" title="5. 关于续算"></a>5. 关于续算</h4><h5 id="1-没有跑到设置条件（没有跑足1ns…"><a href="#1-没有跑到设置条件（没有跑足1ns…" class="headerlink" title="1.没有跑到设置条件（没有跑足1ns…)"></a>1.没有跑到设置条件（没有跑足1ns…)</h5><ol><li><p>删除最后一代</p></li><li><p>在scheduling.sh里修改代数，不从0开始，而是输入刚刚删除的代的代号，这样，会自动从前一代读取最后的模型，进行训练。在提交任务后，新的一代内训练输出2-train.log中会出现提示（需要向下多翻）</p><p>例如，跑完是 28 29 30 ，那就删除30的文件夹，同时把scheduling.sh里改为30</p></li><li><p>提交任务应该注意 &gt;&gt; log 否则续算会把之前的log覆盖掉  nohup .&#x2F;scheduling.sh &gt;&gt; log 2&gt;&amp;1 &amp;</p></li><li><p>可以适当修改分子动力学里的晶胞尺寸、模拟时间、训练圈数和保存的步数、训练用到的卡</p></li></ol><h5 id="2-跑到预定条件，但是希望续算"><a href="#2-跑到预定条件，但是希望续算" class="headerlink" title="2.跑到预定条件，但是希望续算"></a>2.跑到预定条件，但是希望续算</h5><ol><li><p>正常训练结束后log会报没有输出oos的错误，可以通过看最后一代3md的输出log.lammps判断</p></li><li><p>不需要删除最后一代，只需要修改scheduling.sh为下一代开始，例如，出现了 28 29 30 那么scheduling.sh里修改为 31</p></li><li><p>需要把前一代的Ap_inv_*拷贝过来，因为没有select的步骤没有办法产生这些文件，但是分子动力学计算需要这些文件</p><p><code>Ap_inv_*用于主动学习中选择结构，如果用al的模式跑分子动力学，需要用到Ap_inv*来作为选择结构的标准，但是分子动力学过程并没有用到。用不用al跑出来的结果是一样的，差别是要不要获取结构</code></p><p><code>可以不用al，也就不需要Ap_inv</code></p></li><li><p>修改in.lmp里面的模拟晶胞尺寸、模拟时间、以及升温过程的温度（更广一些）、增加训练和保存的圈数、等等方式来增加采样  </p><p>还有修改 restart_lr &#x3D; 1e-4  学习率</p><p>学习率1e-7训练就差别不大了</p><p>inv2中 select_thr  调低 1.2</p></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207153430496.png" alt="image-20231207153430496"></p><h4 id="6-批量化杀掉当前文件夹内后台的任务（不想继续跑的时候）"><a href="#6-批量化杀掉当前文件夹内后台的任务（不想继续跑的时候）" class="headerlink" title="6 批量化杀掉当前文件夹内后台的任务（不想继续跑的时候）"></a>6 批量化杀掉当前文件夹内后台的任务（不想继续跑的时候）</h4><pre><code class="shell"># 获取用户 lijx的所有进程 IDpgrep -u lijx &gt; lijx_processes.txt # 通过进程 ID 获取对应的进程所在目录while read -r pid; do    ls -l /proc/&quot;$pid&quot;/cwd 2&gt;/dev/null | grep -q &quot;/public/lijx/work/Na-X-Si-O/2-onthefly/3-gpu&quot; &amp;&amp; echo &quot;$      pid&quot;done &lt; lijx_processes.txt &gt; lijx_processes_in_test.txt# 通过进程 ID 杀死位于 /home/lijx/test 及其子目录下的所有进程while read -r pid; do    kill -9 &quot;$pid&quot;done &lt; lijx_processes_in_test.txt</code></pre><pre><code class="shell">for i in $(cat lijx_processes_in_test.txt);do llpid $i;done</code></pre><h2 id="2-集成化提任务方式"><a href="#2-集成化提任务方式" class="headerlink" title="2.集成化提任务方式"></a>2.集成化提任务方式</h2><p>四个文件  1-convert_outcar_xsf.py   2-convert_POSCAR_data.py   3-makepot.py    make_initial.sh</p><p>只需要配置make_initial.sh</p><pre><code class="shell">#需要的文件：POSCAR OUTCAR sub.slurm(vasp) sub-md.sh(lammps)#先切换到python3环境 conda activate #set vasp_systermvasp_folder=&#39;&quot;/work/home/liz/workspace/1-system-MD/5-Na-X-Si-O/md/1-Mo&quot;&#39;md_duty_name=&#39;md-Mo&#39;vasp_label_duty_name=&#39;vasp-Mo&#39;#set GPUGPU_number=0#set mdcat &gt; in.lmp &lt;&lt; lzhvariable        x index 1variable        y index 1variable        z index 1variable        ss equal 0.001variable        tdamp equal &quot;v_ss*100&quot;variable        dmsdx  equal   c_1[1]variable        dmsdy  equal   c_1[2]variable        dmsdz  equal   c_1[3]variable        dmsd   equal   c_1[4]variable        istep  equal   stepvariable        msdx   format  dmsdx  %16.8evariable        msdy   format  dmsdy  %16.8evariable        msdz   format  dmsdz  %16.8evariable        msd    format  dmsd   %16.8evariable        sstep  format  istep  %-10gunits           metalatom_style      atomicboundary        p p pread_data       ./data.nsomass            1 22.990    #importantmass            2 95.94mass            3 28.084mass            4 15.999replicate       \$x \$y \$zpair_style      acnn ./model/all alpair_coeff      * * Na   Mo   Si   O   #importantthermo          1thermo_style    custom step time temp press pe lx ly lz kethermo_modify   format 4 %20.15gcompute         1 all msd com yesdump            out  all custom 100 A.lammpstrj id element xu yu zu fx fy fzdump_modify     out  element 1   2   3   4dump_modify     out  format 1 %4ddump_modify     out  format 2 %4sdump_modify     out  format float %16.8fvelocity        all create 800.0 8728timestep        \$&#123;ss&#125;fix             1 all nvt temp 800 800 \$&#123;tdamp&#125;fix             2 all print 1 &quot;\$&#123;sstep&#125; \$&#123;msdx&#125; \$&#123;msdy&#125; \$&#123;msdz&#125; \$&#123;msd&#125;&quot; screen no file msd.datrun             1000000lzh#not always need changesvasp_ip=&#39;&quot;liz@59.72.114.246&quot;&#39;vasp_port=&#39;&quot;2222&quot;&#39;command_line=&#39;&quot;slurm&quot;&#39;# set vasp_inputvasp_pot_PBE_Path=&#39;&quot;/public/lijx/work/Na-X-Si-O/pot-makepot/potpaw_PBE/&quot;&#39;cat &gt; INCAR &lt;&lt; liz SYSTEM = Na-X-Si-OALGO = NormalISYM = 0LREAL =AutoPREC = NormalEDIFF = 1e-5ENCUT = 400NELMIN = 4NELM = 500IBRION = -1ISIF = 2ISMEAR = 0SIGMA = 0.05NPAR = 6LH5 = .TRUE.lizcat &gt; KPOINTS &lt;&lt; lizhK-Spacing Value to Generate K-Mesh: 0.060Gamma 1   1   1 0.0  0.0  0.0lizhsed -i &quot;s|^base_pseudo_path.*|base_pseudo_path = $vasp_pot_PBE_Path|&quot; 3-makepot.pymv ../POSCAR ../resource/fpmv ../sub.slurm ../resource/fpmv ../sub-md.sh ../resource/mdmv ./INCAR ../resource/fpmv ./KPOINTS ../resource/fpmv ./in.lmp ../resource/mdmv ../OUTCAR ../resource/init_dtcp ./1-convert_outcar_xsf.py ../resource/init_dtcp ./2-convert_POSCAR_data.py ../resource/fpcp ./3-makepot.py ../resource/fpcd ../resource/init_dt/python 1-convert_outcar_xsf.py waitrm OUTCARmv eval-set/* ./cd ../fppython 2-convert_POSCAR_data.py POSCAR data.nsowaitmv data.nso ../mdpython 3-makepot.py rm POSCARwaitif [ &quot;$command_line&quot;=&quot;slurm&quot; ]; then    sed -i &quot;s|^#SBATCH  --job-name.*|#SBATCH  --job-name=$vasp_label_duty_name|&quot; sub.slurmelif [ &quot;$command_line&quot; = &quot;PBS&quot; ]; then    sed -i &quot;s|^#PBS -N|#PBS -N $vasp_label_duty_name|&quot; sub.slurmelse    echo &quot;Command line is not &#39;slurm&#39; or &#39;PBS. Run another command.&quot;ficd ../mdif [ &quot;$command_line&quot;=&quot;slurm&quot; ]; then    sed -i &quot;s|^#SBATCH  --job-name.*|#SBATCH  --job-name=$md_duty_name|&quot; sub-md.shelif [ &quot;$command_line&quot; = &quot;PBS&quot; ]; then    sed -i &quot;s|^#PBS -N|#PBS -N $md_duty_name|&quot; sub-md.shelse    echo &quot;Command line is not &#39;slurm&#39; or &#39;PBS. Run another command.&quot;ficd ..sed -i &quot;9s|.*|remote_work_place=$vasp_folder|&quot; iter_scheduling_remote.shsed -i &quot;s|^remote_ip=.*|remote_ip=$vasp_ip|&quot; iter_scheduling_remote.shsed -i &quot;s|^remote_port=.*|remote_port=$vasp_port|&quot; iter_scheduling_remote.shsed -i &quot;s|^task_sys=.*|task_sys=$command_line|&quot; iter_scheduling_remote.shsed -i &quot;s|^device.*|device          =   cuda:$GPU_number|&quot; inv2</code></pre><p>1-</p><pre><code class="python">import osimport sysimport timeimport scipy.stats as stimport numpy as npfrom sklearn.neighbors import KernelDensityfrom matplotlib import pyplotdef timer(func):    def wrapper(*args, **kwargs):        start_time = time.time()        result = func(*args, **kwargs)        end_time = time.time()        print(f&quot;Time elapsed: &#123;end_time - start_time:.4f&#125; seconds&quot;)        return result    return wrapperdef find_files(directory):    &quot;&quot;&quot;    递归查找一个目录下所有叫OUTCAR的文件，并返回这些文件的完整路径    &quot;&quot;&quot;    outcar_files = []  # 保存所有叫OUTCAR的文件的路径    for root, dirs, files in os.walk(directory):        for file in files:            if file == &quot;OUTCAR&quot;:                outcar_files.append(os.path.join(root, file))        for dir in dirs:            dir_outcar_files = find_files(os.path.join(root, dir))            outcar_files.extend(dir_outcar_files)        # 将子目录添加到 dirs 中，以便递归处理        dirs[:] = [d for d in dirs if os.path.isdir(os.path.join(root, d))]    outcar_files = list(set(outcar_files))    return outcar_files@timerdef read_vasp_xdatcar(file_path):    f = open(file=file_path)    f.readline()    f.readline()    lat = []    pos = []    # lattice    for _ in range(3):        lat.append(f.readline())    ele = f.readline()    npele = f.readline()    cnt = 0    tmp = None    while True:        line = f.readline()        if &quot;configuration&quot; in line:            cnt += 1            if tmp is not None:                pos.append(tmp)            tmp = []            continue        if not line:            pos.append(tmp)            break        tmp.append(line)    for conf_i in range(len(pos)):        for atom_i in range(len(pos[conf_i])):            pos[conf_i][atom_i] = pos[conf_i][atom_i].split()        pos[conf_i] = np.array(pos[conf_i]).astype(float)    ele = ele.split()    npele = npele.split()    npele = [int(i) for i in npele]    for i in range(len(lat)):        lat[i] = lat[i].split()    lat = np.array(lat).astype(float)    return cnt, ele, npele, lat, posdef topn(arr, n):    indices = np.argpartition(arr, -n)[-n:]    values = arr[indices]    sort_indices = np.argsort(values)[::-1]    return values[sort_indices], indices[sort_indices]def split_alp_num(string):    for i in range(len(string)):        if string[i].isdigit():            return [string[:i]], int(string[i:])def write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)        for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            file.write(f&#39;&#123;ele_i[j]:2&#125;&#39;)            # coo            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # force            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)    passdef direct2pos(lat, pos, direction=True):    &quot;&quot;&quot;    lat = [[Ax, Ay, Az],           [Bx, By, Bz],           [Cx, Cy, Cz]]    Pos = [n, 3]    :return: direct = [n, 3]    if direction=True    &quot;&quot;&quot;    if direction:        return pos @ lat    else:        return pos @ np.linalg.inv(lat)class MyDistrib(st.rv_continuous):    def __init__(self, prdf):        super().__init__()        self.prdf = prdf    def _pdf(self, x, *args):        return self.prdf.evaluate(x)class RejectSamp:    def __init__(self, ocr_name: str):        self.ocr_name = ocr_name  # &#123;XDATCAR path: OUTCAR path&#125;        self.rng_min = 0        self.rng_max = None        self.rng_grid = 10000        self.rng = None        self.rng_intv = None        self.pdf = None        self.sum_intv = 10        self.envelope = None        self.samples = None        pass    def sampling(self, num):        nframe, natom, ele, ene, lat, pos, foc = self._anls_ocr()        print(nframe)        print(natom)        # print(ele)        print(ene.shape)        print(lat.shape)        print(pos.shape)        print(foc.shape)        self.rng_max = nframe        self.rng = np.linspace(self.rng_min, self.rng_max, self.rng_grid)        self.rng_intv = self.rng[1] - self.rng[0]        sd = np.sum((pos[1:] - pos[:-1]) ** 2, (1, 2))        nintv = sd.size // self.sum_intv        sum_sd = sd[:nintv * self.sum_intv].reshape(nintv, self.sum_intv).sum(1)        sum_sd_body = np.copy(sum_sd)        sum_sd_body[:sum_sd.size // 20] = sum_sd.min()        topn_val, topn_idx = topn(sum_sd_body, 100)        x_train = topn_idx * self.sum_intv        distrib = KernelDensity(kernel=&quot;gaussian&quot;, bandwidth=int(0.003 * nframe)).fit(x_train[..., None])        self.pdf = lambda x: np.exp(distrib.score_samples(np.reshape(x, [-1, 1])))        self.envelope = self.pdf(self.rng).max()        cnt = 0        iters = 0        res = np.array([])        while cnt &lt; num:            u = np.random.uniform(self.rng_min, self.rng_max, num - cnt)            y = np.random.uniform(0, self.envelope, num - cnt)            pd = self.pdf(u)            res = np.append(res, u[pd &gt; y])            res = res.astype(int)            res = np.unique(res)            cnt = len(res)            iters += 1            print(f&quot;&#123;iters:&lt;3&#125;&#123;cnt:6&#125;&quot;)        print(&quot;Total iters: &quot;, iters)        self.samples = res        return self.samples.size, natom, ele, ene[self.samples], \            lat[self.samples], pos[self.samples], foc[self.samples]    def plot(self):        fig, ax = pyplot.subplots(figsize=(10, 6))        ax.hist(self.samples, bins=int(self.samples.size / 8), density=True, alpha=0.5)        ax.plot(self.rng, self.pdf(self.rng), &#39;r-&#39;, lw=1)        ax.plot(self.samples, -0.01 * self.envelope - 0.03 * self.envelope * np.random.random(self.samples.size), &quot;+k&quot;)        ax.set_ylim(-self.envelope * 0.05)        ax.set_title(self.ocr_name + &quot;  PDF&quot;)        return fig    @timer    def _anls_ocr(self):        pos_foc = []        ene = []        lat = []        ions_per = []        types = []        print(self.ocr_name)        with open(self.ocr_name, &#39;r&#39;) as file:            while True:                line = file.readline()                # eof break                if not line:                    break                # ions per type                #if &quot;POSCAR&quot; in line:                #    ions_per.append(line)                #    continue                if &quot;ions per type&quot; in line:                    ions_per.append(line)                if &quot;VRHFIN&quot; in line:                    types.append(line)                # lat                if &quot;BASIS-vectors&quot; in line:                    file.readline()                    file.readline()                    file.readline()                    file.readline()                    for i in range(3):                        line = file.readline()                        lat.append(line)                    continue                # pos &amp; force                if &quot;TOTAL-FORCE (eV/Angst)&quot; in line:                    line = file.readline()                    while True:                        line = file.readline()                        if not line:                            break                        if &quot;------&quot; in line:                            break                        pos_foc.append(line)                    continue                # ene                if &quot;energy  without entropy&quot; in line:                    ene.append(line)        ene = np.array([i.split(&#39;=&#39;)[1].split()[0] for i in ene]).astype(np.float64)        # lat = np.array([i.split() for i in lat]).astype(np.float64)[:, :3].reshape(-1, 3, 3)        lat = np.array([[float(i[4:][:12]), float(i[4:][12:25]), float(i[4:][25:38])] for i in lat]).astype(np.float64)[:, :3].reshape(-1, 3, 3)        # ele pretreatment        types = [i.split(&#39;=&#39;)[1].split(&#39;:&#39;)[0].rstrip() for i in types]        ions_per = ions_per[0].split()[-4:]        ele = sum([[i] * int(j) for i, j in zip(types, ions_per)], [])        pos_foc = np.array([i.split() for i in pos_foc]).astype(np.float64)        nframe = len(ene)        natom = len(ele)        pos = pos_foc[:, :3][:nframe * natom].reshape(nframe, natom, 3)[:nframe]        foc = pos_foc[:, 3:][:nframe * natom].reshape(nframe, natom, 3)[:nframe]        lat = lat[:nframe]        return nframe, natom, ele, ene, lat, pos, focif __name__ == &#39;__main__&#39;:    # ocr_files = find_files(&quot;/data/ljx/third/lizhao/LYSO/LYSO-src/old&quot;)#训练结构路径    ocr_files = [&quot;./OUTCAR&quot;]    #ocr_files = [&quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/triple/1&quot;,    #             &quot;/work/home/liz/workspace/1-system-MD/3-K5YSi4O12/9-1-lammps/6-train-NNP/outcar/triple/2&quot;] #训练集输出路径      out_path = &quot;./&quot;    if not os.path.exists(out_path):        os.mkdir(out_path)    out_data_path = os.path.join(out_path, &quot;eval-set&quot;)    if not os.path.exists(out_data_path):        os.mkdir(out_data_path)#每个数据取多少点，大概相乘在10000附近    smp_per_ocr = 50    # mode = &#39;rand&#39;    mode = &#39;kde&#39;    # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #    # Be careful when changing the code below    print(f&quot;total trajectories: &#123;len(ocr_files)&#125;&quot;)    if not os.path.exists(out_data_path):        os.mkdir(out_data_path)    xsf_tot_cnt = 0    if mode == &#39;kde&#39;:        for ocr_i in range(len(ocr_files)):            r = RejectSamp(ocr_files[ocr_i])            n_frame, n_atom, ele, ene, lat, pos, foc = r.sampling(smp_per_ocr)            fig = r.plot()            fig_path = os.path.join(out_path, str(ocr_i))            fig.savefig(fig_path)            for j in range(n_frame):                path_j = os.path.join(out_data_path, f&#39;structure&#123;str(smp_per_ocr * ocr_i + j).zfill(5)&#125;.xsf&#39;)                ene_j = ene[j]                lat_j = lat[j]                ele_j = ele                coo_j = pos[j]                foc_j = foc[j]                write2my(path_j, ene_j, lat_j, ele_j, coo_j, foc_j)                xsf_tot_cnt += 1</code></pre><p>2-</p><pre><code class="python"># convert VASP OUTCAT to lammps .data (atomic)## usage:# python &quot;this file&quot; OUTCAR.input data.output## # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ###import numpy as npfrom sys import argvdef anls_poscar(file_name: str):    with open(file_name, &#39;r&#39;) as f:        lines = f.readlines()    bloom_factor = float(lines[1])    lat = np.array([i.split() for i in lines[2: 5]]).astype(float)    lat = bloom_factor * lat    ele_and_num = lines[5][:-1] + &quot;:&quot; + lines[6][:-1]    pos_d = None    if (lines[7][0] == &#39;d&#39;) or (lines[7][0] == &#39;D&#39;):        # direct coord.        pos_d = np.array([i.split() for i in lines[8:]]).astype(float)    elif (lines[7][0] == &#39;c&#39;) or (lines[7][0] == &#39;C&#39;):        # cartesian        pos_c = np.array([i.split() for i in lines[8:]]).astype(float)        pos_d = direct2pos(lat, pos_c, False)    return lat, pos_d, ele_and_numdef direct2pos(lat, pos, direction=True):    &quot;&quot;&quot;    lat = [[Ax, Ay, Az],           [Bx, By, Bz],           [Cx, Cy, Cz]]    Pos = [n, 3]    :return: if True:  return cart   = [n, 3]             if False: return direct = [n, 3]    &quot;&quot;&quot;    if direction:        return pos @ lat    else:        return pos @ np.linalg.inv(lat)def clip_ny_norm(x):    return x / np.linalg.norm(x)def cart2lams(lat):    &quot;&quot;&quot;    vasp (POSCAR) Coordinate System:    O = [[ax, ay, az],         [bx, by, bz],         [cx, cy, cz]]    :return:    lammps Coordinate System:    O&#39; = [[xhi - xlo,  0,  0],          [xy, yhi - ylo,  0],          [xz, yz, zhi - zlo]]    &quot;&quot;&quot;    ax = (lat[0] ** 2).sum() ** 0.5    a_hat = clip_ny_norm(lat[0])    bx = (lat[1] * a_hat).sum()    by = np.linalg.norm(np.cross(a_hat, lat[1]))    cx = (lat[2] * a_hat).sum()    cy = (lat[2] * np.cross(clip_ny_norm(np.cross(lat[0], lat[1])), a_hat)).sum()    cz = np.abs((lat[2] * clip_ny_norm(np.cross(lat[0], lat[1]))).sum())    new = np.array([        [ax, 0, 0],        [bx, by, 0],        [cx, cy, cz]    ])    return newdef write_lmp_atomic_data(filename, lmp_lat, pos_c, strr):    new_coo = pos_c    Z_of_type = strr.split(&#39;:&#39;)[0].split()    n_type = strr.split(&#39;:&#39;)[1].split()    new_ele = [[i] * int(n_type[i - 1]) for i in range(1, len(n_type) + 1)]    new_ele = sum(new_ele, [])    # write    with open(filename, &quot;w&quot;) as f:        f.writelines(&quot;%s (written by ARES-NNP)\n\n&quot; % str(filename + strr))        f.writelines(&quot;%i    atoms\n&quot; % len(new_coo))        f.writelines(&quot;%i    atom types\n&quot; % len(Z_of_type))        f.writelines(&quot;0.0    %.17f  xlo xhi\n&quot; % lmp_lat[0][0])        f.writelines(&quot;0.0    %.17f  ylo yhi\n&quot; % lmp_lat[1][1])        f.writelines(&quot;0.0    %.17f  zlo zhi\n&quot; % lmp_lat[2][2])        f.writelines(            &quot;    %18.12f    %18.12f    %18.12f  xy xz yz\n\n\n&quot; % (lmp_lat[1][0], lmp_lat[2][0], lmp_lat[2][1]))        f.writelines(&quot;Atoms\n\n&quot;)        index_list = np.arange(1, len(new_coo) + 1)        for i, ele, coo in zip(index_list, new_ele, new_coo):            f.writelines(&quot;&#123;0:&gt;6&#125; &#123;1:&gt;3&#125; &#123;2:20.12f&#125; &#123;3:20.12f&#125; &#123;4:20.12f&#125;\n&quot;.format(i, ele, coo[0], coo[1], coo[2]))if __name__ == &#39;__main__&#39;:    filename = argv[2]    lat, pos, strr = anls_poscar(argv[1])    crs_para = np.linalg.norm(lat, axis=1)    exg = np.argsort(crs_para)[::-1]    lat = lat[exg, :]    pos = pos[:, exg]    lmp_lat = cart2lams(lat)    inv = np.linalg.inv(lat) @ lmp_lat    pos_c = direct2pos(lat, pos, True) @ inv    write_lmp_atomic_data(filename, lmp_lat, pos_c, strr)</code></pre><p>3-</p><pre><code class="python"># 字典：元素对应的赝势文件路径base_pseudo_path = &quot;/public/lijx/work/Na-X-Si-O/pot-makepot/potpaw_PBE/&quot;pseudo_paths = &#123;    &#39;H&#39;: &#39;H/POTCAR&#39;,    &#39;Li&#39;: &#39;Li_sv/POTCAR&#39;,    &#39;Be&#39;: &#39;Be_sv/POTCAR&#39;,    &#39;B&#39;: &#39;B/POTCAR&#39;,    &#39;C&#39;: &#39;C/POTCAR&#39;,    &#39;N&#39;: &#39;N/POTCAR&#39;,    &#39;O&#39;: &#39;O/POTCAR&#39;,    &#39;F&#39;: &#39;F/POTCAR&#39;,    &#39;Ne&#39;: &#39;Ne/POTCAR&#39;,    &#39;Na&#39;: &#39;Na_pv/POTCAR&#39;,    &#39;Mg&#39;: &#39;Mg_pv/POTCAR&#39;,    &#39;Al&#39;: &#39;Al/POTCAR&#39;,    &#39;Si&#39;: &#39;Si/POTCAR&#39;,    &#39;P&#39;: &#39;P/POTCAR&#39;,    &#39;S&#39;: &#39;S/POTCAR&#39;,    &#39;Cl&#39;: &#39;Cl/POTCAR&#39;,    &#39;K&#39;: &#39;K_sv/POTCAR&#39;,    &#39;Ca&#39;: &#39;Ca_sv/POTCAR&#39;,    &#39;Sc&#39;: &#39;Sc_sv/POTCAR&#39;,    &#39;Ti&#39;: &#39;Ti_pv/POTCAR&#39;,    &#39;V&#39;: &#39;V_pv/POTCAR&#39;,    &#39;Cr&#39;: &#39;Cr_pv/POTCAR&#39;,    &#39;Mn&#39;: &#39;Mn_pv/POTCAR&#39;,    &#39;Fe&#39;: &#39;Fe_pv/POTCAR&#39;,    &#39;Co&#39;: &#39;Co/POTCAR&#39;,    &#39;Ni&#39;: &#39;Ni_pv/POTCAR&#39;,    &#39;Cu&#39;: &#39;Cu_pv/POTCAR&#39;,    &#39;Zn&#39;: &#39;Zn/POTCAR&#39;,    &#39;Ga&#39;: &#39;Ga_d/POTCAR&#39;,    &#39;Ge&#39;: &#39;Ge_d/POTCAR&#39;,    &#39;As&#39;: &#39;As/POTCAR&#39;,    &#39;Se&#39;: &#39;Se/POTCAR&#39;,    &#39;Br&#39;: &#39;Br/POTCAR&#39;,    &#39;Kr&#39;: &#39;Kr/POTCAR&#39;,    &#39;Rb&#39;: &#39;Rb_sv/POTCAR&#39;,    &#39;Sr&#39;: &#39;Sr_sv/POTCAR&#39;,    &#39;Y&#39;: &#39;Y_sv/POTCAR&#39;,    &#39;Zr&#39;: &#39;Zr_sv/POTCAR&#39;,    &#39;Nb&#39;: &#39;Nb_pv/POTCAR&#39;,    &#39;Mo&#39;: &#39;Mo_pv/POTCAR&#39;,    &#39;Tc&#39;: &#39;Tc_pv/POTCAR&#39;,    &#39;Ru&#39;: &#39;Ru_pv/POTCAR&#39;,    &#39;Rh&#39;: &#39;Rh_pv/POTCAR&#39;,    &#39;Pd&#39;: &#39;Pd/POTCAR&#39;,    &#39;Ag&#39;: &#39;Ag/POTCAR&#39;,    &#39;Cd&#39;: &#39;Cd/POTCAR&#39;,    &#39;In&#39;: &#39;In_d/POTCAR&#39;,    &#39;Sn&#39;: &#39;Sn_d/POTCAR&#39;,    &#39;Sb&#39;: &#39;Sb/POTCAR&#39;,    &#39;Te&#39;: &#39;Te/POTCAR&#39;,    &#39;I&#39;: &#39;I/POTCAR&#39;,    &#39;Xe&#39;: &#39;Xe/POTCAR&#39;,    &#39;Cs&#39;: &#39;Cs_sv/POTCAR&#39;,    &#39;Ba&#39;: &#39;Ba_sv/POTCAR&#39;,    &#39;La&#39;: &#39;La/POTCAR&#39;,    &#39;Ce&#39;: &#39;Ce_3/POTCAR&#39;,    &#39;Pr&#39;: &#39;Pr_3/POTCAR&#39;,    &#39;Nd&#39;: &#39;Nd_3/POTCAR&#39;,    &#39;Pm&#39;: &#39;Pm_3/POTCAR&#39;,    &#39;Sm&#39;: &#39;Sm_3/POTCAR&#39;,    &#39;Eu&#39;: &#39;Eu_3/POTCAR&#39;,    &#39;Gd&#39;: &#39;Gd_3/POTCAR&#39;,    &#39;Tb&#39;: &#39;Tb_3/POTCAR&#39;,    &#39;Dy&#39;: &#39;Dy_3/POTCAR&#39;,    &#39;Ho&#39;: &#39;Ho_3/POTCAR&#39;,    &#39;Er&#39;: &#39;Er_3/POTCAR&#39;,    &#39;Tm&#39;: &#39;Tm_3/POTCAR&#39;,    &#39;Yb&#39;: &#39;Yb_2/POTCAR&#39;,    &#39;Lu&#39;: &#39;Lu_3/POTCAR&#39;,    &#39;Hf&#39;: &#39;Hf_pv/POTCAR&#39;,    &#39;Ta&#39;: &#39;Ta_pv/POTCAR&#39;,    &#39;W&#39;: &#39;W_pv/POTCAR&#39;,    &#39;Re&#39;: &#39;Re_pv/POTCAR&#39;,    &#39;Os&#39;: &#39;Os_pv/POTCAR&#39;,    &#39;Ir&#39;: &#39;Ir/POTCAR&#39;,    &#39;Pt&#39;: &#39;Pt/POTCAR&#39;,    &#39;Au&#39;: &#39;Au/POTCAR&#39;,    &#39;Hg&#39;: &#39;Hg/POTCAR&#39;,    &#39;Tl&#39;: &#39;Tl_d/POTCAR&#39;,    &#39;Pb&#39;: &#39;Pb_d/POTCAR&#39;,    &#39;Bi&#39;: &#39;Bi/POTCAR&#39;,    &#39;Th&#39;: &#39;Th/POTCAR&#39;,    &#39;Pa&#39;: &#39;Pa/POTCAR&#39;,    &#39;U&#39;: &#39;U/POTCAR&#39;,    &#39;Np&#39;: &#39;Np/POTCAR&#39;,    &#39;Pu&#39;: &#39;Pu/POTCAR&#39;,# 添加其他元素和对应的赝势路径&#125;for element, relative_path in pseudo_paths.items():    full_path = f&quot;&#123;base_pseudo_path&#125;&#123;relative_path&#125;&quot;    pseudo_paths[element] = full_path# 读取 POSCAR 文件def read_poscar(poscar_path):      with open(poscar_path, &#39;r&#39;) as f:          lines = f.readlines()          elements = lines[5].split()        periodic_table_order = [&#39;H&#39;, &#39;He&#39;, &#39;Li&#39;, &#39;Be&#39;, &#39;B&#39;, &#39;C&#39;, &#39;N&#39;, &#39;O&#39;, &#39;F&#39;, &#39;Ne&#39;,&#39;Na&#39;, &#39;Mg&#39;, &#39;Al&#39;, &#39;Si&#39;, &#39;P&#39;, &#39;S&#39;, &#39;Cl&#39;, &#39;Ar&#39;,&#39;K&#39;,&#39;Ca&#39;, &#39;Sc&#39;, &#39;Ti&#39;, &#39;V&#39;, &#39;Cr&#39;, &#39;Mn&#39;, &#39;Fe&#39;, &#39;Co&#39;,&#39;Ni&#39;, &#39;Cu&#39;, &#39;Zn&#39;,&#39;Ga&#39;, &#39;Ge&#39;, &#39;As&#39;, &#39;Se&#39;, &#39;Br&#39;, &#39;Kr&#39;, &#39;Rb&#39;, &#39;Sr&#39;, &#39;Y&#39;, &#39;Zr&#39;, &#39;Nb&#39;,&#39;Mo&#39;, &#39;Tc&#39;, &#39;Ru&#39;, &#39;Rh&#39;, &#39;Pd&#39;, &#39;Ag&#39;, &#39;Cd&#39;, &#39;In&#39;, &#39;Sn&#39;, &#39;Sb&#39;, &#39;Te&#39;,&#39;I&#39;, &#39;Xe&#39;, &#39;Cs&#39;, &#39;Ba&#39;, &#39;La&#39;, &#39;Ce&#39;, &#39;Pr&#39;, &#39;Nd&#39;, &#39;Pm&#39;, &#39;Sm&#39;, &#39;Eu&#39;,&#39;Gd&#39;, &#39;Tb&#39;, &#39;Dy&#39;, &#39;Ho&#39;, &#39;Er&#39;, &#39;Tm&#39;, &#39;Yb&#39;, &#39;Lu&#39;, &#39;Hf&#39;, &#39;Ta&#39;, &#39;W&#39;,&#39;Re&#39;, &#39;Os&#39;, &#39;Ir&#39;, &#39;Pt&#39;, &#39;Au&#39;, &#39;Hg&#39;, &#39;Tl&#39;, &#39;Pb&#39;, &#39;Bi&#39;, &#39;Po&#39;,&#39;At&#39;,&#39;Rn&#39;,&#39;Fr&#39;,&#39;Ra&#39;,&#39;Ac&#39;,&#39;Th&#39;, &#39;Pa&#39;,&#39;U&#39;, &#39;Np&#39;, &#39;Pu&#39;, &#39;Am&#39;, &#39;Cm&#39;, &#39;Bk&#39;, &#39;Cf&#39;, &#39;Es&#39;, &#39;Fm&#39;, &#39;Md&#39;, &#39;No&#39;, &#39;Lr&#39;]        sorted_elements = sorted(elements, key=lambda x: periodic_table_order.index(x))        return sorted_elements# 创建大的 POTCAR 文件def create_big_potcar(elements, pseudo_paths, output_path):    with open(output_path, &#39;w&#39;) as f_out:        for element in elements:            if element in pseudo_paths:                pseudo_path = pseudo_paths[element]                with open(pseudo_path, &#39;r&#39;) as f_pseudo:                    f_out.write(f_pseudo.read())            else:                print(f&quot;赝势文件不存在或未定义：&#123;element&#125;&quot;)def copypot(elments,pseudo_paths):    import os     for element in elements:        output_path2 = &quot;POT-&quot;+element        if element in pseudo_paths:            pseudo_path = pseudo_paths[element]            with open(output_path2, &#39;w&#39;) as f_out:                with open(pseudo_path, &#39;r&#39;) as f_pseudo:                    f_out.write(f_pseudo.read())        else:            print(f&quot;赝势文件不存在或未定义：&#123;element&#125;&quot;)            if __name__ == &quot;__main__&quot;:    poscar_path = &quot;POSCAR&quot;  # 输入文件名    output_potcar_path = &quot;POTCAR&quot;  # 输出的大 POTCAR 文件名    elements = read_poscar(poscar_path)    create_big_potcar(elements, pseudo_paths, output_potcar_path)    copypot(elements,pseudo_paths)    print(&quot;大的 POTCAR 文件已创建&quot;)</code></pre><p>nohup .&#x2F;scheduling.sh &gt; log 2&gt;&amp;1 &amp;</p><h2 id="3-华为新版本"><a href="#3-华为新版本" class="headerlink" title="3. 华为新版本"></a>3. 华为新版本</h2><h4 id="1-变化"><a href="#1-变化" class="headerlink" title="1.变化"></a>1.变化</h4><p>增加了server.sh </p><p>改变了iter_scheduling_remote.sh 现在，这两个都不需要做改变可以直接复制</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231227085340259.png" alt="image-20231227085340259"></p><h4 id="2-技巧"><a href="#2-技巧" class="headerlink" title="2. 技巧"></a>2. 技巧</h4><h5 id="1-各个文件夹含义"><a href="#1-各个文件夹含义" class="headerlink" title="1.各个文件夹含义"></a>1.各个文件夹含义</h5><p>pre-select  :主动学习最后一步训练inv2里用的是train_dt，之前步用的看上去是pre-select，但实际上运行时用的也是train_dt</p><p>train_dt</p><p>非主动学习训练：只需要把主动学习inv2的restart改为false，不能删掉，记得更改inv2中cuda的编号，只申请了一张卡</p><p>如果一直训练出的只能跑几步，可以尝试修改select_thr      &#x3D;   2.0 为1.2</p><h2 id="4-主动学习能量、力误差"><a href="#4-主动学习能量、力误差" class="headerlink" title="4. 主动学习能量、力误差"></a>4. 主动学习能量、力误差</h2><blockquote><p>主动学习训练势过程中要求能量和力的误差不能太大，因此需要注意监控能量、力的误差</p></blockquote><p>存在的一种可能是，主动学习的能量和力误差太大，但是可以正常跑完主动学习的过程（如111胞 1ns)。这时候，由于主动学习的误差很大，因此不能认为跑出的1ns结果是合理的。</p><p>目前的版本中还不能设置在能量和力达到一个标准时自动停止，因此需要多注意能量和力</p><blockquote><p>提升训练精度的方法</p></blockquote><ul><li>学习率和训练圈数是重要的可设置的参数，通过增加训练圈数，可以有效地提高训练的精度（降低能量和力的误差）</li><li>清洗数据，可能存在某个误差值很大的结构，导致整体训练效果变差，把这个结构删除掉，能够有效降低误差</li></ul><p><img src="/./../../%E5%9B%BE%E7%89%87/19340802d41eb56259054c6d35d962b.png" alt="19340802d41eb56259054c6d35d962b"></p><h2 id="5-主动学习-minima-hopping"><a href="#5-主动学习-minima-hopping" class="headerlink" title="5. 主动学习+ minima_hopping"></a>5. 主动学习+ minima_hopping</h2><h3 id="1-minima-hopping使用方法"><a href="#1-minima-hopping使用方法" class="headerlink" title="1. minima_hopping使用方法"></a>1. minima_hopping使用方法</h3><p>minimaohopping是在ase中使用的一种采样算法，相比普通的md，采样更有效，这在华为机器<code>liz/2-Na-X-Si-O/2_NaXSiO_big_model/1_one_element/05_Sb_mini</code>  中得到了充分的验证。</p><p>使用minimahopping需改动的地方：</p><ul><li><code>resource/md</code> 文件夹中的data.nso 改为data.vasp，data.vasp的文件内容格式为POSCAR</li><li><code>resource/md</code> 文件夹中增加minima_hopping.py，用于进行minima_hopping模拟，可直接copy，需要修改脚本内的元素类型</li><li><code>resource/md</code>  文件夹中的sub-md.sh需要改变，从提交md变为提交mini</li><li><code>resource</code> 文件夹中的iter_scheduling_remote.sh 有一点改变，可直接copy</li></ul><p><code>sub-md.sh,iter_scheduling_remote.sh, mimima_hopping.py </code>可直接从华为机器<code>/liz/2-Na-X-Si-O/2_NaXSiO_big_model/1_one_element/05_Sb_mini/resource</code> 路径下复制到对应位置，minima_hopping.py需要修改元素类型</p><h1 id="6-经验总结"><a href="#6-经验总结" class="headerlink" title="6. 经验总结"></a>6. 经验总结</h1><h2 id="关于训练集"><a href="#关于训练集" class="headerlink" title="关于训练集"></a>关于训练集</h2><blockquote><ul><li>需要用高精度的第一性原理数据来进行势的训练</li><li>结构优化过程中的结构不能用来进行势的训练，因为计算的能量不准确</li><li>不同ENCUT和k点密度的结构不能用来训练同一个势</li></ul></blockquote><ol><li>升温过程 </li><li>增加压力点</li><li>NPT系综</li><li>meta-dynamics</li><li>minim</li><li>AIMD加大步长</li><li>添加拉伸剪切数据集</li></ol><h2 id="关于训练一个效果好的势"><a href="#关于训练一个效果好的势" class="headerlink" title="关于训练一个效果好的势"></a>关于训练一个效果好的势</h2><ol><li><strong>机器学习计算高温比低温更难，因此高温的数据集很重要</strong></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152709890.png" alt="image-20240814152709890"></p><ol start="2"><li><strong>训练圈数多的要好于少，尽管大小圈数的精度可能相同</strong></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152845687.png" alt="image-20240814152845687"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152857344.png" alt="image-20240814152857344"></p><ol start="3"><li><strong>高精度数据集比低精度好，高精度的数据集训练的势更好</strong></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152919374.png" alt="image-20240814152919374"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152928535.png" alt="image-20240814152928535"></p><ol start="4"><li><strong>主动学习的势要比普通训练更优</strong></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152528733.png" alt="image-20240814152528733"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814152459575.png" alt="image-20240814152459575"></p><ol start="5"><li><strong>如果不算压强，带不带力的计算结果变化并不大</strong></li></ol><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814153029267.png" alt="image-20240814153029267"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240814153038885.png" alt="image-20240814153038885"></p><h2 id="关于训练势的误差"><a href="#关于训练势的误差" class="headerlink" title="关于训练势的误差"></a>关于训练势的误差</h2><p>1. </p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240819104207502.png" alt="image-20240819104207502"></p><p>能量的误差不在线上是没问题的，因为机器学习框架中会加一个常数项，现在学习率还是太高，导致这个常数项比较高。通过<strong>降低学习率</strong>可以解决这个问题（手动降低或者训练足够长的时间自动降低）</p><ol start="2"><li>一个结构对应着1个能量，但是对应着3*原子数个力，所以一个结构的能量偏差大，对应的是许多个力的偏差大</li></ol><h1 id="7-代码理解"><a href="#7-代码理解" class="headerlink" title="7. 代码理解"></a>7. 代码理解</h1><p>Ap_*由sub-select根据现有的结构产生，和训练势过程和分子动力学过程无关。</p><p>主动学习和普通机器学习的区别</p><ul><li>训练势时是从上一个势开始训练，增加新的数据集，重置学习率等</li><li>跑分子动力学时in.lmp添加al，并且使用由select.sh生成的Ap_*,用来监控并终止分子动力学，产生oos</li></ul><p>sub-select.sh需要在GPU上跑，不然会报错</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20240810162052521.png" alt="image-20240810162052521"></p><p>acnn -grade 用Ap_*产生某一个结构的grade，如果grade值高于某一个数，就可以加到oos中</p><p>主动学习停掉的情况</p><ol><li>grade值大于3且小于八累计了50个结构</li><li>grade值大于8</li></ol><h2 id="主动学习的两次select"><a href="#主动学习的两次select" class="headerlink" title="主动学习的两次select"></a>主动学习的两次select</h2><p>第一次是md过程中select不能判断的结构，第二次select是产生oos后对相似oos结构的一个去重，然后选择grade值相对大一些的结构（张开一个maxvol空间，选取边缘）</p><p>第一次是用现有的Ap_inv ，第二次是新产生一批Ap_inv   </p><h1 id="训练标准"><a href="#训练标准" class="headerlink" title="训练标准"></a>训练标准</h1><p>达到能量低于1meV&#x2F;atom</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;0-deepmd&quot;&gt;&lt;a href=&quot;#0-deepmd&quot; class=&quot;headerlink&quot; title=&quot;0. deepmd&quot;&gt;&lt;/a&gt;0. deepmd&lt;/h1&gt;&lt;p&gt;使用案例路径：海康&amp;gt;A&amp;gt;A-6工作&amp;gt;deep</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>画相图-计算可能的合成路径</title>
    <link href="http://example.com/2025/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E7%94%BB%E7%9B%B8%E5%9B%BE-%E8%AE%A1%E7%AE%97%E5%90%88%E6%88%90%E8%B7%AF%E5%BE%84/"/>
    <id>http://example.com/2025/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E7%94%BB%E7%9B%B8%E5%9B%BE-%E8%AE%A1%E7%AE%97%E5%90%88%E6%88%90%E8%B7%AF%E5%BE%84/</id>
    <published>2025-05-25T01:23:40.000Z</published>
    <updated>2025-12-02T12:08:50.910Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-绘图脚本（相图、巨势相图）"><a href="#1-绘图脚本（相图、巨势相图）" class="headerlink" title="1.绘图脚本（相图、巨势相图）"></a>1.绘图脚本（相图、巨势相图）</h1><blockquote><p>使用material project数据库中的数据要求赝势和mp数据库一致，如果有磁性要求U值也一致。否则在MaterialsProjectCompatibility() 后会输出为none</p></blockquote><p>检查为什么是none</p><pre><code class="python">from pymatgen.entries.compatibility import MaterialsProjectCompatibility# 检查原始条目print(&quot;原始条目:&quot;, entry_vasp)print(&quot;结构:&quot;, entry_vasp.structure)print(&quot;成分:&quot;, entry_vasp.composition)print(&quot;能量:&quot;, entry_vasp.energy)# 处理条目并调试compatibility = MaterialsProjectCompatibility(check_potcar_hash=False)entry_from_vasp = compatibility.process_entry(entry_vasp)if entry_from_vasp is None:    print(&quot;处理失败，原因:&quot;, compatibility.get_explanation_dict(entry_vasp))else:    print(&quot;处理后结果:&quot;, entry_from_vasp)</code></pre><p>查看material project中使用的u值</p><pre><code class="python">from mp_api.client import MPRester# 用你的 API 密钥替换 &quot;your_api_key&quot;with MPRester(&quot;your_api_key&quot;) as mpr:    # 查询特定材料，例如 Fe2O3 (mp-565814)    material_id = &quot;mp-565814&quot;  # Fe2O3 的 Materials Project ID    entry = mpr.get_entry_by_material_id(material_id)        # 检查计算参数中的 hubbards    parameters = entry.parameters    if &quot;hubbards&quot; in parameters:        hubbards = parameters[&quot;hubbards&quot;]        print(f&quot;Hubbard U values: &#123;hubbards&#125;&quot;)    else:        print(&quot;This calculation does not use Hubbard U (likely GGA only).&quot;)</code></pre><h2 id="制作与mp数据库相容的计算参数"><a href="#制作与mp数据库相容的计算参数" class="headerlink" title="制作与mp数据库相容的计算参数"></a>制作与mp数据库相容的计算参数</h2><blockquote><p><a href="https://docs.materialsproject.org/methodology/materials-methodology/calculation-details/gga+u-calculations/pseudopotentials">https://docs.materialsproject.org/methodology/materials-methodology/calculation-details/gga+u-calculations/pseudopotentials</a></p></blockquote><pre><code class="python">from pymatgen.core import Structurefrom pymatgen.io.vasp.sets import MPStaticSetimport osstructure = Structure.from_file(&quot;POSCAR&quot;)vasp_input_set = MPStaticSet(structure)output_dir = &quot;static_vasp_inputs&quot;os.makedirs(output_dir, exist_ok=True)vasp_input_set.write_input(output_dir)</code></pre><p><strong>需要与mp数据库相符的赝势库</strong> （tianyi  &#x2F;home&#x2F;lizhao&#x2F;software&#x2F;pmg_potpat_PBE）</p><h2 id="1-相图-新的"><a href="#1-相图-新的" class="headerlink" title="1. 相图-新的"></a>1. 相图-新的</h2><h3 id="1-代码"><a href="#1-代码" class="headerlink" title="1.代码"></a>1.代码</h3><p>已经修正能量，注意关掉翻墙软件，需要安装新的API</p><p>新API地址：<a href="https://next-gen.materialsproject.org/api">https://next-gen.materialsproject.org/api</a></p><p><a href="https://matgenb.materialsvirtuallab.org/2021/05/12/Explanation-of-Corrections.html">https://matgenb.materialsvirtuallab.org/2021/05/12/Explanation-of-Corrections.html</a> 有时候新api并不好用</p><p><a href="https://docs.materialsproject.org/methodology/materials-methodology/thermodynamic-stability/phase-diagrams-pds">https://docs.materialsproject.org/methodology/materials-methodology/thermodynamic-stability/phase-diagrams-pds</a></p><p>新的MaterialsProject2020compatibility</p><p><a href="https://matgenb.materialsvirtuallab.org/2021/05/12/Explanation-of-Corrections.html">https://matgenb.materialsvirtuallab.org/2021/05/12/Explanation-of-Corrections.html</a> </p><pre><code class="python">from mp_api.client import MPResterfrom pymatgen.core import compositionfrom pymatgen.io.vasp.outputs import Vasprunfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntryfrom pymatgen.entries.compatibility import MaterialsProjectCompatibilitympr=MPRester(&quot;5BhZJctjosrFpkuE6qErpu85dT4gw4VZ&quot;)#entries = mpr.get_entries_in_chemsys(elements=[&quot;Li&quot;, &quot;Fe&quot;, &quot;O&quot;])#entries = mpr.materials.tasks.get_data_by_id(&quot;mp-2352&quot;)mp_entries = mpr.get_entries_in_chemsys(elements=[&quot;Na&quot;,&quot;Y&quot;,&quot;Si&quot;,&quot;O&quot;],additional_criteria=&#123;&quot;thermo_types&quot;: [&quot;GGA_GGA+U&quot;]&#125;) vasprun=Vasprun(&#39;vasprun.xml&#39;)entry_vasp = vasprun.get_computed_entry(inc_structure=True)compatibility = MaterialsProjectCompatibility()entry_from_vasp = compatibility.process_entry(entry_vasp)mp_entries.append(entry_from_vasp)pd=PhaseDiagram(mp_entries)stable_entries=list(pd.stable_entries)stable_entries.append(entry_from_vasp)pd2=PhaseDiagram(stable_entries)#print(stable_entries)print(stable_entries[-1])print(pd2.get_e_above_hull(stable_entries[-1]))plotter = PDPlotter(pd2)plotter.show()plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)###下面是拓展#可以看出四元相图中重合的线，关掉了颜色显示a = plotter.get_plot(fill=False)a.show()https://pymatgen.org/pymatgen.analysis.html#pymatgen.analysis.phase_diagram.PhaseDiagramprint(pd2.dim)print(pd2.qhull_entries) 给出hull上所有化合物print(pd2.qhull_data) 给出构建相图的数据，比实际相图维度少一维度，以三元A B C为例 知道A:B A:C那么B:C自然确定了，不需要增加维度print(pd2.all_entries_hulldata)   给出hull上的所有坐标和大小print(pd2.get_e_above_hull)  输出所有稳定的化合物print(pd2.computed_data) #包括facets(连接方式)  #simplexes(所有的形的坐标，三维相图里的形是二维的三角形，每个点都是坐标，坐标只有两个是因为只需要两个就能确定三角相图中一个点的位置) #all_entries(所有的化合物) #qhull_data(注意不能直接复制下来做相图，因为比实际多一行带2的，去掉这一行才可以。用于构建hull的数据，比实际相图维度少一维度，以三元A B C为例 知道A:B A:C那么B:C自然确定了，不需要增加) 多了一个1/3，1/3，2 多的应该是有用的，可能是用来做标定，实际上用不到，在算facets的时候没有考虑这一行。用自己下面的脚本才比较合适#确定的方法：1-x-y #material project自己画的图是顺时针坐标，给出了x和z，y没给出，需要1-x-z,但是origin是逆时针画图,需要双击画出来的图，点三元图，改成顺时针，然后双击图，调出图层属性界面，在显示部分，勾选输在坐标轴前面#dim(维度) #el_refs(hull的顶点)  #qhull_entries(用于构建hull/hull上的化合物)#print([entrya.composition.reduced_formula for entrya in pd2.qhull_entries])输出用于构建相图的简化的化学式#输出带某个化合物的组合，用于构建化合物相图import numpy as npprint(pd2.facets)  输出所有连接方式  每个数字与qhull.entries中的对应位置对应matrix2 = np.vstack(pd2.facets)rows_with_12 = matrix2[(matrix2 == 12).any(axis=1)]#绘制化合物相图from pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntry,CompoundPhaseDiagramfrom pymatgen.core import CompositionSiO2 = Composition(&quot;SiO2&quot;)Y2O3 = Composition(&quot;Y2O3&quot;)Na2O = Composition(&quot;Na2O&quot;)pd3=CompoundPhaseDiagram(stable_entries,[SiO2,Y2O3,Na2O])plotterb=PDPlotter(pd3)plotterb.show()#给出各个化合物的配比方式 注意是在stable_entries的基础上进行计算的，这样得到的都是稳定的#自动排除了那些不在面上的化合物，相当于一个提取，得到了转换后的化合物ppp,ddd=pd3.transform_entries(stable_entries,[SiO2,Y2O3,Na2O])print(ppp) #可以显示出实际的配分数#给出各个化合物的配比方式-更加直接一点import numpy as np#elements_fomula = sorted(&#123;els for e in ppp for els in e.composition.elements&#125;)#elements_fomula=list(elements_fomula)elements_fomula=pd3.elementsdata_fomula = np.array([[e.composition.get_atomic_fraction(el) for el in elements_fomula] for e in ppp])print(data_fomula)np.savetxt(&#39;fra.dat&#39;,data_fomula, delimiter=&#39;\t&#39;, fmt=&#39;%d&#39;)#data = np.array([e.composition.get_atomic_fraction(el) for el in elements] + [e.energy_per_atom] for e in min_entries])print(ppp)print([aa.composition for aa in ppp])#在绘制伪二元相图（两边都是二元化合物）时，pd3.get_e_above_hull会不好使用，这时候用下面的程序能够输出所有高于相图的能量for entrya in pd3.qhull_entries:     print(entrya)</code></pre><h3 id="2-绘制三元-赝三元相图代码"><a href="#2-绘制三元-赝三元相图代码" class="headerlink" title="2. 绘制三元&#x2F;赝三元相图代码"></a>2. 绘制三元&#x2F;赝三元相图代码</h3><pre><code class="python">plotter = PDPlotter(pd2)plotter.show()plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)</code></pre><p>以Y-O为顶点和以Y-YSi为顶点，得到的Y5Si3的线段是不同的，赝三元相图的图像里线段不能直接比来获得比例</p><p><img src="/./../../%E5%9B%BE%E7%89%87/d82b7383013d9a8faa68b0a453108e3.jpg"></p><h3 id="3-化合物相图的输出"><a href="#3-化合物相图的输出" class="headerlink" title="3. 化合物相图的输出"></a>3. 化合物相图的输出</h3><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231208093545191.png" alt="image-20231208093545191"></p><p>输出的化合物相图的数据，以虚拟物种的方式出现，其中，上面是每个化合物，虚拟物种xf0、xh0、xg0后面的数字代表需要的组合</p><h3 id="4-pymatgen中的代码结构"><a href="#4-pymatgen中的代码结构" class="headerlink" title="4.pymatgen中的代码结构"></a>4.pymatgen中的代码结构</h3><h4 id="1-化学式相图"><a href="#1-化学式相图" class="headerlink" title="1.化学式相图"></a>1.化学式相图</h4><p>本质上是重构entry，把化合物变成单元的</p><pre><code class="python">class CompoundPhaseDiagram(PhaseDiagram):    #化学式相图的类是继承自phaseDiagram,继承的意思是子类可以使用父类的所有属性和方法    def __init__(self, entries, terminal_compositions, normalize_terminal_compositions=True):        self.original_entries = entries        self.terminal_compositions = terminal_compositions        self.normalize_terminals = normalize_terminal_compositions        p_entries, species_mapping = self.transform_entries(entries, terminal_compositions)        self.species_mapping = species_mapping        super().__init__(p_entries, elements=species_mapping.values())        #super()是用子类的这两个参数去实例化phasediagram 这两个参数分别对应虚拟化的化合物分数坐标和顶点的虚拟化物种</code></pre><h4 id="2-相图"><a href="#2-相图" class="headerlink" title="2. 相图"></a>2. 相图</h4><pre><code class="python">class PhaseDiagram(MSONable):    #phasediagram继承MSONable，这个类可以用来做json的数据输入输出    formation_energy_tol = 1e-11    numerical_tol = 1e-8    def __init__(        self,        entries: Sequence[PDEntry] | set[PDEntry],        elements: Sequence[Element] = (),        *,        computed_data: dict[str, Any] | None = None,    ) -&gt; None:        #computed_data接受一个字典或者none。如果提供的是一个字典，它可能包含了一些预先计算好的数据，如相图中的临界点、相边界等        #-none表示构造函数没有返回值        self.elements = elements        self.entries = entries        if computed_data is None:            computed_data = self._compute()            #计算得到相图的数据</code></pre><pre><code class="python">def _compute(self) -&gt; dict[str, Any]:        if self.elements == ():            self.elements = sorted(&#123;els for e in self.entries for els in e.elements&#125;)            #找出所有的元素，四种、三种、两种。。。        elements = list(self.elements)        dim = len(elements)        entries = sorted(self.entries, key=lambda e: e.composition.reduced_composition)        el_refs: dict[Element, PDEntry] = &#123;&#125;        min_entries: list[PDEntry] = []        all_entries: list[PDEntry] = []        for composition, group_iter in itertools.groupby(entries, key=lambda e: e.composition.reduced_composition):            group = list(group_iter)            min_entry = min(group, key=lambda e: e.energy_per_atom)            if composition.is_element:                el_refs[composition.elements[0]] = min_entry            min_entries.append(min_entry)            all_entries.extend(group)            #对entries进行分类，找到顶点、和各个组成中能量最低的entry        if missing := set(elements) - set(el_refs):            raise ValueError(f&quot;Missing terminal entries for elements &#123;sorted(map(str, missing))&#125;&quot;)        if extra := set(el_refs) - set(elements):            raise ValueError(f&quot;There are more terminal elements than dimensions: &#123;sorted(map(str, extra))&#125;&quot;)        data = np.array(            [[e.composition.get_atomic_fraction(el) for el in elements] + [e.energy_per_atom] for e in min_entries]        )        #用于绘图的坐标数据        #前三列是分数的坐标，最后一列是能量，后面dot一下就计算出来每个entry相对原点的形成能        # Use only entries with negative formation energy        vec = [el_refs[el].energy_per_atom for el in elements] + [-1]        #-1直接拼接在后面，点乘后相当于直接 -E        form_e = -np.dot(data, vec)        idx = np.where(form_e &lt; -PhaseDiagram.formation_energy_tol)[0].tolist()        #得到        # Add the elemental references        idx.extend([min_entries.index(el) for el in el_refs.values()])        #extend 把list2中的元素一个接一个添加到list1        #把min_entries对应顶点位置的坐标的索引增加到idx        qhull_entries = [min_entries[i] for i in idx]        #所有hull上的entries        qhull_data = data[idx][:, 1:]                # Add an extra point to enforce full dimensionality.        # This point will be present in all upper hull facets.        extra_point = np.zeros(dim) + 1 / dim        #创建一个一行三列的向量        extra_point[-1] = np.max(qhull_data) + 1        qhull_data = np.concatenate([qhull_data, [extra_point]], axis=0)        #增加了一行                 if dim == 1:            facets = [qhull_data.argmin(axis=0)]        else:            facets = get_facets(qhull_data)            final_facets = []            for facet in facets:                # Skip facets that include the extra point                if max(facet) == len(qhull_data) - 1:                    continue                    #去除增加的行的影响，不清楚为什么加一行，可能前面有用                m = qhull_data[facet]                m[:, -1] = 1                if abs(np.linalg.det(m)) &gt; 1e-14:                    final_facets.append(facet)            facets = final_facets        simplexes = [Simplex(qhull_data[f, :-1]) for f in facets]        #用于找到不同三角形的坐标        self.elements = elements        return &#123;            &quot;facets&quot;: facets,            &quot;simplexes&quot;: simplexes,            &quot;all_entries&quot;: all_entries,            &quot;qhull_data&quot;: qhull_data,            &quot;dim&quot;: dim,            # Dictionary with Element keys is not JSON-serializable            &quot;el_refs&quot;: list(el_refs.items()),            &quot;qhull_entries&quot;: qhull_entries,        &#125;</code></pre><p>computed_data</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231208101440132.png" alt="image-20231208101440132"></p><h2 id="2-老的，似乎不能用了-不能从materials-project数据库中提取数据了"><a href="#2-老的，似乎不能用了-不能从materials-project数据库中提取数据了" class="headerlink" title="2. 老的，似乎不能用了 (不能从materials project数据库中提取数据了)"></a>2. 老的，似乎不能用了 (不能从materials project数据库中提取数据了)</h2><pre><code class="python">from pymatgen.io.vasp.outputs import Vasprunimport pandas as pdfrom pymatgen.ext.matproj import MPResterfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntryfrom pymatgen.entries.compatibility import MaterialsProjectCompatibilityvasprun=Vasprun(&#39;vasprun.xml&#39;)entry = vasprun.get_computed_entry(inc_structure=True)compatibility = MaterialsProjectCompatibility()entry = compatibility.process_entry(entry)a=MPRester(&#39;BhgyCJsKOOA3cRIi&#39;)mp_entries=a.get_entries_in_chemsys([&#39;K&#39;,&#39;Y&#39;,&#39;Si&#39;,&#39;O&#39;])mp_entries.append(entry)phd=PhaseDiagram(mp_entries)stable_entries=list(phd.stable_entries)stable_entries.append(entry)phd2=PhaseDiagram(stable_entries)#print(stable_entries)print(stable_entries[-1])print(phd2.get_e_above_hull(stable_entries[-1]))plotter = PDPlotter(phd2)plotter.show()</code></pre><h2 id="3-从现有数据的-dat文件读取然后绘制相图"><a href="#3-从现有数据的-dat文件读取然后绘制相图" class="headerlink" title="3. 从现有数据的.dat文件读取然后绘制相图"></a>3. 从现有数据的.dat文件读取然后绘制相图</h2><p>（&#x2F;work&#x2F;home&#x2F;liz&#x2F;workspace&#x2F;1-system-MD&#x2F;2-Li5YSi4O12&#x2F;6phase&#x2F;LYSO）</p><p>一个lyso.dat</p><pre><code class="shell">H2 , -25.63388726Ca8H32 , -8476.873008Ca8H46 , -8661.706131Ca8H48 , -8686.909539#注意下面不能有空行</code></pre><pre><code class="python">from pymatgen.analysis.phase_diagram import *from pymatgen.core.periodic_table import Element, DummySpeciefrom pymatgen.core.composition import Compositionentries = []with open(&#39;lyso.dat&#39;) as fr :    for line in fr :      comp, ene = line.split(&#39;,&#39;)      entries.append(PDEntry(Composition(comp), float(ene)))pd = PhaseDiagram(entries)#stable_formulas = [ent.name for ent in pd.stable_entries]entry=PDEntry(composition=&#39;Li30Y6Si24O72&#39;,energy=-937.044768)stable_entries=list(pd.stable_entries)#print(pd.stable_entries)stable_entries.append(entry)phd=PhaseDiagram(stable_entries)#plotter=PDPlotter(phd,show_unstable=True)#plotter.show()print(stable_entries[-1])print(phd.get_e_above_hull(stable_entries[-1]))#for formula in stable_formulas:#        print (formula)#for e in entries:#        ehull = pd.get_e_above_hull(e)#        print (e.composition, ehull)#plotter = PDPlotter(pd)#plotter.show()#plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)#entries.append(PDEntry(Composition(&#39;Li2MgH15&#39;),float(3.1)))#pd.get_e_above_hull(entries[9])#pd.get_all_chempots(Composition(&#39;MgH16&#39;)) 一个化合物中所有元素的化学势#pd.get_chempot_range_map([Element(&#39;Li&#39;)])  每个化合物某种元素的化学势范围#get_composition_chempots(comp)print(phd.get_decomp_and_e_above_hull(stable_entries[-1]))</code></pre><h2 id="4-巨势相图"><a href="#4-巨势相图" class="headerlink" title="4. 巨势相图"></a>4. 巨势相图</h2><p>考虑一种特殊情况，有同组分亚稳相和稳定相在相图上，这时绘制巨势相图会发生什么，</p><p><img src="/./../../%E5%9B%BE%E7%89%87/e97d86684e01ec3eb45810292f0c2d81.png" alt="e97d86684e01ec3eb45810292f0c2d81"></p><p>在其他位置，亚稳相和稳相的分解都是一样的，因为在绘制相图时，稳相都会分解（落到三角或者连线上），更何况亚稳相呢，同样会分解。但是在uptake &#x3D; 0 时（一定有这个点，因为巨势相图就是减去多少个Li后的相图，减去之后一定在同样的位置要么稳定，要么分解。稳相在uptake&#x3D;0处是稳定的，但是亚稳相会落到稳相上，因为稳相能量更低。所以表面看uptake&#x3D;0时没有分解，实际暗含了那里会发生相变，这里不用考虑是否有相变势垒，因为这张图里都是热力学判断，实际上其他反应也会有动力学势垒。</p><hr><p>引申处一个问题，实际上只需要给出相图，给出组分，就能绘制巨势相图，因为给出组分，如果没有对应的稳相，那就一定会按照相图分解（在三角里或者连线上），所以实际不需要组分的能量了，只需要绘制相图时把这个组分加能量考虑进去就可以了</p><hr><pre><code class="python">from pymatgen.io.vasp.outputs import Vasprunimport pandas as pdfrom pymatgen.ext.matproj import MPResterfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntryfrom pymatgen.entries.compatibility import MaterialsProjectCompatibilityfrom pymatgen.analysis.phase_diagram import *from pymatgen.core.periodic_table import Element, DummySpeciefrom pymatgen.core.composition import Compositionvasprun=Vasprun(&#39;vasprun.xml&#39;)entry = vasprun.get_computed_entry(inc_structure=True)compatibility = MaterialsProjectCompatibility()entry = compatibility.process_entry(entry)a=MPRester(&#39;BhgyCJsKOOA3cRIi&#39;)mp_entries=a.get_entries_in_chemsys([&#39;Na&#39;,&#39;Y&#39;,&#39;Si&#39;,&#39;O&#39;])mp_entries.append(entry)phd=PhaseDiagram(mp_entries)li_entries = [e for e in mp_entries if e.composition.reduced_formula == &quot;Li&quot;]uli0 = min(li_entries, key=lambda e: e.energy_per_atom).energy_per_atomel_profile = phd.get_element_profile(Element(&quot;Li&quot;), mp_entries[-1].composition)for i, d in enumerate(el_profile):    voltage = -(d[&quot;chempot&quot;] - uli0)    print(&quot;Voltage: %s V&quot; % voltage)    print(d[&quot;reaction&quot;])    print(&quot;&quot;)</code></pre><p>新的</p><pre><code class="python">from mp_api.client import MPResterfrom pymatgen.core import compositionfrom pymatgen.io.vasp.outputs import Vasprunfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntryfrom pymatgen.entries.compatibility import MaterialsProjectCompatibilityfrom pymatgen.core.periodic_table import Element, DummySpeciempr=MPRester(&quot;5BhZJctjosrFpkuE6qErpu85dT4gw4VZ&quot;)mp_entries = mpr.get_entries_in_chemsys(elements=[&quot;Na&quot;,&quot;Y&quot;,&quot;Si&quot;,&quot;O&quot;],additional_criteria=&#123;&quot;thermo_types&quot;: [&quot;GGA_GGA+U&quot;]&#125;)vasprun=Vasprun(&#39;Na.xml&#39;)entry_vasp = vasprun.get_computed_entry(inc_structure=True)compatibility = MaterialsProjectCompatibility()entry_from_vasp = compatibility.process_entry(entry_vasp)mp_entries.append(entry_from_vasp)phd=PhaseDiagram(mp_entries)na_entries = [e for e in mp_entries if e.composition.reduced_formula == &quot;Na&quot;]una0 = min(na_entries, key=lambda e: e.energy_per_atom).energy_per_atomprint(mp_entries[-1])el_profile = phd.get_element_profile(Element(&quot;Na&quot;), mp_entries[-1].composition)for i, d in enumerate(el_profile):    voltage = -(d[&quot;chempot&quot;] - una0)    print(&quot;Voltage: %s V&quot; % voltage)    print(d[&quot;reaction&quot;])    print(&quot;&quot;)</code></pre><h1 id="2-相图解释-（三元三角、四元四面体）"><a href="#2-相图解释-（三元三角、四元四面体）" class="headerlink" title="2. 相图解释 （三元三角、四元四面体）"></a>2. 相图解释 （三元三角、四元四面体）</h1><h2 id="1-相图的理解"><a href="#1-相图的理解" class="headerlink" title="1.相图的理解"></a>1.相图的理解</h2><p><code>E_above_hull</code>并不是垂直线的高度，而是分解为hull上的三个化合物的分解能，经过了验证</p><p>x y z z </p><p>四面体只需要四个顶点</p><p>三维相图</p><p>&#x3D;&#x3D;逆时针&#x3D;&#x3D;</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231208204343051.png" alt="image-20231208204343051"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211650444.png" alt="image-20231206211650444"></p><p>#print([entrya.composition.reduced_formula for entrya in pd2.qhull_entries]) </p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211706137.png" alt="image-20231206211706137"></p><p>#print(pd2.facets)  输出所有连接方式 (标号是上面的顺序，从0开始 )     pymatgen</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211658347.png" alt="image-20231206211658347"></p><p>四元</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213332475.png" alt="image-20231206213332475"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213336951.png" alt="image-20231206213336951"><br><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213344510.png" alt="image-20231206213344510"></p><p>目的是把块分成区域，每个区域不再能分配</p><p>三元相图是由一系列的小三元相图构图的，而小三元相图是由二元相图的边构成的。同理，四元相图由小四元四面体组成，而小四元四面体的边是二元相图，一个面是一个三元相图。</p><p>&#x3D;&#x3D;三元比二元多出来的特征在于可以有一些只能分解为三元的化合物，其他的稳定化合物完全可以以化合物为顶点做一个二元相图。而四元多出来的特征在于可以有一些只能分解为四个组分的化合物（在四面体中间）,要表示四元相图中的稳定结构，完全可以用三元相图、或者二元相图表示。&#x3D;&#x3D;</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207101533459.png" alt="image-20231207101533459"></p><h2 id="1-关于分解"><a href="#1-关于分解" class="headerlink" title="1.关于分解"></a>1.关于分解</h2><p>&#x3D;&#x3D;二元相图的横坐标是 y&#x2F;(x+y) ，y占据的百分数，二元的点本质上纵坐标都可以设置为在线上，只不过是能量的差别设置出来了高度，落在哪条线上就会分解为线的两端，三元落在哪个三角里就会分解为哪三个化合物，四元落在哪个四面体内就会分解为哪四个化合物&#x3D;&#x3D;</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207102045578.png" alt="image-20231207102045578"></p><p>E总是可以分解为C和D，当然也可以分解为A和D，但是一部分A和D可以生成更低的C，说明</p><p>E—A+D 并不完全，后面还有  A+D—–C+D，因此最终必然分解为C+D</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207102428536.png" alt="image-20231207102428536"></p><p>对于三元，F总能分解为A+B+D ，也可分解为A+B+C，但是A+B+C会合成D（因为D总是由A、B、C组成，道理和下面的解释一样），最终还是直接分解为D的能量最稳定。</p><p>为什么F总能分解为A+B+D? ： 因为F总能由A和H组成，而H总能由B+D组成，最终，F总能由A+B+D组成。</p><p>四元同理，四元的四面体内的不稳定化合物总可以先分解为三角形，再分解为线，再分解为点。</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207102751346.png" alt="image-20231207102751346"></p><h1 id="3-计算化合物的分解"><a href="#3-计算化合物的分解" class="headerlink" title="3. 计算化合物的分解"></a>3. 计算化合物的分解</h1><pre><code class="python">print(phd.get_decomp_and_e_above_hull(stable_entries[-1]))可以给出分解的路径，进而求出结合能（和e_above_hull是不一样的）也可以给出结合能，用里面的函数</code></pre><p>路径 &#x2F;work&#x2F;home&#x2F;liz&#x2F;workspace&#x2F;1-system-MD&#x2F;2-Li5YSi4O12&#x2F;6phase&#x2F;LYSO</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231013111011468.png" alt="image-20231013111011468"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/a.jpg" alt="a"></p><h1 id="4-绘制相图（三元三角、四元四面体）"><a href="#4-绘制相图（三元三角、四元四面体）" class="headerlink" title="4.绘制相图（三元三角、四元四面体）"></a>4.绘制相图（三元三角、四元四面体）</h1><p><a href="https://www.originlab.com/doc/en/Origin-Help/3d-Tetrahedral">https://www.originlab.com/doc/en/Origin-Help/3d-Tetrahedral</a></p><p>x y z z </p><p>四面体只需要四个顶点</p><p>三维相图</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211650444.png" alt="image-20231206211650444"></p><p>#print([entrya.composition.reduced_formula for entrya in pd2.qhull_entries]) </p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211706137.png" alt="image-20231206211706137"></p><p>#print(pd2.facets)  输出所有连接方式 (标号是上面的顺序，从0开始 )     pymatgen</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206211658347.png" alt="image-20231206211658347"></p><p>四元</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213332475.png" alt="image-20231206213332475"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213336951.png" alt="image-20231206213336951"><br><img src="/./../../%E5%9B%BE%E7%89%87/image-20231206213344510.png" alt="image-20231206213344510"></p><p>目的是把块分成区域，每个区域不再能分配</p><p>三元相图是由一系列的小三元相图构图的，而小三元相图是由二元相图的边构成的。同理，四元相图由小四元四面体组成，而小四元四面体的边是二元相图，一个面是一个三元相图。</p><p>&#x3D;&#x3D;三元比二元多出来的特征在于可以有一些只能分解为三元的化合物，其他的稳定化合物完全可以以化合物为顶点做一个二元相图。而四元多出来的特征在于可以有一些只能分解为四个组分的化合物（在四面体中间）,要表示四元相图中的稳定结构，完全可以用三元相图、或者二元相图表示。&#x3D;&#x3D;</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231207101533459.png" alt="image-20231207101533459"></p><h1 id="5-计算可能的合成路径"><a href="#5-计算可能的合成路径" class="headerlink" title="5. 计算可能的合成路径"></a>5. 计算可能的合成路径</h1><pre><code class="python">from pymatgen.core import Compositionfrom sympy import symbols, Eq, solvefrom itertools import combinationsfrom mp_api.client import MPResterfrom pymatgen.io.vasp.outputs import Vasprunfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntry,CompoundPhaseDiagramfrom pymatgen.entries.compatibility import MaterialsProjectCompatibilitydef solvechemical(entries,target_entry):    reduced_a = entries[0].composition.reduced_formula    reduced_b = entries[1].composition.reduced_formula    reduced_c = entries[2].composition.reduced_formula#     print(reduced_a,reduced_b,reduced_c)    reduced_aa = entries[0].composition    reduced_bb = entries[1].composition    reduced_cc = entries[2].composition    reduced_target = target_entry.composition    compounda, factora = reduced_aa.get_reduced_composition_and_factor()    compoundb, factorb = reduced_bb.get_reduced_composition_and_factor()    compoundc, factorc = reduced_cc.get_reduced_composition_and_factor()    reduced_compositiontarget, factortarget = reduced_target.get_reduced_composition_and_factor()    compound_a = Composition(reduced_a)    compound_b = Composition(reduced_b)    compound_c = Composition(reduced_c)    target_compound = Composition(target_entry.composition.reduced_formula)    x, y, z = symbols(&#39;x y z&#39;)    # 获取目标化合物所含元素    elements = target_compound.as_dict().keys()       #不能是通解的判断     # 为每个元素设置质量守恒方程    equations = []        for element in elements:        # 每个化合物中该元素的数量        eq = (            x * compound_a.get(element, 0) +            y * compound_b.get(element, 0) +            z * compound_c.get(element, 0) -            target_compound.get(element, 0)        )        equations.append(Eq(eq, 0))    solution = solve(equations, (x, y, z), dict = True)    # 判断并输出结果    if solution:        valid_solutions = []        for sol in solution:        # 检查解是否包含自由变量(通解）            has_free_symbols = any(val.free_symbols for val in sol.values())        # 检查解是否全部非负且为具体数值            if not has_free_symbols:                 is_valid = all(val.evalf() &gt;= 0 and val.is_number for val in sol.values())                if is_valid:                    valid_solutions.append(sol)        if valid_solutions:            coeffs = solution[0]            formation_energy = (target_entry.energy / factortarget - (coeffs[x] / factora * entries[0].energy + coeffs[y] / factorb * entries[1].energy + coeffs[z] / factorc * entries[2].energy))            results.append((f&quot;&#123;reduced_a&#125;, &#123;reduced_b&#125;, &#123;reduced_c&#125;&quot;, [value for value in solution[0].values()], formation_energy))            # print(reduced_a,reduced_b,reduced_c,end = &#39;,&#39;)            # print(entries[0].energy/factora,entries[1].energy/factorb,entries[2].energy/factorc, target_entry.energy / factortarget,  end = &#39;,&#39;)            # print(f&quot;可以生成目标化合物，配平系数为: &#123;solution&#125;&quot;)            # print(print([value for item in solution for value in item.values()]),end = &#39;,&#39;)            #print(([value for value in solution[0].values()]),end = &#39;,&#39;)            #entries = list(entries)            #entries.append(target_entry)            print(f&quot;目标化合物 &#123;target_entry.composition.reduced_formula&#125; 的形成能为: &#123;formation_energy:.3f&#125; eV&quot;)            #print(f&quot;&#123;formation_energy:.3f&#125; eV&quot;)    #    else:    #        print(&quot;无法通过这三个化合物生成目标化合物。&quot;)mpr=MPRester(&quot;5BhZJctjosrFpkuE6qErpu85dT4gw4VZ&quot;)mp_entries = mpr.get_entries_in_chemsys(elements=[&quot;K&quot;,&quot;Y&quot;,&quot;Si&quot;,&quot;O&quot;],additional_criteria=&#123;&quot;thermo_types&quot;: [&quot;GGA_GGA+U&quot;]&#125;) vasprun=Vasprun(&#39;vasprun.xml&#39;)vasprun2 = Vasprun(&#39;oqmd.xml&#39;)entry_vasp = vasprun.get_computed_entry(inc_structure=True)entry_vasp2 = vasprun2.get_computed_entry(inc_structure=True)compatibility = MaterialsProjectCompatibility()entry_from_vasp = compatibility.process_entry(entry_vasp)entry_from_vasp2 = compatibility.process_entry(entry_vasp2)mp_entries.append(entry_from_vasp)mp_entries.append(entry_from_vasp2)pd=PhaseDiagram(mp_entries)stable_entries=list(pd.stable_entries)print(len(stable_entries))entry_vasp_formula = entry_vasp.composition.reduced_formulastable_entries = [entry for entry in stable_entries if entry.composition.reduced_formula != entry_vasp_formula]print(len(stable_entries))results = []for entry_combo in combinations(stable_entries,3):    solvechemical(entry_combo,entry_from_vasp)results.sort(key=lambda x: x[2])for combo, coeffs, energy in results:    print(f&quot;&#123;combo&#125;, &#123;coeffs&#125;, &#123;energy:.3f&#125; eV&quot;)</code></pre><p>转换输出为latex格式</p><pre><code class="python">import redef read_and_convert(file_path):    with open(file_path, &#39;r&#39;, encoding=&#39;utf-16&#39;) as file:        lines = file.readlines()    latex_output = []    for line in lines:        # 去除空格和换行符        line = line.strip()        # 使用正则表达式匹配和提取数据        match = re.match(r&#39;([\w\d, ]+), \[([\d., ]+)\], (-?\d+\.\d+) eV&#39;, line)        if match:            compounds = match.group(1).split(&#39;, &#39;)            coefficients = list(map(float, match.group(2).split(&#39;, &#39;)))            energy = match.group(3)            # 构建LaTeX化学方程式            latex_equation = &#39;\\ce&#123;&#39;            for i, compound in enumerate(compounds):                coefficient = coefficients[i]                if coefficient != 1.0:                    latex_equation += f&#39;&#123;coefficient:.3g&#125; &#39;                latex_equation += compound + &#39; + &#39;            latex_equation = latex_equation.rstrip(&#39; + &#39;) + &#39;&#125; &amp; &#39; + energy + &#39; eV&#39; + &#39; \\\\&#39;            latex_output.append(latex_equation)    return latex_output# 读取文件并转换file_path = &#39;solve2.dat&#39;latex_equations = read_and_convert(file_path)# 打印转换后的LaTeX化学方程式for equation in latex_equations:    print(equation)</code></pre><h1 id="6-help-Pr-congwei"><a href="#6-help-Pr-congwei" class="headerlink" title="6. help Pr. congwei"></a>6. help Pr. congwei</h1><ol><li>画普通相图</li></ol><pre><code class="python">#usage: python pd.py PD.txtimport pandas as pdfrom pymatgen.core import Compositionfrom pymatgen.analysis.phase_diagram import PDEntry, PhaseDiagram, PDPlotterimport argparseif __name__ == &quot;__main__&quot;:    parser = argparse.ArgumentParser(description=&quot;Process phase diagram from input file.&quot;)    parser.add_argument(&#39;filename&#39;, type=str, help=&#39;Path to the input file&#39;)    args = parser.parse_args()    filename = args.filenamecolspecs = [    (0, 25),    # Name    (25, 42),   # Composition    (42, 46),   # Sym    (46, 61),   # Enthalpy    (61, 80),   # Volume    (80, 97),  # Density    (99, 116), # FormEnthalpy    (116, None) # Instability (到最后)]all_data = pd.read_fwf(filename, colspecs=colspecs,header=0)compositions = all_data[&#39;Composition&#39;]enthalpy = all_data[&#39;Enthalpy&#39;]entries = []for i in range(len(compositions)):    comp = Composition(compositions[i])    total_atoms = comp.num_atoms    total_energy = total_atoms * enthalpy[i]    entry = PDEntry(comp,total_energy)    entries.append(entry)phase_diagram = PhaseDiagram(entries)plotter = PDPlotter(phase_diagram,show_unstable = 1)plotter.show()#plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)</code></pre><ol start="2"><li><p>画化合物相图</p><pre><code class="python">#usage: python pd.py PD.txtimport pandas as pdfrom pymatgen.core import Compositionfrom pymatgen.analysis.phase_diagram import PDEntry, PhaseDiagram, PDPlotter,CompoundPhaseDiagramimport argparseif __name__ == &quot;__main__&quot;:    parser = argparse.ArgumentParser(description=&quot;Process phase diagram from input file.&quot;)    parser.add_argument(&#39;filename&#39;, type=str, help=&#39;Path to the input file&#39;)    args = parser.parse_args()    filename = args.filenamecolspecs = [    (0, 25),    # Name    (25, 42),   # Composition    (42, 46),   # Sym    (46, 61),   # Enthalpy    (61, 80),   # Volume    (80, 97),  # Density    (99, 116), # FormEnthalpy    (116, None) # Instability (到最后)]all_data = pd.read_fwf(filename, colspecs=colspecs,header=0)compositions = all_data[&#39;Composition&#39;]enthalpy = all_data[&#39;Enthalpy&#39;]entries = []for i in range(len(compositions)):    comp = Composition(compositions[i])    total_atoms = comp.num_atoms    total_energy = total_atoms * enthalpy[i]    entry = PDEntry(comp,total_energy)    entries.append(entry)phase_diagram = PhaseDiagram(entries)plotter = PDPlotter(phase_diagram,show_unstable = 1)#plotter.show()plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)aa = Composition(&quot;Li2O&quot;)bb = Composition(&quot;B2O3&quot;)cc = Composition(&quot;LiF&quot;)pd2=CompoundPhaseDiagram(entries,[aa,bb,cc])plotterb=PDPlotter(pd2)plotterb.write_image(&#39;pd2.png&#39;, image_format=&#39;png&#39;)#plotterb.show()print(pd2.qhull_data)#plotter.write_image(&#39;pd.png&#39;, image_format=&#39;png&#39;)</code></pre></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-绘图脚本（相图、巨势相图）&quot;&gt;&lt;a href=&quot;#1-绘图脚本（相图、巨势相图）&quot; class=&quot;headerlink&quot; title=&quot;1.绘图脚本（相图、巨势相图）&quot;&gt;&lt;/a&gt;1.绘图脚本（相图、巨势相图）&lt;/h1&gt;&lt;blockqu</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>一些python脚本</title>
    <link href="http://example.com/2025/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%B8%80%E4%BA%9Bpython%E8%84%9A%E6%9C%AC/"/>
    <id>http://example.com/2025/05/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E4%B8%80%E4%BA%9Bpython%E8%84%9A%E6%9C%AC/</id>
    <published>2025-05-25T01:23:40.000Z</published>
    <updated>2025-12-02T12:08:58.967Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-确定空间群信息"><a href="#1-确定空间群信息" class="headerlink" title="1. 确定空间群信息"></a>1. 确定空间群信息</h1><pre><code class="python">from pymatgen.core.structure import Structure, Moleculefrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerposcar = Structure.from_file(&#39;CONTCAR&#39;)analyzer = SpacegroupAnalyzer(poscar,0.05,5)print(analyzer.get_space_group_symbol())print(analyzer.get_space_group_number())</code></pre><h1 id="2-做吸附构型"><a href="#2-做吸附构型" class="headerlink" title="2. 做吸附构型"></a>2. 做吸附构型</h1><pre><code class="python">from pymatgen.core.structure import Structure, Moleculefrom pymatgen.core.lattice import Latticefrom pymatgen.ext.matproj import MPResterfrom pymatgen.analysis.adsorption import *from pymatgen.core.surface import generate_all_slabsfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerfrom matplotlib import pyplot as pltfrom pymatgen.ext.matproj import MPResterfrom pymatgen.io.vasp.inputs import Poscarfrom pymatgen.io.vasp import Poscar#---okInitial_POSCAR_File=&quot;./Initial-POSCAR&quot;poscar = Poscar.from_file(Initial_POSCAR_File)IniStructure = poscar.structureasf_ni_111 = AdsorbateSiteFinder(IniStructure)#创建一个AdsorbateSiteFinder对象。ads_sites = asf_ni_111.find_adsorption_sites()#然后用AdsorbateSiteFinder找到有可能的吸附位点。#print(ads_sites)adsorbate = Molecule(&quot;H&quot;,[[0,0,0]] )ads_structs = asf_ni_111.generate_adsorption_structures(adsorbate, repeat=[1, 1, 1])#--- ok#fig = plt.figure()index=0#for ads_struct in ads_structs:#    ads_struct.to(&#39;NewPOSCAR&#39;+str(index),&#39;poscar&#39;)#    index=index+1for cry_str in ads_structs:    Vasp_Str = Poscar(cry_str)    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%index)    index=index+1</code></pre><h1 id="3-构建POTCAR"><a href="#3-构建POTCAR" class="headerlink" title="3. 构建POTCAR"></a>3. 构建POTCAR</h1><pre><code class="python"># 字典：元素对应的赝势文件路径base_pseudo_path = &#39;/work/home/liz/vasp_pot/potpaw_PBE/&#39;pseudo_paths = &#123;    &#39;H&#39;: &#39;H/POTCAR&#39;,    &#39;Li&#39;: &#39;Li_sv/POTCAR&#39;,    &#39;Be&#39;: &#39;Be_sv/POTCAR&#39;,    &#39;B&#39;: &#39;B/POTCAR&#39;,    &#39;C&#39;: &#39;C/POTCAR&#39;,    &#39;N&#39;: &#39;N/POTCAR&#39;,    &#39;O&#39;: &#39;O/POTCAR&#39;,    &#39;F&#39;: &#39;F/POTCAR&#39;,    &#39;Ne&#39;: &#39;Ne/POTCAR&#39;,    &#39;Na&#39;: &#39;Na_pv/POTCAR&#39;,    &#39;Mg&#39;: &#39;Mg_pv/POTCAR&#39;,    &#39;Al&#39;: &#39;Al/POTCAR&#39;,    &#39;Si&#39;: &#39;Si/POTCAR&#39;,    &#39;P&#39;: &#39;P/POTCAR&#39;,    &#39;S&#39;: &#39;S/POTCAR&#39;,    &#39;Cl&#39;: &#39;Cl/POTCAR&#39;,    &#39;K&#39;: &#39;K_sv/POTCAR&#39;,    &#39;Ca&#39;: &#39;Ca_sv/POTCAR&#39;,    &#39;Sc&#39;: &#39;Sc_sv/POTCAR&#39;,    &#39;Ti&#39;: &#39;Ti_pv/POTCAR&#39;,    &#39;V&#39;: &#39;V_pv/POTCAR&#39;,    &#39;Cr&#39;: &#39;Cr_pv/POTCAR&#39;,    &#39;Mn&#39;: &#39;Mn_pv/POTCAR&#39;,    &#39;Fe&#39;: &#39;Fe_pv/POTCAR&#39;,    &#39;Co&#39;: &#39;Co/POTCAR&#39;,    &#39;Ni&#39;: &#39;Ni_pv/POTCAR&#39;,    &#39;Cu&#39;: &#39;Cu_pv/POTCAR&#39;,    &#39;Zn&#39;: &#39;Zn/POTCAR&#39;,    &#39;Ga&#39;: &#39;Ga_d/POTCAR&#39;,    &#39;Ge&#39;: &#39;Ge_d/POTCAR&#39;,    &#39;As&#39;: &#39;As/POTCAR&#39;,    &#39;Se&#39;: &#39;Se/POTCAR&#39;,    &#39;Br&#39;: &#39;Br/POTCAR&#39;,    &#39;Kr&#39;: &#39;Kr/POTCAR&#39;,    &#39;Rb&#39;: &#39;Rb_sv/POTCAR&#39;,    &#39;Sr&#39;: &#39;Sr_sv/POTCAR&#39;,    &#39;Y&#39;: &#39;Y_sv/POTCAR&#39;,    &#39;Zr&#39;: &#39;Zr_sv/POTCAR&#39;,    &#39;Nb&#39;: &#39;Nb_pv/POTCAR&#39;,    &#39;Mo&#39;: &#39;Mo_pv/POTCAR&#39;,    &#39;Tc&#39;: &#39;Tc_pv/POTCAR&#39;,    &#39;Ru&#39;: &#39;Ru_pv/POTCAR&#39;,    &#39;Rh&#39;: &#39;Rh_pv/POTCAR&#39;,    &#39;Pd&#39;: &#39;Pd/POTCAR&#39;,    &#39;Ag&#39;: &#39;Ag/POTCAR&#39;,    &#39;Cd&#39;: &#39;Cd/POTCAR&#39;,    &#39;In&#39;: &#39;In_d/POTCAR&#39;,    &#39;Sn&#39;: &#39;Sn_d/POTCAR&#39;,    &#39;Sb&#39;: &#39;Sb/POTCAR&#39;,    &#39;Te&#39;: &#39;Te/POTCAR&#39;,    &#39;I&#39;: &#39;I/POTCAR&#39;,    &#39;Xe&#39;: &#39;Xe/POTCAR&#39;,    &#39;Cs&#39;: &#39;Cs_sv/POTCAR&#39;,    &#39;Ba&#39;: &#39;Ba_sv/POTCAR&#39;,    &#39;La&#39;: &#39;La/POTCAR&#39;,    &#39;Ce&#39;: &#39;Ce/POTCAR&#39;,    &#39;Pr&#39;: &#39;Pr_3/POTCAR&#39;,    &#39;Nd&#39;: &#39;Nd_3/POTCAR&#39;,    &#39;Pm&#39;: &#39;Pm_3/POTCAR&#39;,    &#39;Sm&#39;: &#39;Sm_3/POTCAR&#39;,    &#39;Eu&#39;: &#39;Eu/POTCAR&#39;,    &#39;Gd&#39;: &#39;Gd/POTCAR&#39;,    &#39;Tb&#39;: &#39;Tb_3/POTCAR&#39;,    &#39;Dy&#39;: &#39;Dy_3/POTCAR&#39;,    &#39;Ho&#39;: &#39;Ho_3/POTCAR&#39;,    &#39;Er&#39;: &#39;Er_3/POTCAR&#39;,    &#39;Tm&#39;: &#39;Tm_3/POTCAR&#39;,    &#39;Yb&#39;: &#39;Yb_2/POTCAR&#39;,    &#39;Lu&#39;: &#39;Lu_3/POTCAR&#39;,    &#39;Hf&#39;: &#39;Hf_pv/POTCAR&#39;,    &#39;Ta&#39;: &#39;Ta_pv/POTCAR&#39;,    &#39;W&#39;: &#39;W_pv/POTCAR&#39;,    &#39;Re&#39;: &#39;Re_pv/POTCAR&#39;,    &#39;Os&#39;: &#39;Os_pv/POTCAR&#39;,    &#39;Ir&#39;: &#39;Ir/POTCAR&#39;,    &#39;Pt&#39;: &#39;Pt/POTCAR&#39;,    &#39;Au&#39;: &#39;Au/POTCAR&#39;,    &#39;Hg&#39;: &#39;Hg/POTCAR&#39;,    &#39;Tl&#39;: &#39;Tl_d/POTCAR&#39;,    &#39;Pb&#39;: &#39;Pb_d/POTCAR&#39;,    &#39;Bi&#39;: &#39;Bi/POTCAR&#39;,    &#39;Th&#39;: &#39;Th/POTCAR&#39;,    &#39;Pa&#39;: &#39;Pa/POTCAR&#39;,    &#39;U&#39;: &#39;U/POTCAR&#39;,    &#39;Np&#39;: &#39;Np/POTCAR&#39;,    &#39;Pu&#39;: &#39;Pu/POTCAR&#39;,# 添加其他元素和对应的赝势路径&#125;for element, relative_path in pseudo_paths.items():    full_path = f&quot;&#123;base_pseudo_path&#125;&#123;relative_path&#125;&quot;    pseudo_paths[element] = full_path# 读取 POSCAR 文件def read_poscar(poscar_path):    with open(poscar_path, &#39;r&#39;) as f:        lines = f.readlines()        elements = lines[5].split()        return elements# 创建大的 POTCAR 文件def create_big_potcar(elements, pseudo_paths, output_path):    with open(output_path, &#39;w&#39;) as f_out:        for element in elements:            if element in pseudo_paths:                pseudo_path = pseudo_paths[element]                with open(pseudo_path, &#39;r&#39;) as f_pseudo:                    f_out.write(f_pseudo.read())            else:                print(f&quot;赝势文件不存在或未定义：&#123;element&#125;&quot;)if __name__ == &quot;__main__&quot;:    poscar_path = &quot;POSCAR&quot;  # 输入文件名    output_potcar_path = &quot;POTCAR&quot;  # 输出的大 POTCAR 文件名    elements = read_poscar(poscar_path)    create_big_potcar(elements, pseudo_paths, output_potcar_path)    print(&quot;大的 POTCAR 文件已创建&quot;)</code></pre><h1 id="4-提任务"><a href="#4-提任务" class="headerlink" title="4. 提任务"></a>4. 提任务</h1><h3 id="提到不同节点"><a href="#提到不同节点" class="headerlink" title="提到不同节点"></a>提到不同节点</h3><pre><code class="shell">for folder in */; do    if [ -d &quot;$folder&quot; ]; then        cp pot.py &quot;$folder&quot;        cd &quot;$folder&quot;        python pot.py &amp;        sleep 10        cd ..    fidone</code></pre><pre><code class="shell">for folder in */;do cp KPOINTS make-pot.py &quot;$folder&quot;;cd &quot;$folder&quot;;python make-pot.py;sleep  5;cd ..;done</code></pre><h3 id="把任务提到一个节点上"><a href="#把任务提到一个节点上" class="headerlink" title="把任务提到一个节点上"></a>把任务提到一个节点上</h3><pre><code class="shell">for folder in */; do    # 进入当前文件夹    cd &quot;$folder&quot;    # 运行任务    mpirun -n 48 /home/software/apps/vasp/intelmpi/5.4.4/bin/vasp &gt; vasp.log 2&gt;&amp;1 &amp;    # 等待当前任务完成    wait $!    echo &quot;Task completed in $folder&quot;    # 返回上级目录    cd ..done</code></pre><h1 id="5-批量转换-cif"><a href="#5-批量转换-cif" class="headerlink" title="5. 批量转换.cif"></a>5. 批量转换.cif</h1><pre><code class="python">from pymatgen.core import Structurefrom pymatgen.io.vasp.inputs import Poscarfor i in range(0,361,10):   cif_file = f&quot;&#123;i&#125;.cif&quot;   vasp_file = f&quot;&#123;i&#125;.vasp&quot;   structure = Structure.from_file(cif_file)   vasp_str = Poscar(structure)   vasp_str.write_file(vasp_file)</code></pre><h1 id="6-按照自己选定的轴旋转分子"><a href="#6-按照自己选定的轴旋转分子" class="headerlink" title="6. 按照自己选定的轴旋转分子"></a>6. 按照自己选定的轴旋转分子</h1><p>相当于是在ab平面内旋转，应该也可以做成在ac平面或者bc平面的旋转</p><p>依据的原理：</p><p><img src="/./../../%E5%9B%BE%E7%89%87/c01b1c44d8d60fee02cbac775926010.jpg" alt="c01b1c44d8d60fee02cbac775926010"></p><pre><code class="python">import math  import numpyfrom pymatgen.core import Structurefrom pymatgen.io.vasp.inputs import Poscardef transform_coordinates(x1, y1,x2,y2, angle):    x=x2-x1    y=y2-y1#    a=math.sqrt(x**2 + y**2)    # 计算x&#39;    x_prime = x*math.cos(angle) - y*math.sin(angle)     # 计算y&#39;      y_prime = x*math.sin(angle) + y*math.cos(angle)      aa = math.sqrt(x_prime**2 + y_prime**2)    return x_prime, y_primeposcar = Poscar.from_file(&#39;POSCAR&#39;)structure = poscar.structure#设置旋转点rotation_point = structure[0]x_coord,y_coord,_= rotation_point.coords#设置旋转角度/设置每步旋转的角度ten = math.pi/18for a in range(1,37):    m = ten#设置要旋转的原子的次序    for atom in structure[1:]:        x_coord_rotation, y_coord_rotation, _ = atom.coords        new_x,new_y= transform_coordinates(x_coord,y_coord,x_coord_rotation,y_coord_rotation,m)        new_x1 = new_x + x_coord        new_y1 = new_y + y_coord#注意下面有一个递增的效果，因为是直接修改的原来的坐标        atom.coords = [new_x1,new_y1,atom.coords[2]]    new_poscar = Poscar(structure)    new_poscar.write_file(f&quot;&#123;a&#125;.vasp&quot;)</code></pre><h1 id="7-绘制图像"><a href="#7-绘制图像" class="headerlink" title="7. 绘制图像"></a>7. 绘制图像</h1><h2 id="1-绘制并排图像"><a href="#1-绘制并排图像" class="headerlink" title="1. 绘制并排图像"></a>1. 绘制并排图像</h2><p>python  .py   file1 fil2  name</p><pre><code class="python">import argparseimport matplotlib.pyplot as plt# 创建命令行参数解析器parser = argparse.ArgumentParser(description=&#39;Plot data from two files.&#39;)# 添加命令行参数选项parser.add_argument(&#39;file1&#39;, help=&#39;First data file to plot&#39;)parser.add_argument(&#39;file2&#39;, help=&#39;Second data file to plot&#39;)parser.add_argument(&#39;title&#39;, help=&#39;Title for the plot&#39;)# 解析命令行参数args = parser.parse_args()# 读取第一个数据文件data1 = open(args.file1, &#39;r&#39;).readlines()x1 = [float(line.split()[0]) for line in data1]y1 = [float(line.split()[1]) for line in data1]# 读取第二个数据文件data2 = open(args.file2, &#39;r&#39;).readlines()x2 = [float(line.split()[0]) for line in data2]y2 = [float(line.split()[1]) for line in data2]# 创建两个子图fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))# 绘制第一个图ax1.plot(x1, y1, label=args.file1)ax1.set_title(&#39;Plot 1&#39;)ax1.set_xlabel(&#39;X-axis&#39;)ax1.set_ylabel(&#39;Y-axis&#39;)ax1.legend()# 绘制第二个图ax2.plot(x2, y2, label=args.file2)ax2.set_title(&#39;Plot 2&#39;)ax2.set_xlabel(&#39;X-axis&#39;)ax2.set_ylabel(&#39;Y-axis&#39;)ax2.legend()# 设置图的标题为传入的参数值plt.suptitle(args.title)# 调整子图之间的距离plt.tight_layout()plt.savefig(args.title + &#39;.png&#39;) # 显示图形plt.show()</code></pre><h2 id="2-计算不同百分比斜率"><a href="#2-计算不同百分比斜率" class="headerlink" title="2. 计算不同百分比斜率"></a>2. 计算不同百分比斜率</h2><pre><code class="python">import numpy as npfrom scipy import stats# 读取数据文件并存储数据data = np.loadtxt(&#39;newdata2.txt&#39;)# 定义不同的百分比值percentages = range(1,100)print(percentages)# 针对每个百分比执行线性拟合for percent_to_select in percentages:    print (percent_to_select,end=&#39; &#39;)    # 计算要选择的数据点数量    num_points_to_select = int(len(data) * percent_to_select / 100)    # 根据百分比选择数据点    selected_data = data[:num_points_to_select]    # 线性拟合并计算斜率    x = selected_data[:, 0]  # 假设第一列是 x 值    y = selected_data[:, 1]  # 假设第二列是 y 值    # 执行线性回归拟合    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)    # 输出斜率    print (slope)print()</code></pre><h2 id="3-根据log-lammps处理得到能量、势能的变化"><a href="#3-根据log-lammps处理得到能量、势能的变化" class="headerlink" title="3.根据log.lammps处理得到能量、势能的变化"></a>3.根据log.lammps处理得到能量、势能的变化</h2><pre><code class="python">import matplotlibmatplotlib.use(&#39;Agg&#39;)  # 使用非图形化的Agg后端import matplotlib.pyplot as pltimport numpy as np# 从.dat文件中读取数据data = np.genfromtxt(&#39;log.dat&#39;, delimiter=None, skip_header=1)x = data[:, 1]  # 第二列作为x轴y1 = data[:, 2]  # 第三列作为第一组y值y2 = data[:, 3]  # 第四列作为第二组y值y3 = data[:, 4]  # 第五列作为第三组y值# 创建三个子图fig, axes = plt.subplots(1, 3, figsize=(15, 5))# 绘制第一个图axes[0].plot(x, y1, label=&#39;Column 3&#39;)axes[0].set_title(&#39;Plot 1&#39;)axes[0].set_xlabel(&#39;X-axis&#39;)axes[0].set_ylabel(&#39;Y-axis&#39;)axes[0].legend()# 绘制第二个图axes[1].plot(x, y2, label=&#39;Column 4&#39;)axes[1].set_title(&#39;Plot 2&#39;)axes[1].set_xlabel(&#39;X-axis&#39;)axes[1].set_ylabel(&#39;Y-axis&#39;)axes[1].legend()# 绘制第三个图axes[2].plot(x, y3, label=&#39;Column 5&#39;)axes[2].set_title(&#39;Plot 3&#39;)axes[2].set_xlabel(&#39;X-axis&#39;)axes[2].set_ylabel(&#39;Y-axis&#39;)axes[2].legend()# 保存为all.pngplt.tight_layout()plt.savefig(&#39;all.png&#39;)</code></pre><h1 id="8-根据需要直接求出斜率"><a href="#8-根据需要直接求出斜率" class="headerlink" title="8.根据需要直接求出斜率"></a>8.根据需要直接求出斜率</h1><p>python 文件名 百分比</p><pre><code class="python">import argparseimport numpy as npfrom scipy import stats# 创建命令行参数解析器parser = argparse.ArgumentParser(description=&#39;Perform linear regression on data.&#39;)# 添加命令行参数选项parser.add_argument(&#39;file&#39;, type=str, help=&#39;Data file name&#39;, default=&#39;a.dat&#39;)parser.add_argument(&#39;percent&#39;, type=float, help=&#39;Percentage of data to use for regression&#39;, default=80)# 解析命令行参数args = parser.parse_args()# 1. 读取数据文件并存储数据data = np.loadtxt(args.file)# 2. 选择指定百分比的数据点percent_to_select = args.percentnum_points_to_select = int(len(data) * percent_to_select / 100)selected_data = data[:num_points_to_select]# 3. 线性拟合并计算斜率x = selected_data[:, 0]  # 假设第一列是 x 值y = selected_data[:, 1]  # 假设第二列是 y 值# 执行线性回归拟合slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)# 输出斜率print(f&quot;斜率: &#123;slope&#125;&quot;)</code></pre><h2 id="2-同一条轨迹的不同百分比"><a href="#2-同一条轨迹的不同百分比" class="headerlink" title="2. 同一条轨迹的不同百分比"></a>2. 同一条轨迹的不同百分比</h2><pre><code class="python">import numpy as npfrom scipy import statsimport math # 读取数据文件并存储数据data = np.loadtxt(&#39;msd.dat&#39;)# 定义不同的百分比值percentages = range(1,101)print(percentages)# 针对每个百分比执行线性拟合for percent_to_select in percentages:    print (percent_to_select,end=&#39; &#39;)    # 计算要选择的数据点数量    num_points_to_select = int(len(data) * percent_to_select / 100)    # 根据百分比选择数据点    selected_data = data[:num_points_to_select]    # 线性拟合并计算斜率    x = selected_data[:, 0]  # 假设第一列是 x 值    y = selected_data[:, 4]  # 假设第二列是 y 值    # 执行线性回归拟合    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)    # 输出斜率    print (slope,end=&#39; &#39;)    D= slope*1000/60000    print(D,end=&#39; &#39;)    logD = math.log10(D)    print(logD)print()</code></pre><h1 id="9-把一个不带对称性的cif写成标准的带空间群和wycoff-posion的cif"><a href="#9-把一个不带对称性的cif写成标准的带空间群和wycoff-posion的cif" class="headerlink" title="9. 把一个不带对称性的cif写成标准的带空间群和wycoff posion的cif"></a>9. 把一个不带对称性的cif写成标准的带空间群和wycoff posion的cif</h1><p>可以自己手动改写，找到空间群后手动改写</p><p><a href="https://www.cryst.ehu.es/cgi-bin/cryst/programs/nph-wp-list?gnum=193">Wyckoff Positions of Space Groups (ehu.es)</a></p><p>这个网站找对称性，注意小数点后输入至少5位</p><p>可能存在一个wycoff positon有两个点占用的情况，需要一个一个手动来做</p><h1 id="10-处理POSCAR"><a href="#10-处理POSCAR" class="headerlink" title="10.处理POSCAR"></a>10.处理POSCAR</h1><pre><code class="python">&quot;&quot;&quot;输入可以有多种形式,输出目前只有writ_poscar可以&quot;&quot;&quot;import numpy as npdef read_poscar(filename):    with open(filename,&#39;r&#39;) as file:        lines =file.readlines()    title =lines[0].strip()    scale =float(lines[1].strip())    #for i in range(2,5):    #    lattice_list.append(list(map(float,line[i].split())))    #lattice_array=np.array(lattice_list)    lattice_vectors = np.array([list(map(float,lines[i].split())) for i in range(2,5)])    element_type=lines[5].split()    atomnumbers=np.array(list(map(int,lines[6].split())))    atompositions=np.array([list(map(float,lines[i].split())) for i in range(8,8+sum(atomnumbers))])    cor_type = lines[7].strip()    if cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions[atompositions &lt; 0]+=1        atompositions[atompositions &gt;=1]-=1    elif cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    return(title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions)def write_poscar(filename,title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions):     with open(filename,&#39;w&#39;) as file:        file.write(f&quot;&#123;title&#125;\n&quot;)        file.write(f&quot;&#123;scale&#125;\n&quot;)        for vec in lattice_vectors:            file.write(f&quot;&#123;vec[0]:.8f&#125;  &#123;vec[1]:.8f&#125;  &#123;vec[2]:.8f&#125;\n&quot;)        #&quot;  &quot;.join用于将可迭代对象的元素以空格连接起来        file.write(&quot;  &quot;.join(element_type)+ &quot;\n&quot;)        file.write(&quot;  &quot;.join(map(str,atomnumbers))+ &quot;\n&quot;)        file.write(f&quot;&#123;cor_type&#125;\n&quot;)        for pos in atompositions:            file.write(f&quot;&#123;pos[0]:.8f&#125;  &#123;pos[1]:.8f&#125;  &#123;pos[2]:.8f&#125;\n&quot;)#输出不同类型原子的坐标def atom_pos(atomnumbers,atompositions):    atom_pos = np.split(atompositions,np.cumsum(atomnumbers)[:-1])    return atom_pos        &quot;&quot;&quot;功能区&quot;&quot;&quot;#计算体积def cal_volume(lattice_vectors):    volume = np.abs(np.dot(lattice_vectors[0],np.cross(lattice_vectors[1],lattice_vectors[2])))    return volume#转换分数坐标为笛卡尔坐标def change_dir_to_car(filename):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= read_poscar(filename)    if cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions = np.dot(atompositions,lattice_vectors)        cor_type = &quot;Cartesian&quot;    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#笛卡尔坐标转换为分数坐标def change_car_to_dir(filename):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions = read_poscar(filename)    if cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        atompositions = np.dot(atompositions,np.linalg.inv(lattice_vectors))        cor_type = &quot;Direct&quot;        atompositions[atompositions &lt; 0]+=1        atompositions[atompositions &gt;=1]-=1    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        pass    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#晶格向量的变换def lattice_change(filename,cell_type):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= change_car_to_dir(filename)    if cell_type == &#39;211&#39;:        pass    elif cell_type == &quot;121&quot;:        middle_value = lattice_vectors[0].copy()        lattice_vectors[0] = lattice_vectors[1]        lattice_vectors[1] = lattice_vectors[2]        lattice_vectors[2] = middle_value        middle_value2 = atompositions[:,0].copy()        atompositions[:,0] = atompositions[:,1]        atompositions[:,1] = atompositions[:,2]        atompositions[:,2] = middle_value2    elif cell_type == &quot;112&quot;:        middle_value = lattice_vectors[0].copy()        lattice_vectors[0] = lattice_vectors[2]        lattice_vectors[2] = lattice_vectors[1]        lattice_vectors[1] = middle_value        middle_value2 = atompositions[:,0].copy        atompositions[:,0] = atompositions[:,2]        atompositions[:,2] = atompositions[:,1]        atompositions[:,1] = middle_value2    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#将211晶胞分解为两个111晶胞def split_211(input_file,cell_type,output_file1,output_file2):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= lattice_change(input_file,cell_type)    atom_type_pos = atom_pos(atomnumbers,atompositions)    total_atoms1 = []    total_atoms_number1 = []    total_atoms2 = []    total_atoms_number2 = []    if cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        for sub_atom_type_pos in atom_type_pos:            print(sub_atom_type_pos)            coord_1=sub_atom_type_pos[sub_atom_type_pos[:,0]&lt;=0.48].copy()            total_atoms1.append(coord_1)            total_atoms_number1.append(len(coord_1))                    coord_2=sub_atom_type_pos[sub_atom_type_pos[:,0]&gt;0.52].copy()            total_atoms2.append(coord_2)            total_atoms_number2.append(len(coord_2))    # elif cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:    #     for sub_atom_type_pos in atom_type_pos:    #         sub_atom_type_pos=np.dot(sub_atom_type_pos,np.linalg.inv(lattice_vectors))    #         sub_atom_type_pos[sub_atom_type_pos &lt; 0]+=1    #         sub_atom_type_pos[sub_atom_type_pos &gt;=1]-=1    #         coord_1=sub_atom_type_pos[sub_atom_type_pos[:,0]&lt;=0.5].copy()    #         total_atoms1.append(coord_1)    #         total_atoms_number1.append(len(coord_1))            #         coord_2=sub_atom_type_pos[sub_atom_type_pos[:,0]&gt;0.5].copy()    #         total_atoms2.append(coord_2)    #         total_atoms_number2.append(len(coord_2))    #     cor_type = &quot;Direct&quot;    total_atoms1=np.vstack(total_atoms1)    total_atoms2=np.vstack(total_atoms2)    total_atoms1=np.dot(total_atoms1,lattice_vectors)    total_atoms2=np.dot(total_atoms2,lattice_vectors)    lattice_vectors[0]/=2    cor_type =&quot;Cartesian&quot;    # print(total_atoms1)    # print(total_atoms_number1)    write_poscar(output_file1,title,scale,lattice_vectors,element_type,total_atoms_number1,cor_type,total_atoms1)    write_poscar(output_file2,title,scale,lattice_vectors,element_type,total_atoms_number2,cor_type,total_atoms2)#POSCAR坐标的修改def coords_change(axis,value,cor_type,atompositions):    if cor_type.lower() == &quot;Cartesian&quot; or cor_type.lower() == &quot;c&quot;:        print(&quot;use change_car_to_dir to input&quot;)    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        if axis == &#39;x&#39;:           atompositions[:,0] += value        elif axis == &#39;y&#39;:           atompositions[:,1] += value        elif axis == &#39;z&#39;:           atompositions[:,2] += value        else:            raise ValueError(f&quot;invalid axis&quot;)    else:        raise ValueError(f&quot;invalid cor_type&quot;)    return atompositions</code></pre><h1 id="11-处理XDATCAR"><a href="#11-处理XDATCAR" class="headerlink" title="11. 处理XDATCAR"></a>11. 处理XDATCAR</h1><pre><code class="python">#只适用于NVT模拟import numpy as npdef read_xdatcar(filename):    with open (filename, &#39;r&#39;) as file:        lines = file.readlines()        title=lines[0].strip()        scale=float(lines[1].strip())        lattice_vectors=np.array([list(map(float,lines[i].split())) for i in range(2,5)])        element_type=lines[5].split()        atom_numbers=np.array(list(map(int,lines[6].split())))        total_atom=np.sum(atom_numbers)        cor_type = lines[7].split()[0]        if cor_type.lower()==&quot;direct&quot; or &quot;d&quot;:            step_positions =[]            for i in range(7,len(lines),total_atom+1):                atompositions=np.array([list(map(float,lines[j].split())) for j in range(i+1,i+1+total_atom)])                step_positions.append(atompositions)        else:            raise ValueError(&quot;unknown coordinate type!&quot;)        #序号是从0开始，第一个序号对应的结构是1        step_positions=np.array(step_positions)        return title,scale,lattice_vectors,element_type,atom_numbers,cor_type,step_positions def unwrap_xdatcar(filename):     title,scale,lattice_vectors,element_type,atom_numbers,cor_type,step_positions=read_xdatcar(filename)    total_atom=np.sum(atom_numbers)    all_step=len(step_positions)    for i in range(1,all_step):        for j in range(total_atom):            if step_positions[i][j][0]-step_positions[i-1][j][0]&gt;=0.5:                for k in range(i,all_step):                    step_positions[k][j][0]-=1             elif step_positions[i][j][0]-step_positions[i-1][j][0]&lt;-0.5:                for k in range(i,all_step):                    step_positions[k][j][0]+=1            if step_positions[i][j][1]-step_positions[i-1][j][1]&gt;=0.5:                for k in range(i,all_step):                    step_positions[k][j][1]-=1            elif step_positions[i][j][1]-step_positions[i-1][j][1]&lt;-0.5:                for k in range(i,all_step):                    step_positions[k][j][1]+=1            if step_positions[i][j][2]-step_positions[i-1][j][2]&gt;=0.5:                for k in range(i,all_step):                    step_positions[k][j][2]-=1            elif step_positions[i][j][2]-step_positions[i-1][j][2]&lt;-0.5:                for k in range(i,all_step):                    step_positions[k][j][2]+=1    return title,scale,lattice_vectors,element_type,atom_numbers,cor_type,step_positionsdef write_xdatcar(filename,title,scale,lattice_vectors,element_type,atom_numbers,cor_type,step_positions):    with open (filename,&quot;w&quot;) as file:        file.write(f&quot;&#123;title&#125;\n&quot;)        file.write(f&quot;&#123;scale&#125;\n&quot;)        for vec in lattice_vectors:            file.write(f&quot;&#123;vec[0]:.8f&#125;  &#123;vec[1]:.8f&#125;  &#123;vec[2]:.8f&#125;\n&quot;)        file.write(&quot;  &quot;.join(element_type)+ &quot;\n&quot;)        file.write(&quot;  &quot;.join(map(str,atom_numbers))+ &quot;\n&quot;)        for i in range(len(step_positions)):            file.write(f&quot;Direct configuration= &#123;i+1&#125;\n&quot;)        #这里的等号必须紧挨着tion,不然会报错            for pos in step_positions[i]:                file.write(f&quot;&#123;pos[0]:.8f&#125;  &#123;pos[1]:.8f&#125;  &#123;pos[2]:.8f&#125;\n&quot;)&quot;&quot;&quot;功能区&quot;&quot;&quot;#输出某一帧作为POSCARdef write_step(step_number,filename_output,title,scale,lattice_vectors,element_type,atom_numbers,cor_type,step_positions):#x是帧数    step_number =step_number-1    with open (filename_output,&quot;w&quot;) as file:        file.write(f&quot;&#123;title&#125;\n&quot;)        file.write(f&quot;&#123;scale&#125;\n&quot;)        for vec in lattice_vectors:            file.write(f&quot;&#123;vec[0]:.8f&#125;  &#123;vec[1]:.8f&#125;  &#123;vec[2]:.8f&#125;\n&quot;)        file.write(&quot;  &quot;.join(element_type)+ &quot;\n&quot;)        file.write(&quot;  &quot;.join(map(str,atom_numbers))+ &quot;\n&quot;)        file.write(f&quot;&#123;cor_type&#125;\n&quot;)        for pos in step_positions[step_number]:            file.write(f&quot;&#123;pos[0]:.8f&#125;  &#123;pos[1]:.8f&#125;  &#123;pos[2]:.8f&#125;\n&quot;)# 求x原子每步移动的距离def diff_atom_positio(step_positions_unwrapped,atom_number):#unwrapped step positions来源于unwrap_xdatcar#x是第几个原子    atom_number=atom_number-1    all_step=len(step_positions_unwrapped)    diff_vector=[]    for i in range(all_step):        if i &lt;= all_step-2:            print(i)            diff_vector.append(step_positions_unwrapped[i+1][atom_number]-step_positions_unwrapped[i][atom_number])    diff_vector=np.array(diff_vector)    diff_distance=np.sqrt(np.sum(diff_vector**2,axis=1))    return(diff_distance,diff_vector)    </code></pre><h1 id="12-计算距离矩阵"><a href="#12-计算距离矩阵" class="headerlink" title="12.计算距离矩阵"></a>12.计算距离矩阵</h1><pre><code class="python">from pymatgen.core import Structureimport numpy as npimport oslowest_distance =[]for i in range(0,70000):    print(i)    filename=f&quot;&#123;i&#125;.vasp&quot;    if os.path.exists(filename):        structure = Structure.from_file(filename)        na_indices = [i for i, site in enumerate(structure) if site.specie.symbol == &quot;Na&quot;]# 初始化距离矩阵        na_count = len(na_indices)        distance_matrix = np.zeros((na_count, na_count))# 计算钠原子之间的距离        for i in range(na_count):            for j in range(i + 1, na_count):                distance = structure.get_distance(na_indices[i], na_indices[j])                distance_matrix[i, j] = distance                distance_matrix[j, i] = distance        np.fill_diagonal(distance_matrix, 10)        print(np.min(distance_matrix))        lowest_distance.append(np.min(distance_matrix))    else:        continue     lowest_distance =np.array(lowest_distance)np.savetxt(&#39;distance.txt&#39;,lowest_distance)</code></pre><h1 id="13-使用pymatgen"><a href="#13-使用pymatgen" class="headerlink" title="13.使用pymatgen"></a>13.使用pymatgen</h1><h2 id="1-查看元素u值"><a href="#1-查看元素u值" class="headerlink" title="1. 查看元素u值"></a>1. 查看元素u值</h2><pre><code class="python">from mp_api.client import MPResterfrom pymatgen.core import compositionfrom pymatgen.io.vasp.outputs import Vasprunfrom pymatgen.analysis.phase_diagram import PhaseDiagram, PDPlotter, PDEntryfrom pymatgen.entries.compatibility import MaterialsProject2020Compatibilityfrom pymatgen.core import Elementfrom pymatgen.io.vasp import Poscarposcar = Poscar.from_file(&quot;./POSCAR&quot;)structure = poscar.structurempr=MPRester(&quot;5BhZJctjosrFpkuE6qErpu85dT4gw4VZ&quot;)#vasprun = Vasprun(&#39;vasprun.xml&#39;, parse_potcar_file=False)#elements = list(vasprun.final_structure.composition.elements)elements = list(structure.composition.elements)element_symbols = [element.symbol for element in elements]element_symbols = [element_symbols[1],&quot;O&quot;]mp_entries = mpr.get_entries_in_chemsys(elements=element_symbols, additional_criteria=&#123;&quot;thermo_types&quot;: [&quot;GGA_GGA+U&quot;]&#125;)filtered_entries = []for entry in mp_entries:    # 获取当前条目的元素组成    entry_elements = list(entry.composition.elements)    entry_element_symbols = [element.symbol for element in entry_elements]        # 检查是否只包含指定的两种元素    if set(entry_element_symbols) == set(element_symbols):        filtered_entries.append(entry)entry1 = filtered_entries[-1]print(entry1.composition)parameters = entry1.parametersif &quot;hubbards&quot; in parameters:    hubbards = parameters[&quot;hubbards&quot;]    print(f&quot;Hubbard U values: &#123;hubbards&#125;&quot;)else:    print(&quot;This calculation does not use Hubbard U (likely GGA only).&quot;)</code></pre><h2 id="2-计算体积值"><a href="#2-计算体积值" class="headerlink" title="2.计算体积值"></a>2.计算体积值</h2><pre><code class="python">from pymatgen.core import Structurestructure = Structure.from_file(&quot;POSCAR&quot;)volume = structure.volumeprint(f&quot;&#123;volume&#125;&quot;)</code></pre><h2 id="3-修改POSCAR中元素顺序"><a href="#3-修改POSCAR中元素顺序" class="headerlink" title="3. 修改POSCAR中元素顺序"></a>3. 修改POSCAR中元素顺序</h2><pre><code class="python">import osfrom pymatgen.core import Structurefrom pymatgen.io.vasp import Poscardef change_position(poscar_path):    &quot;&quot;&quot;修改 POSCAR 文件中的元素顺序并保存修改后的文件&quot;&quot;&quot;    structure = Poscar.from_file(poscar_path).structure    # 重新排序元素    new_element_order = [&quot;Na&quot;, &quot;Y&quot;, &quot;Si&quot;, &quot;O&quot;]    structure = structure.get_sorted_structure(        key=lambda site: new_element_order.index(site.species.elements[0].symbol)    )    # 写入新的 POSCAR 文件    new_poscar_path = os.path.join(os.path.dirname(poscar_path), &quot;POSCAR_modified&quot;)    new_poscar = Poscar(structure)    new_poscar.write_file(new_poscar_path)    print(f&quot;Modified POSCAR saved as &#123;new_poscar_path&#125;&quot;)# 遍历当前目录的所有子目录，查找并修改 &quot;poscar&quot; 文件root_dir = os.getcwd()  # 当前目录for subdir, _, files in os.walk(root_dir):    for file in files:        if file.lower() == &quot;poscar&quot;:  # 兼容大小写            poscar_path = os.path.join(subdir, file)            print(f&quot;Processing &#123;poscar_path&#125; ...&quot;)            change_position(poscar_path)</code></pre><h2 id="4-扩胞"><a href="#4-扩胞" class="headerlink" title="4. 扩胞"></a>4. 扩胞</h2><pre><code class="python">from pymatgen.io.vasp import Poscarposcar = Poscar.from_file(&quot;POSCAR&quot;)supercell = poscar.structure * [3, 3, 3]poscar = Poscar(supercell)poscar.write_file(&quot;POSCAR333.vasp&quot;)</code></pre><h1 id="14-处理不同温度MSD得到离子电导率"><a href="#14-处理不同温度MSD得到离子电导率" class="headerlink" title="14. 处理不同温度MSD得到离子电导率"></a>14. 处理不同温度MSD得到离子电导率</h1><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import linregressimport matplotlibmatplotlib.use(&#39;Agg&#39;)def dealMSD(inputfile):    data = np.loadtxt(inputfile,skiprows = 1 )    data[:,0] = data[:,0] / 1000    n = data.shape[0]    start_idx = int(n*0.05)    end_idx = int(n*0.95)    time = data[start_idx:end_idx,0]    MSD = data[start_idx:end_idx,4]    slope, intercept, r_value, _, _ = linregress(time, MSD)    MSD_fit = slope * time + intercept        fig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)    ax1.plot(data[:, 0], data[:, 4],             linewidth=2,          # 更粗的线条            color=&#39;#2ecc71&#39;,      # 柔和的绿色            alpha=0.8)      ax1.plot(time, MSD_fit, label=f&#39;Fit: y = &#123;slope:.2f&#125;x + &#123;intercept:.2f&#125;&#39;, color=&#39;red&#39;)    ax1.set_xlabel(&#39;t (ps)&#39;)    ax1.set_ylabel(r&#39;MSD ($\AA^2$)&#39;)    ax1.legend()    ax1.grid(True)    output_filename = inputfile.replace(&#39;/&#39;,&#39;-&#39;) + &#39;.png&#39;    plt.savefig(output_filename, bbox_inches=&#39;tight&#39;)    plt.close()#    plt.show()    return slopediffD = []temp = [600, 800,1000]for i in temp:    inputfile = f&quot;&#123;i&#125;/msd/MSD.dat&quot;    try:        slope = dealMSD(inputfile)        diffuD = slope / 60000        diffD.append(diffuD)    except FileNotFoundError:        print(f&quot;文件 &#123;inputfile&#125; 不存在&quot;)        diffD.append(0)logdiffD = np.log10(diffD)temp_inv = np.array([1000 / i for i in temp]) slope_D, intercept_D, _, _, _ = linregress(temp_inv, logdiffD)temp_inv_fit = np.linspace(1, 3.3, 100)logDfit = slope_D * temp_inv_fit + intercept_D#logDfit = slope_D * temp_inv + intercept_Dfig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)ax1.plot(temp_inv_fit, logDfit, label=f&#39;Fit: y = &#123;slope_D:.2f&#125;x + &#123;intercept_D:.2f&#125;&#39;, color=&#39;red&#39;)ax1.scatter(temp_inv, logdiffD,color = &#39;blue&#39;,marker = &#39;o&#39;)ax1.set_xlabel(&#39;1000/T (1/K)&#39;)ax1.set_ylabel(r&#39;log10(D) (Å$^2$/ps)&#39;)ax1.set_xlim(0.9, 3.5)ax1.set_ylim(slope_D * 3.5 + intercept_D, slope_D * 0.9 + intercept_D+ 0.3)y_min = slope_D * 3.5 + intercept_Dy_max = slope_D * 0.9 + intercept_D + 0.3y_ticks = np.arange(np.floor(y_min / 0.5) * 0.5, np.ceil(y_max / 0.5) * 0.5, 0.5)ax1.set_yticks(y_ticks)ax1.legend()ax1.grid(True)# plt.savefig(&#39;arrnius.png&#39;,bbox_inches=&#39;tight&#39;)from pymatgen.core import Structurestructure = Structure.from_file(&quot;POSCAR&quot;)volume = structure.volumeatomsnumber = 30Eanumber = 0.198691478T = 300logD300 = slope_D * (1000 / T) + intercept_DD = 10 ** logD300sigma300 = 1.858 * atomsnumber / volume / 300 * (10**12) * DEa = -Eanumber * slope_Dprint(Ea, sigma300)text_str = f&#39;Ea = &#123;Ea:.2f&#125;  eV\nSigma300K = &#123;sigma300:.2f&#125; mS/cm&#39;ax1.text(0.95, 0.95, text_str, transform=ax1.transAxes,          fontsize=10, verticalalignment=&#39;top&#39;, horizontalalignment=&#39;right&#39;,         bbox=dict(facecolor=&#39;white&#39;, alpha=0.8, edgecolor=&#39;black&#39;))# 保存图像plt.savefig(&#39;arrhenius.png&#39;, bbox_inches=&#39;tight&#39;)  # 修正拼写plt.close()</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-确定空间群信息&quot;&gt;&lt;a href=&quot;#1-确定空间群信息&quot; class=&quot;headerlink&quot; title=&quot;1. 确定空间群信息&quot;&gt;&lt;/a&gt;1. 确定空间群信息&lt;/h1&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;from</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>分子动力学理解及可利用的计算</title>
    <link href="http://example.com/2025/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%90%86%E8%A7%A3%E5%8F%8A%E5%8F%AF%E5%88%A9%E7%94%A8%E7%9A%84%E8%AE%A1%E7%AE%97/"/>
    <id>http://example.com/2025/05/25/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%90%86%E8%A7%A3%E5%8F%8A%E5%8F%AF%E5%88%A9%E7%94%A8%E7%9A%84%E8%AE%A1%E7%AE%97/</id>
    <published>2025-05-25T01:23:40.000Z</published>
    <updated>2025-12-02T12:08:29.539Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-分子动力学本身的理论"><a href="#1-分子动力学本身的理论" class="headerlink" title="1.分子动力学本身的理论"></a>1.分子动力学本身的理论</h1><h2 id="1-关于温度波动与smass值"><a href="#1-关于温度波动与smass值" class="headerlink" title="1. 关于温度波动与smass值"></a>1. 关于温度波动与smass值</h2><blockquote><p><strong><a href="http://staff.ustc.edu.cn/~zqj/posts/NVT-MD/">http://staff.ustc.edu.cn/~zqj/posts/NVT-MD/</a></strong></p></blockquote><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230531161007404.png" alt="image-20230531161007404"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230710142226751.png" alt="image-20230710142226751"></p><p>ceder的关于smass为0的文章 ：</p><p><img src="/./../../../%E5%9B%BE%E7%89%87/86e5ae008edff6b274b7507f556f30f.png" alt="86e5ae008edff6b274b7507f556f30f"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/a29986e6075d43d8479b176c47a30ea.png" alt="a29986e6075d43d8479b176c47a30ea"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/0bee3b31f14b240bb64deebfbe3d9ab.png" alt="0bee3b31f14b240bb64deebfbe3d9ab"></p><h1 id="2-理论理解"><a href="#2-理论理解" class="headerlink" title="2. 理论理解"></a>2. 理论理解</h1><p>参考文献：</p><p> <a href="https://mp.weixin.qq.com/s?__biz=MzkyODI1MDkzMQ==&mid=2247483732&idx=1&sn=dcf54d4949081f3f373e83e94e41926c&chksm=c21aec08f56d651ea88b5bbc7b88dc871d0bea4a4c76a1b54229715ef318512810915d921b62&scene=21#wechat_redirect">第一性原理计算能量，我们忽略了什么？ (qq.com)</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzkyODI1MDkzMQ==&mid=2247483745&idx=1&sn=6d778b432d19423188077b703dd2dca1&chksm=c21aec3df56d652bc30408adaed234b6d1cc0bc7c23bfc816251ab5775cbbb999b6c8ab76cdb&scene=21#wechat_redirect">声子零点振动能（ZPE）和吉布斯自由能的理论推导及VASP计算实操（一） (qq.com)</a></p><h2 id="1-能量的区分"><a href="#1-能量的区分" class="headerlink" title="1.能量的区分"></a>1.能量的区分</h2><blockquote><p><a href="https://mp.weixin.qq.com/s/wg2cKE9KDRvCXM4f-1RrtA">https://mp.weixin.qq.com/s/wg2cKE9KDRvCXM4f-1RrtA</a></p></blockquote><ol><li>能量分为体系的内能U、焓H、吉布斯自由能G</li></ol><p>其中</p><p>$G&#x3D;U+PV-TS$</p><p>$H&#x3D;U+PV$</p><ol start="2"><li><p>内能等于</p><p>$U&#x3D;E_{electron}+ZPE$</p><blockquote><p>其中，E_electron是电子总能，ZPE是声子的零点振动能</p><p>&#x3D;&#x3D;vasp中，使用grep withoutentropy OUTCAR得到的能量就只是电子总能&#x3D;&#x3D;</p><p>因为第一性原理计算没有温度，所以不包括体系的动能项</p></blockquote><p>零点振动能对于较轻元素的体系（H He Li)的体系需要考虑，对于较重的元素，ZPE远远小于E_electron，通常不考虑</p></li><li><p>讨论吉布斯自由能</p><p>吉布斯自由能中有熵</p><p>$S&#x3D;S_k+S_{vib}+S_{elec}$</p></li></ol><p>其中S_k是构型熵，S_vib是振动熵（一般温度下贡献较小，第一性原理计算中未考虑），S_elec是电子熵</p><h2 id="2-声子振动能和振动熵、零点振动能"><a href="#2-声子振动能和振动熵、零点振动能" class="headerlink" title="2. 声子振动能和振动熵、零点振动能"></a>2. 声子振动能和振动熵、零点振动能</h2><blockquote><p><a href="https://mp.weixin.qq.com/s/Eq9Iug3tnkCy5eFXaBLAFQ">https://mp.weixin.qq.com/s/Eq9Iug3tnkCy5eFXaBLAFQ</a></p></blockquote><blockquote><p>声子作为准粒子来看，借助玻尔兹曼分布和准粒子的概念</p></blockquote><ol><li><p>一个材料体系，可以看作是由声子和电子组成，材料体系的总能是电子总能与声子总能之和</p><p>电子总能包括电子间相互作用能，原子核对电子的作用，范德华相互作用（均包括在grep without OUTCAR）中，和电子熵对总能的贡献（未包括）</p><p>第一性原理计算不能包括振动的能量</p><p>声子总能包括声子内能（振动能）和振动熵的贡献，零点振动能是声子总能的一部分（从下面的推导中可见）。第一性原理计算通常只能计算出声子的本征振动模（振动频率），而声子的ZPE和吉布斯自由能需要结合统计力学处理得到。</p></li><li><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529093355268.png" alt="image-20230529093355268"></p></li></ol><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529093446648.png" alt="image-20230529093446648"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529093557533.png" alt="image-20230529093557533"></p><h2 id="3-vasp计算声子振动能和振动熵"><a href="#3-vasp计算声子振动能和振动熵" class="headerlink" title="3.vasp计算声子振动能和振动熵"></a>3.vasp计算声子振动能和振动熵</h2><blockquote><p>vasp可以直接计算出材料的振动熵</p><p><a href="https://mp.weixin.qq.com/s/2uJtj6wEKAAqYwAMgvTi1A">https://mp.weixin.qq.com/s/2uJtj6wEKAAqYwAMgvTi1A</a></p></blockquote><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094334669.png" alt="image-20230529094334669"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094401003.png" alt="image-20230529094401003"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094511167.png" alt="image-20230529094511167"></p><h2 id="4-构型熵的计算"><a href="#4-构型熵的计算" class="headerlink" title="4. 构型熵的计算"></a>4. 构型熵的计算</h2><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094748544.png" alt="image-20230529094748544"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094820635.png" alt="image-20230529094820635"></p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529094857420.png" alt="image-20230529094857420"></p><h1 id="3-可以利用的计算"><a href="#3-可以利用的计算" class="headerlink" title="3.可以利用的计算"></a>3.可以利用的计算</h1><h2 id="1-振动熵"><a href="#1-振动熵" class="headerlink" title="1. 振动熵"></a>1. 振动熵</h2><blockquote><p>Influence of Lattice Dynamics on Na+ Transport in the Solid Electrolyte Na3PS4−xSex</p><p>How Chemical Composition Alone Can Predict Vibrational Free Energies and Entropies of Solids</p></blockquote><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529085044405.png" alt="image-20230529085044405"></p><h2 id="2-声子带心（phonon-band-center-标识平均振动频率"><a href="#2-声子带心（phonon-band-center-标识平均振动频率" class="headerlink" title="2.声子带心（phonon band center) (标识平均振动频率)"></a>2.声子带心（phonon band center) (标识平均振动频率)</h2><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529085819657.png" alt="image-20230529085819657"></p><h2 id="3-配分声子DOS和总声子DOS-速度自关联函数的的傅里叶变换"><a href="#3-配分声子DOS和总声子DOS-速度自关联函数的的傅里叶变换" class="headerlink" title="3. 配分声子DOS和总声子DOS -速度自关联函数的的傅里叶变换"></a>3. 配分声子DOS和总声子DOS -速度自关联函数的的傅里叶变换</h2><blockquote><p>Extreme phonon anharmonicity underpins superionic diffusion and ultralow thermal conductivity in argyrodite Ag8SnSe6</p></blockquote><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20230529090207571.png" alt="image-20230529090207571"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-分子动力学本身的理论&quot;&gt;&lt;a href=&quot;#1-分子动力学本身的理论&quot; class=&quot;headerlink&quot; title=&quot;1.分子动力学本身的理论&quot;&gt;&lt;/a&gt;1.分子动力学本身的理论&lt;/h1&gt;&lt;h2 id=&quot;1-关于温度波动与sma</summary>
      
    
    
    
    <category term="分子动力学" scheme="http://example.com/categories/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    
    <category term="分析方法" scheme="http://example.com/categories/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>关于毕业论文</title>
    <link href="http://example.com/2025/04/16/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/%E5%85%B3%E4%BA%8E%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/"/>
    <id>http://example.com/2025/04/16/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/%E5%85%B3%E4%BA%8E%E6%AF%95%E4%B8%9A%E8%AE%BA%E6%96%87/</id>
    <published>2025-04-16T02:29:27.000Z</published>
    <updated>2025-04-16T02:31:18.847Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="7c58f0a92fde60daf05aba79a36e05f0c00f433dd94983028d4c854f52c970dd">bda6ad0d5f606e7055c23f770369041ffa354e2b32a3834b155272c56f6444661ab7718603343a34c88aae20fc7454471ea25f2bf446205a6e0c3c512e3382d18684d1f39ffc2160101eedf9a9c1921c79e3b7f060e42f52505d7d4e245ac3e6440f5a6ac673744685875d76ad8b4041fb07f439ebb53b4250a386e8095159a4d3eaa727dce331e57c44488365c30ba2e7a3705f8e9e1fb2ccbacec4142ea887959eb02cce5283888538a79bd2d02848f1a740d1534328195c68896a1fd2a8959218e2098feb26048954fdacf02fcbcb4d74c15bc8c8e4cf5638dea23fac0f9d2e6be258dd267d6bcd526169157aa91c77b104fbf43ffe17972d70440383707e20bcaafc79c8d98de715601d29c6ef639b50f20abb33617db6043cbb76c8ef3e90ce4863c9372f9f922c755754a68de862510510be655855ca307c709032f18f031a57decac413adbae060843b60d1b30aae3b443541f50def21ce5ff6abf6b4c27445dd01c92136912cb450c869c0b5</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="绘图与写作" scheme="http://example.com/categories/%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>vasp中的分子动力学系综</title>
    <link href="http://example.com/2025/04/14/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/vasp%E4%B8%AD%E7%9A%84%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%B3%BB%E7%BB%BC/"/>
    <id>http://example.com/2025/04/14/3-%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/vasp%E4%B8%AD%E7%9A%84%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6%E7%B3%BB%E7%BB%BC/</id>
    <published>2025-04-14T01:17:38.000Z</published>
    <updated>2025-06-17T07:42:07.002Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-关于NPT系综"><a href="#1-关于NPT系综" class="headerlink" title="1.关于NPT系综"></a>1.关于NPT系综</h1><p>NPT系综中需要设置压力PSTRESS，在OUTCAR中，搜索关键词pressure会出现三个值</p><p><img src="/./../../../%E5%9B%BE%E7%89%87/image-20250414092020654.png" alt="image-20250414092020654"></p><p>其中，<code>in kB</code>行是压力矩阵的元素，前三个是主对角元元素，后面三个是非对角元元素</p><p>Pullay stress 是设置的PSTRESS的值，也就是外部压力（pstress)</p><p>external pressure是in kB行三个主对角元素平均值和Pullay stress的差值，对应于压力的波动量，这是结构的压力。</p><p><strong>由于有动能压力，导致内部会有一个内应力，动能压力总是正的，内应力也就是负数，对应于使得体积膨胀</strong></p><p>kinetic pressure 动能也会有一个压力，total pressure是最终的总压力，检查NPT是否平衡，计算这个值随步数的波动情况就可以</p><p>当有温度时，kinetic pressure不为0 ，且温度越高，kinetic pressure越大，external pressrure的值倾向于保持为负的kinetic pressure，这样，保持total pressure &#x3D; pullay stress，也就会导致热膨胀的出现</p><p><strong>不同的计算精度（ENCUT和KPOINTS对同一个结构计算的external pressure不同，尤其是对于大体系来说ENCUT, 如果要计算NPT来得到热膨胀效应，就必须要用相同的参数，不然可能会出现没有热膨胀，反而在设置的温度下体积相比结构优化的0 K下降的现象</strong></p><p>提取pressure的值</p><pre><code class="sh">grep &#39;total pressure&#39; OUTCAR | awk &#39;&#123;print NR, $4&#125;&#39; &gt; pressure.dat</code></pre><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#39;Agg&#39;)import argparse parser = argparse.ArgumentParser(description=&#39;deal two column data&#39;)parser.add_argument(&#39;filename&#39;)parser.add_argument(&#39;xlabel&#39;)parser.add_argument(&#39;ylabel&#39;)args = parser.parse_args()data_txt = args.filenamex_label = args.xlabely_label = args.ylabel# print(&#39;=&#39;*40)# filename = input(&#39;请输入文件名：&#39;).strip()# x_label = input(&quot;请输入x轴标签（直接回车可跳过）: &quot;).strip()# y_label = input(&quot;请输入y轴标签（直接回车可跳过）: &quot;).strip()# print(&quot;=&quot;*40)# data_txt = filename# x_label = x_label# y_label = y_labeldata = np.loadtxt(data_txt)plt.rcParams[&#39;font.size&#39;] = 16plt.rcParams[&#39;axes.linewidth&#39;] = 2plt.rcParams[&#39;axes.titlesize&#39;] = 16plt.rcParams[&#39;axes.labelsize&#39;] = 20plt.rcParams[&#39;axes.labelweight&#39;] = &#39;bold&#39;plt.rcParams[&#39;xtick.labelsize&#39;] = 16plt.rcParams[&#39;ytick.labelsize&#39;] = 16plt.rcParams[&#39;legend.fontsize&#39;] = 16figure = plt.figure(figsize=(8,6),dpi = 300)grid = figure.add_gridspec(1,1)ax1 = figure.add_subplot(grid[0,0])x = data[:,0]y = data[:,1]ax1.plot(x,y,linewidth=1.8)plt.xlabel(x_label)plt.ylabel(y_label)plt.xticks(fontweight = &#39;bold&#39;)plt.yticks(fontweight = &#39;bold&#39;)plt.legend(prop = &#123;&#39;weight&#39; :&#39;bold&#39;&#125;)plt.tight_layout()plt.savefig(&#39;plot.png&#39;,dpi = 300)</code></pre><h1 id="2-关于NVE系综"><a href="#2-关于NVE系综" class="headerlink" title="2. 关于NVE系综"></a>2. 关于NVE系综</h1><p>NVE系综是最简单的系综，只根据牛顿定律进行动力学演化，没有热浴和控压器的影响。NVT就是在NVE的基础上加一个控温器。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-关于NPT系综&quot;&gt;&lt;a href=&quot;#1-关于NPT系综&quot; class=&quot;headerlink&quot; title=&quot;1.关于NPT系综&quot;&gt;&lt;/a&gt;1.关于NPT系综&lt;/h1&gt;&lt;p&gt;NPT系综中需要设置压力PSTRESS，在OUTCAR中，</summary>
      
    
    
    
    <category term="分子动力学" scheme="http://example.com/categories/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/"/>
    
    <category term="vasp模拟" scheme="http://example.com/categories/%E5%88%86%E5%AD%90%E5%8A%A8%E5%8A%9B%E5%AD%A6/vasp%E6%A8%A1%E6%8B%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>vscode快捷键</title>
    <link href="http://example.com/2025/04/09/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://example.com/2025/04/09/5-linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/vscode%E5%BF%AB%E6%8D%B7%E9%94%AE/</id>
    <published>2025-04-09T12:47:00.000Z</published>
    <updated>2025-04-09T12:49:28.919Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><ul><li>关闭文件 ctrl +w</li><li>打开终端 ctrl + &#96;</li><li>关闭终端 ctrl +&#96;</li><li>转换终端ctrl + pagedown&#x2F;pageup</li><li>新建终端 ctrl + shift  + &#96;</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;关闭文件 ctrl +w&lt;/li&gt;
&lt;li&gt;打开终端 ctrl + &amp;#96;&lt;/li&gt;
&lt;li&gt;关闭终端 ctrl +&amp;#96;&lt;/li&gt;
&lt;li&gt;转换终端ctrl + pagedown&amp;#x2F;pageup&lt;/li&gt;
&lt;li&gt;新建</summary>
      
    
    
    
    <category term="linux学习使用" scheme="http://example.com/categories/linux%E5%AD%A6%E4%B9%A0%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>python绘图图案规范</title>
    <link href="http://example.com/2025/04/07/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/python%E7%BB%98%E5%9B%BE%E5%9B%BE%E6%A1%88%E8%A7%84%E8%8C%83/"/>
    <id>http://example.com/2025/04/07/7-%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/python%E7%BB%98%E5%9B%BE%E5%9B%BE%E6%A1%88%E8%A7%84%E8%8C%83/</id>
    <published>2025-04-07T01:12:00.000Z</published>
    <updated>2025-09-16T02:33:16.224Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-一行一列图"><a href="#1-一行一列图" class="headerlink" title="1. 一行一列图"></a>1. 一行一列图</h1><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltimport matplotlib.font_manager as fmfont_path = &#39;/usr/share/fonts/truetype/msttcorefonts/Times_New_Roman.ttf&#39;fm.fontManager.addfont(font_path)plt.rcParams[&#39;font.family&#39;] = &#39;Times New Roman&#39;plt.rcParams[&#39;axes.linewidth&#39;] = 4fig = plt.figure(figsize=(8,8),dpi = 300)grid = fig.add_gridspec(1,1)ax = fig.add_subplot(grid[0,0])data1 = np.loadtxt(&#39;li_energy.dat&#39;)sort_indices = np.argsort(data1[:, 0])data1 = data1[sort_indices]data2 = np.loadtxt(&#39;na_energy.dat&#39;)sort_indices = np.argsort(data2[:, 0])data2 = data2[sort_indices]data3 = np.loadtxt(&#39;k_energy.dat&#39;)sort_indices = np.argsort(data3[:, 0])data3 = data3[sort_indices]x = np.sort(data2[:,0])y1 = data1[:,1]y2 = data2[:,1]y3 = data3[:,1]y1_lowest = y1.min()y1_minus = y1 - y1_lowesty1_per_atom = y1_minus/ 132 * 1000y1_per_atom_pad = np.pad(y1_per_atom,(0,x.shape[0] - len(y1_per_atom)),&#39;constant&#39;,constant_values = np.nan)y2_lowest = y2.min()y2_minus = y2 - y2_lowesty2_per_atom = y2_minus/ 132 * 1000y2_per_atom_pad = np.pad(y2_per_atom,(0,x.shape[0] - len(y2_per_atom)),&#39;constant&#39;,constant_values = np.nan)y3_lowest = y3.min()y3_minus = y3 - y3_lowesty3_per_atom = y3_minus/ 132 * 1000y3_per_atom_pad = np.pad(y3_per_atom,(0,x.shape[0] - len(y3_per_atom)),&#39;constant&#39;,constant_values = np.nan)x = np.arange(1,21,1)ax.plot(x,y1_per_atom_pad[0:20],label=&#39;Li&#39;)ax.scatter(x,y1_per_atom_pad[0:20])ax.plot(x,y2_per_atom_pad[0:20],label=&#39;Na&#39;)ax.scatter(x,y2_per_atom_pad[0:20])ax.plot(x,y3_per_atom_pad[0:20],label=&#39;K&#39;)ax.scatter(x,y3_per_atom_pad[0:20])ax.set_ylabel(&#39;Energy relative the lowest energy per atom (meV)&#39;, fontsize = 35,fontname=&#39;Times New Roman&#39;,fontweight=&quot;bold&quot;)plt.xticks(fontsize = 30,fontname = &quot;Times New Roman&quot;,fontweight = &quot;bold&quot;)plt.yticks(fontsize = 30,fontname = &quot;Times New Roman&quot;,fontweight = &quot;bold&quot;)plt.legend(prop=&#123;&#39;family&#39;: &#39;Times New Roman&#39;, &#39;weight&#39;: &#39;bold&#39;, &#39;size&#39;: 25&#125;)plt.gca().xaxis.set_major_locator(plt.MaxNLocator(4))plt.savefig(&#39;plot.png&#39;,dpi = 1000)plt.show()</code></pre><h1 id="2-一行两列图"><a href="#2-一行两列图" class="headerlink" title="2. 一行两列图"></a>2. 一行两列图</h1><pre><code class="python">import matplotlib.pyplot as pltplt.rcParams[&#39;font.family&#39;] = &#39;Times New Roman&#39;plt.rcParams[&#39;font.size&#39;] = 12plt.rcParams[&#39;axes.titlesize&#39;] = 16plt.rcParams[&#39;axes.labelsize&#39;] = 14plt.rcParams[&#39;xtick.labelsize&#39;] = 12plt.rcParams[&#39;ytick.labelsize&#39;] = 12plt.rcParams[&#39;legend.fontsize&#39;] = 12plt.rcParams[&#39;axes.linewidth&#39;] = 1.5fig, axs = plt.subplots(1, 2, figsize=(12, 5), dpi=300)for ax in axs:    ax.plot([1,2,3], [4,5,6], linewidth=1.8, marker=&#39;o&#39;, markersize=7)    ax.set_xlabel(&quot;X轴&quot;, fontsize=14)    ax.set_ylabel(&quot;Y轴&quot;, fontsize=14)    ax.legend([&quot;数据&quot;], frameon=True)plt.tight_layout()plt.savefig(&#39;figure.png&#39;,dpi=600)plt.savefig(&#39;figure.pdf&#39;,dpi=600)plt.show()</code></pre><h1 id="3-两行两列图"><a href="#3-两行两列图" class="headerlink" title="3. 两行两列图"></a>3. 两行两列图</h1><pre><code class="python">import matplotlib.pyplot as pltplt.rcParams[&#39;font.family&#39;] = &#39;Times New Roman&#39;plt.rcParams[&#39;font.size&#39;] = 12plt.rcParams[&#39;axes.titlesize&#39;] = 16plt.rcParams[&#39;axes.labelsize&#39;] = 14plt.rcParams[&#39;xtick.labelsize&#39;] = 12plt.rcParams[&#39;ytick.labelsize&#39;] = 12plt.rcParams[&#39;legend.fontsize&#39;] = 12plt.rcParams[&#39;axes.linewidth&#39;] = 1.5fig, axs = plt.subplots(2, 2, figsize=(10, 8), dpi=300)for ax in axs.flat:    ax.plot([1,2,3], [4,5,6], linewidth=1.8, marker=&#39;o&#39;, markersize=7)    ax.set_xlabel(&quot;X轴&quot;, fontsize=14)    ax.set_ylabel(&quot;Y轴&quot;, fontsize=14)    ax.legend([&quot;数据&quot;], frameon=True)plt.tight_layout()plt.show()</code></pre><h1 id="4-处理-1-2数据"><a href="#4-处理-1-2数据" class="headerlink" title="4. 处理 1*2数据"></a>4. 处理 1*2数据</h1><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltimport matplotlibmatplotlib.use(&#39;Agg&#39;)import argparse parser = argparse.ArgumentParser(description=&#39;deal two column data&#39;)parser.add_argument(&#39;filename&#39;)parser.add_argument(&#39;xlabel&#39;)parser.add_argument(&#39;ylabel&#39;)args = parser.parse_args()data_txt = args.filenamex_label = args.xlabely_label = args.ylabel# print(&#39;=&#39;*40)# filename = input(&#39;请输入文件名：&#39;).strip()# x_label = input(&quot;请输入x轴标签（直接回车可跳过）: &quot;).strip()# y_label = input(&quot;请输入y轴标签（直接回车可跳过）: &quot;).strip()# print(&quot;=&quot;*40)# data_txt = filename# x_label = x_label# y_label = y_labeldata = np.loadtxt(data_txt)plt.rcParams[&#39;font.size&#39;] = 16plt.rcParams[&#39;axes.linewidth&#39;] = 2plt.rcParams[&#39;axes.titlesize&#39;] = 16plt.rcParams[&#39;axes.labelsize&#39;] = 20plt.rcParams[&#39;axes.labelweight&#39;] = &#39;bold&#39;plt.rcParams[&#39;xtick.labelsize&#39;] = 16plt.rcParams[&#39;ytick.labelsize&#39;] = 16plt.rcParams[&#39;legend.fontsize&#39;] = 16figure = plt.figure(figsize=(8,6),dpi = 300)grid = figure.add_gridspec(1,1)ax1 = figure.add_subplot(grid[0,0])x = data[:,0]y = data[:,1]ax1.plot(x,y,linewidth=1.8)plt.xlabel(x_label)plt.ylabel(y_label)plt.xticks(fontweight = &#39;bold&#39;)plt.yticks(fontweight = &#39;bold&#39;)plt.legend(prop = &#123;&#39;weight&#39; :&#39;bold&#39;&#125;)plt.tight_layout()plt.savefig(&#39;plot.png&#39;,dpi = 300)</code></pre><h1 id="5-大成版"><a href="#5-大成版" class="headerlink" title="5. 大成版"></a>5. 大成版</h1><pre><code class="python">import numpy as npimport osimport matplotlib.pyplot as pltfrom matplotlib import cmimport matplotlib.font_manager as fmfont_path = &#39;/usr/share/fonts/truetype/msttcorefonts/Times_New_Roman.ttf&#39;font_path_bold = &#39;/usr/share/fonts/truetype/msttcorefonts/Times_New_Roman_Bold.ttf&#39;fm.fontManager.addfont(font_path)fm.fontManager.addfont(font_path_bold)plt.rcParams[&#39;font.family&#39;] = &#39;Times New Roman&#39;plt.rcParams[&#39;axes.linewidth&#39;] = 4font_bold_prop = fm.FontProperties(family=&#39;Times New Roman&#39;, weight=&#39;bold&#39;)plt.rcParams.update(&#123;    &#39;mathtext.fontset&#39;: &#39;custom&#39;,    &#39;mathtext.rm&#39;: &#39;Times New Roman&#39;,    &#39;mathtext.bf&#39;: &#39;Times New Roman:bold&#39;,    &#39;font.family&#39;: &#39;Times New Roman&#39;,    &#39;font.weight&#39;: &#39;bold&#39;,    &#39;axes.labelweight&#39;: &#39;bold&#39;, &#125;)data = np.loadtxt(&#39;&#39;)x = data[:,0]y_data = data[:,1]fig = plt.figure(figsize=(8, 8),dpi=1000)fig.subplots_adjust(left=0.2, right=0.8, top=0.8, bottom=0.2)grid = fig.add_gridspec(1,1)ax = fig.add_subplot(grid[0,0])ax.plot(x, y_data,color=color, linewidth=4)    # ax.legend(#     # loc=&#39;center&#39;,#     # bbox_to_anchor=(0.8, 0.17),#     framealpha=1,#     labelspacing=0.3,#     prop=&#123;&#39;family&#39;: &#39;Times New Roman&#39;, &#39;size&#39;:18,&#39;weight&#39;: &#39;bold&#39;&#125;# )ax.tick_params(        axis=&#39;x&#39;,        which=&#39;both&#39;,        top=False,        bottom = True,        labelbottom = True,        labeltop=False,          width=2    )ax.tick_params(        axis=&#39;y&#39;,        which=&#39;both&#39;,        right=False,        left = True,        labelright=False,        labelleft = True,        width=2    )ax.set_xlim(0,200)ax.set_ylim(0,None)ax.set_xlabel(&#39;Time (ps)&#39;,fontsize = 35,fontname=&#39;Times New Roman&#39;,fontweight=&quot;bold&quot;)ax.set_ylabel(r&#39;MSD ($\mathrm&#123;\AA&#125;^2$)&#39;,fontsize = 35,fontname=&#39;Times New Roman&#39;,fontweight=&quot;bold&quot;)plt.xticks(fontsize = 30,fontname = &quot;Times New Roman&quot;,fontweight = &quot;bold&quot;)plt.yticks(fontsize = 30,fontname = &quot;Times New Roman&quot;,fontweight = &quot;bold&quot;)# plt.legend(prop=&#123;&#39;family&#39;: &#39;Times New Roman&#39;, &#39;weight&#39;: &#39;bold&#39;, &#39;size&#39;: 20&#125;)plt.gca().xaxis.set_major_locator(plt.MaxNLocator(4))plt.gca().yaxis.set_major_locator(plt.MaxNLocator(5))#plt.gca().spines[&#39;top&#39;].set_linewidth(4)#plt.gca().spines[&#39;right&#39;].set_linewidth(4)#plt.gca().spines[&#39;bottom&#39;].set_linewidth(4)#plt.gca().spines[&#39;left&#39;].set_linewidth(4)plt.savefig(&#39;msd.png&#39;,dpi=1000)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-一行一列图&quot;&gt;&lt;a href=&quot;#1-一行一列图&quot; class=&quot;headerlink&quot; title=&quot;1. 一行一列图&quot;&gt;&lt;/a&gt;1. 一行一列图&lt;/h1&gt;&lt;pre&gt;&lt;code class=&quot;python&quot;&gt;import numpy</summary>
      
    
    
    
    <category term="绘图与写作" scheme="http://example.com/categories/%E7%BB%98%E5%9B%BE%E4%B8%8E%E5%86%99%E4%BD%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>不同的主动学习方法</title>
    <link href="http://example.com/2025/03/29/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%8D%E5%90%8C%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2025/03/29/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E4%B8%8D%E5%90%8C%E7%9A%84%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/</id>
    <published>2025-03-29T00:08:58.000Z</published>
    <updated>2025-12-02T12:09:45.310Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-随机位移的方法"><a href="#1-随机位移的方法" class="headerlink" title="1. 随机位移的方法"></a>1. 随机位移的方法</h1><p><strong>给整个结构中的原子引入随机位移，代替md过程，增加采样</strong></p><p>核心脚本</p><p><code>random_distance.py</code></p><p>需要的文件</p><p><code>sub-grade.sh  POSCAR inv2 </code></p><pre><code class="python">import numpy as npfrom sys import argvfrom ase.atoms import Atomsfrom ase.io import readimport osimport subprocessimport shutildef write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i, vir_i=None):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        if vir_i is not None:            vir_i = np.array(vir_i).reshape(-1)            file.write(f&quot;VIRIAL\n&quot;)            for i in vir_i:                file.write(f&#39;&#123;i:20.8f&#125;&#39;)        file.write(&quot;\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)        for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            file.write(f&#39;&#123;ele_i[j]:2&#125;&#39;)            # coo            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # force            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)    passdef read_from_ase_atoms(atoms: Atoms):    try:        ene = atoms.get_potential_energy()    except:        ene = 0.0    lat = atoms.get_cell()    pos = atoms.get_positions()    try:        foc = atoms.get_forces()    except:        foc = np.zeros_like(pos)    try:        sts = atoms.get_stress()        xx, yy, zz, yz, xz, xy = - sts * atoms.get_volume()        vir = np.array(            [[xx, xy, xz],             [xy, yy, yz],             [xz, yz, zz]]).reshape(-1)    except:        vir = np.zeros([3, 3])    ele = atoms.get_chemical_symbols()    return ene, lat, pos, foc, vir, eledef read_poscar(filename):    with open(filename,&#39;r&#39;) as file:        lines =file.readlines()    title =lines[0].strip()    scale =float(lines[1].strip())    #for i in range(2,5):    #    lattice_list.append(list(map(float,line[i].split())))    #lattice_array=np.array(lattice_list)    lattice_vectors = np.array([list(map(float,lines[i].split())) for i in range(2,5)])    element_type=lines[5].split()    atomnumbers=np.array(list(map(int,lines[6].split())))    atompositions=np.array([list(map(float,lines[i].split()[:3])) for i in range(8,8+sum(atomnumbers))])    cor_type = lines[7].strip()    if cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions[atompositions &lt; 0]+=1        atompositions[atompositions &gt;=1]-=1    elif cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    return(title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions)def write_poscar(filename,title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions):     with open(filename,&#39;w&#39;) as file:        file.write(f&quot;&#123;title&#125;\n&quot;)        file.write(f&quot;&#123;scale&#125;\n&quot;)        for vec in lattice_vectors:            file.write(f&quot;&#123;vec[0]:.8f&#125;  &#123;vec[1]:.8f&#125;  &#123;vec[2]:.8f&#125;\n&quot;)        #&quot;  &quot;.join用于将可迭代对象的元素以空格连接起来        file.write(&quot;  &quot;.join(element_type)+ &quot;\n&quot;)        file.write(&quot;  &quot;.join(map(str,atomnumbers))+ &quot;\n&quot;)        file.write(f&quot;&#123;cor_type&#125;\n&quot;)        for pos in atompositions:            file.write(f&quot;&#123;pos[0]:.8f&#125;  &#123;pos[1]:.8f&#125;  &#123;pos[2]:.8f&#125;\n&quot;)def change_dir_to_car(filename):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= read_poscar(filename)    if cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions = np.dot(atompositions,lattice_vectors)        cor_type = &quot;Cartesian&quot;    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositionsdef atom_pos(atomnumbers,atompositions):    atom_pos = np.split(atompositions,np.cumsum(atomnumbers)[:-1])    return atom_posparent_dir = os.path.dirname(os.getcwd())oos_dir = os.path.join(parent_dir, &quot;oos&quot;)os.makedirs(oos_dir, exist_ok=True)counter = 0counter2 = 0counter3 = 0exit_flag = Falsefor itera in range(1, 100):    if exit_flag:        break    mignitude1 = 0.0018*itera    mignitude2 = 0.0009*itera    mignitude3 = 0.0017*itera    mignitude4 = 0.0022*itera        title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions=change_dir_to_car(&quot;POSCAR&quot;)        atom_pos = atom_pos(atomnumbers,atompositions)    for i in range(1,10001):        counter3 +=1        if exit_flag:            break        counter +=1        rd1=np.random.randn(atomnumbers[0],3)*mignitude1        rd2=np.random.randn(atomnumbers[1],3)*mignitude1        rd3=np.random.randn(atomnumbers[2],3)*mignitude1        rd4=np.random.randn(atomnumbers[3],3)*mignitude1        atom_pos[0]+=rd1        atom_pos[1]+=rd2        atom_pos[2]+=rd3        atom_pos[3]+=rd4        atompositions = np.concatenate(atom_pos)        write_poscar(f&quot;&#123;counter&#125;.vasp&quot;,title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions)        datas = read(f&quot;&#123;counter&#125;.vasp&quot;, index=&quot;:&quot;)        ene, lat, pos, foc, vir, ele = read_from_ase_atoms(datas[0])        write2my(            f&quot;&#123;counter&#125;.xsf&quot;,            ene_i=ene,            lat_i=lat,            ele_i=ele,            coo_i=pos,            foc_i=foc,            vir_i=vir)        os.remove(f&quot;&#123;counter&#125;.vasp&quot;)        print(counter3)        if counter3 &gt;= 100:            subprocess.run([&quot;sh&quot;, &quot;sub-grade.sh&quot;], check=True)            with open(&quot;grade-log&quot;, &#39;r&#39;) as file:                lines = file.readlines()                        for line in lines:                if exit_flag:                    break                if &#39;grad:&#39; in line:                    parts = line.split()                    filename = parts[0]                    grade_value = float(parts[-1])                    if 3 &lt; grade_value &lt; 8:                        counter2 += 1                        if counter2 &gt;= 50:                            exit_flag = True                            break                        src = os.path.join(os.getcwd(), filename)                        dst = os.path.join(oos_dir, filename)                        shutil.move(src, dst)                    else:                        os.remove(filename)                    counter3 = 0            </code></pre><p><code>iter_scheduling_remote.sh</code> 中修改的内容</p><pre><code class="shell">echo &quot;Simulation ...&quot;# run local &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;mkdir random_vaspmv sub-grade.sh random_vaspmv Ap_inv* random_vaspmv inv2 random_vaspmv random_distance.py random_vaspmv POSCAR random_vaspcd random_vasppython random_distance.py</code></pre><h1 id="2-切割的方法"><a href="#2-切割的方法" class="headerlink" title="2. 切割的方法"></a>2. 切割的方法</h1><p>切割后的结构grade比较大，不太好用</p><pre><code class="python">&quot;&quot;&quot;输入可以有多种形式,输出目前只有writ_poscar可以&quot;&quot;&quot;import numpy as npimport osfrom ase.atoms import Atomsfrom ase.io import readimport subprocessimport shutildef write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i, vir_i=None):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        if vir_i is not None:            vir_i = np.array(vir_i).reshape(-1)            file.write(f&quot;VIRIAL\n&quot;)            for i in vir_i:                file.write(f&#39;&#123;i:20.8f&#125;&#39;)        file.write(&quot;\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)        for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            file.write(f&#39;&#123;ele_i[j]:2&#125;&#39;)            # coo            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # force            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)    passdef read_from_ase_atoms(atoms: Atoms):    try:        ene = atoms.get_potential_energy()    except:        ene = 0.0    lat = atoms.get_cell()    pos = atoms.get_positions()    try:        foc = atoms.get_forces()    except:        foc = np.zeros_like(pos)    try:        sts = atoms.get_stress()        xx, yy, zz, yz, xz, xy = - sts * atoms.get_volume()        vir = np.array(            [[xx, xy, xz],             [xy, yy, yz],             [xz, yz, zz]]).reshape(-1)    except:        vir = np.zeros([3, 3])    ele = atoms.get_chemical_symbols()    return ene, lat, pos, foc, vir, eledef read_poscar(filename):    with open(filename,&#39;r&#39;) as file:        lines =file.readlines()    title =lines[0].strip()    scale =float(lines[1].strip())    #for i in range(2,5):    #    lattice_list.append(list(map(float,line[i].split())))    #lattice_array=np.array(lattice_list)    lattice_vectors = np.array([list(map(float,lines[i].split())) for i in range(2,5)])    element_type=lines[5].split()    atomnumbers=np.array(list(map(int,lines[6].split())))    atompositions=np.array([list(map(float,lines[i].split()[:3])) for i in range(8,8+sum(atomnumbers))])    cor_type = lines[7].strip()    if cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions[atompositions &lt; 0]+=1        atompositions[atompositions &gt;=1]-=1    elif cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    return(title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions)def write_poscar(filename,title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions):     with open(filename,&#39;w&#39;) as file:        file.write(f&quot;&#123;title&#125;\n&quot;)        file.write(f&quot;&#123;scale&#125;\n&quot;)        for vec in lattice_vectors:            file.write(f&quot;&#123;vec[0]:.8f&#125;  &#123;vec[1]:.8f&#125;  &#123;vec[2]:.8f&#125;\n&quot;)        #&quot;  &quot;.join用于将可迭代对象的元素以空格连接起来        file.write(&quot;  &quot;.join(element_type)+ &quot;\n&quot;)        file.write(&quot;  &quot;.join(map(str,atomnumbers))+ &quot;\n&quot;)        file.write(f&quot;&#123;cor_type&#125;\n&quot;)        for pos in atompositions:            file.write(f&quot;&#123;pos[0]:.8f&#125;  &#123;pos[1]:.8f&#125;  &#123;pos[2]:.8f&#125;\n&quot;)#输出不同类型原子的坐标def atom_pos(atomnumbers,atompositions):    atom_pos = np.split(atompositions,np.cumsum(atomnumbers)[:-1])    return atom_pos        &quot;&quot;&quot;功能区&quot;&quot;&quot;#计算体积def cal_volume(lattice_vectors):    volume = np.abs(np.dot(lattice_vectors[0],np.cross(lattice_vectors[1],lattice_vectors[2])))    return volume#转换分数坐标为笛卡尔坐标def change_dir_to_car(filename):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= read_poscar(filename)    if cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        pass    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        atompositions = np.dot(atompositions,lattice_vectors)        cor_type = &quot;Cartesian&quot;    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#笛卡尔坐标转换为分数坐标def change_car_to_dir(filename):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions = read_poscar(filename)    if cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:        atompositions = np.dot(atompositions,np.linalg.inv(lattice_vectors))        cor_type = &quot;Direct&quot;        atompositions[atompositions &lt; 0]+=1        atompositions[atompositions &gt;=1]-=1    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        pass    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#晶格向量的变换def lattice_change(filename,cell_type):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= change_car_to_dir(filename)    if cell_type == &#39;211&#39;:        pass    elif cell_type == &quot;121&quot;:        middle_value = lattice_vectors[0].copy()        lattice_vectors[0] = lattice_vectors[1]        lattice_vectors[1] = lattice_vectors[2]        lattice_vectors[2] = middle_value        middle_value2 = atompositions[:,0].copy()        atompositions[:,0] = atompositions[:,1]        atompositions[:,1] = atompositions[:,2]        atompositions[:,2] = middle_value2    elif cell_type == &quot;112&quot;:        middle_value = lattice_vectors[0].copy()        lattice_vectors[0] = lattice_vectors[2]        lattice_vectors[2] = lattice_vectors[1]        lattice_vectors[1] = middle_value        middle_value2 = atompositions[:,0].copy        atompositions[:,0] = atompositions[:,2]        atompositions[:,2] = atompositions[:,1]        atompositions[:,1] = middle_value2    return title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions#将211晶胞分解为两个111晶胞def split_211(input_file,cell_type,output_file1,output_file2):    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= lattice_change(input_file,cell_type)    atom_type_pos = atom_pos(atomnumbers,atompositions)    total_atoms1 = []    total_atoms_number1 = []    total_atoms2 = []    total_atoms_number2 = []    if cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        for sub_atom_type_pos in atom_type_pos:            print(sub_atom_type_pos)            coord_1=sub_atom_type_pos[sub_atom_type_pos[:,0]&lt;=0.48].copy()            total_atoms1.append(coord_1)            total_atoms_number1.append(len(coord_1))                    coord_2=sub_atom_type_pos[sub_atom_type_pos[:,0]&gt;0.52].copy()            total_atoms2.append(coord_2)            total_atoms_number2.append(len(coord_2))    # elif cor_type.lower() == &quot;cartesian&quot; or cor_type.lower() == &quot;c&quot;:    #     for sub_atom_type_pos in atom_type_pos:    #         sub_atom_type_pos=np.dot(sub_atom_type_pos,np.linalg.inv(lattice_vectors))    #         sub_atom_type_pos[sub_atom_type_pos &lt; 0]+=1    #         sub_atom_type_pos[sub_atom_type_pos &gt;=1]-=1    #         coord_1=sub_atom_type_pos[sub_atom_type_pos[:,0]&lt;=0.5].copy()    #         total_atoms1.append(coord_1)    #         total_atoms_number1.append(len(coord_1))            #         coord_2=sub_atom_type_pos[sub_atom_type_pos[:,0]&gt;0.5].copy()    #         total_atoms2.append(coord_2)    #         total_atoms_number2.append(len(coord_2))    #     cor_type = &quot;Direct&quot;    total_atoms1=np.vstack(total_atoms1)    total_atoms2=np.vstack(total_atoms2)    total_atoms1=np.dot(total_atoms1,lattice_vectors)    total_atoms2=np.dot(total_atoms2,lattice_vectors)    lattice_vectors[0]/=2    cor_type =&quot;Cartesian&quot;    # print(total_atoms1)    # print(total_atoms_number1)    write_poscar(output_file1,title,scale,lattice_vectors,element_type,total_atoms_number1,cor_type,total_atoms1)    write_poscar(output_file2,title,scale,lattice_vectors,element_type,total_atoms_number2,cor_type,total_atoms2)#POSCAR坐标的修改def coords_change(axis,value,cor_type,atompositions):    if cor_type.lower() == &quot;Cartesian&quot; or cor_type.lower() == &quot;c&quot;:        print(&quot;use change_car_to_dir to input&quot;)    elif cor_type.lower() == &quot;direct&quot; or cor_type.lower() == &quot;d&quot;:        if axis == &#39;x&#39;:           atompositions[:,0] += value        elif axis == &#39;y&#39;:           atompositions[:,1] += value        elif axis == &#39;z&#39;:           atompositions[:,2] += value        else:            raise ValueError(f&quot;invalid axis&quot;)    else:        raise ValueError(f&quot;invalid cor_type&quot;)    return atompositions#切割x*y*z倍的晶胞为x*y*z个晶胞def split_cell(input_file,x,y,z):    input_file_without_ext = os.path.splitext(input_file)[0]    title,scale,lattice_vectors,element_type,atomnumbers,cor_type,atompositions= change_car_to_dir(input_file)    atom_type_pos = atom_pos(atomnumbers,atompositions)    total_cells = [[[&#123;elem:[] for elem in element_type&#125; for _ in range(z)] for _ in range(y)] for _ in range(x)]    total_atom_numbers = [[[&#123;elem: 0 for elem in element_type&#125; for _ in range(z)] for _ in range(y)] for _ in range(x)]    if cor_type.lower() == &quot;direct&quot; or cor_type.lower == &quot;d&quot;:        for elem_index,sub_atom_type_pos in enumerate(atom_type_pos):            elem = element_type[elem_index]            for atom in sub_atom_type_pos:                ix = int(atom[0]*x)                iy = int(atom[1]*y)                iz = int(atom[2]*z)                total_cells[ix][iy][iz][elem].append(atom)                total_atom_numbers[ix][iy][iz][elem] += 1    cor_type =&quot;Cartesian&quot;    lattice_vectors_changed = np.zeros((3,3))    lattice_vectors_changed[0] = lattice_vectors[0]/x    lattice_vectors_changed[1] = lattice_vectors[1]/y    lattice_vectors_changed[2] = lattice_vectors[2]/z    os.makedirs(&quot;xsf_output&quot;)    for ix in range(x):        for iy in range(y):            for iz in range(z):                output_file =f&quot;&#123;input_file_without_ext&#125;_&#123;ix&#125;&#123;iy&#125;&#123;iz&#125;.vasp&quot;                sub_atoms = np.vstack([atom for elem in element_type for atom in total_cells[ix][iy][iz][elem]])                sub_atoms = np.dot(sub_atoms, lattice_vectors)                subcell_numbers = [total_atom_numbers[ix][iy][iz][elem] for elem in element_type]                write_poscar(output_file,title,scale,lattice_vectors_changed,element_type,subcell_numbers,cor_type,sub_atoms)                datas = read(output_file, index=&quot;:&quot;)                ene, lat, pos, foc, vir, ele = read_from_ase_atoms(datas[0])                output_file_path = os.path.join(&quot;xsf_output&quot;, f&quot;&#123;output_file&#125;.xsf&quot;)                write2my(                    output_file_path,                    ene_i=ene,                    lat_i=lat,                    ele_i=ele,                    coo_i=pos,                    foc_i=foc,                    vir_i=vir)split_cell(&quot;POSCAR&quot;,2,2,2)#os.remove(&quot;POS&quot;)subprocess.run([&quot;dsub&quot;,&quot;-s&quot;,&quot;sub-grade.sh&quot;], check=True)counter = 0with open(&quot;grade-log&quot;, &#39;r&#39;) as file:    lines = file.readlines()    for line in lines:        if &#39;grad:&#39; in line:            parts = line.split()            filename = parts[0]            filename_first = os.path.splitext(filename)[0]            grade_value = float(parts[-1])            if 3 &lt; grade_value &lt; 8:                os.makedirs(filename_first)                shutil.move(f&quot;&#123;filename_first&#125;&quot;, os.path.join (filename_first, &quot;POSCAR&quot;))            else:                os.remove(f&quot;&#123;filename_first&#125;&quot;)</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-随机位移的方法&quot;&gt;&lt;a href=&quot;#1-随机位移的方法&quot; class=&quot;headerlink&quot; title=&quot;1. 随机位移的方法&quot;&gt;&lt;/a&gt;1. 随机位移的方法&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;给整个结构中的原子引入随机位移，代替m</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>处理分数占据结构</title>
    <link href="http://example.com/2025/02/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%A4%84%E7%90%86%E5%88%86%E6%95%B0%E5%8D%A0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    <id>http://example.com/2025/02/25/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%A4%84%E7%90%86%E5%88%86%E6%95%B0%E5%8D%A0%E6%8D%AE%E7%BB%93%E6%9E%84/</id>
    <published>2025-02-25T01:23:40.000Z</published>
    <updated>2025-12-02T12:09:17.887Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量化操作"><a href="#1-自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量化操作" class="headerlink" title="1. 自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量化操作"></a>1. 自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量化操作</h1><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20251015135455841.png" alt="image-20251015135455841"></p><pre><code class="python">#通过修改 tolerance可以限制哪些结构扩胞到100个原子以上import osimport mathfrom pymatgen.core import Structurefrom pymatgen.core.composition import Compositionimport reimport warningsfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerimport shutilfrom pymatgen.core.periodic_table import Elementdef get_scaling_factors(n):    &quot;&quot;&quot;    将整数n分解为三个尽可能接近的整数因子。    &quot;&quot;&quot;    if n == 1:        return [1, 1, 1]        cbrt = int(round(n**(1/3.0)))    for a in range(cbrt, 0, -1):        if n % a == 0:            rem = n // a            sqrt_rem = int(round(rem**0.5))            for b in range(sqrt_rem, 0, -1):                if rem % b == 0:                    c = rem // b                    factors = sorted([a, b, c], reverse=True)                    return factors    return [n, 1, 1]def get_site_contributions_from_cif(filepath):    &quot;&quot;&quot;    【新功能】直接从CIF文件中解析 _atom_site_ 循环，    计算每一行 (多样性 * 占有率) 的值。    Args:        filepath (str): CIF文件的路径。    Returns:        list: 包含每一行计算出的原子数的列表。如果解析失败则返回空列表。    &quot;&quot;&quot;    # contributions = []    site_info_list = []        try:        with open(filepath, &#39;r&#39;) as f:            lines = f.readlines()    except Exception as e:        print(f&quot;错误: 无法打开文件 &#123;filepath&#125;. 原因: &#123;e&#125;&quot;)        return []    in_atom_loop = False    header_keys = []        # 正则表达式用于安全地分割可能包含带引号字符串的行    line_splitter = re.compile(r&#39;&#39;&#39;((?:[^ &quot;&#39;]|&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;)+)&#39;&#39;&#39;)    for line in lines:        line = line.strip()        if not line or line.startswith(&#39;#&#39;):            continue        # 遇到 &#39;loop_&#39; 开始寻找 _atom_site_ 标签        if line.lower() == &#39;loop_&#39;:            in_atom_loop = False            header_keys = []            continue        # if line.lower().startswith(&#39;_atom_site_&#39;):        if line.lower().startswith(&#39;_atom_site_&#39;) and not line.lower().startswith(&#39;_atom_site_aniso_&#39;):            in_atom_loop = True            header_keys.append(line.lower())            continue        # 如果我们在一个原子循环中，并且已经读取了头部信息，那么这就是数据行        if in_atom_loop and header_keys:            # 如果遇到新的数据块或循环，则当前原子循环结束            if line.startswith(&#39;data_&#39;) or line.startswith(&#39;loop_&#39;) or line.startswith(&#39;_&#39;):                in_atom_loop = False                continue            # 使用正则表达式分割行以处理带空格的标签等情况            parts = [p.strip() for p in line_splitter.split(line) if p.strip()]            if len(parts) &lt; len(header_keys):                continue            try:                # 找到多样性和占有率所在的列索引                mult_idx = header_keys.index(&#39;_atom_site_symmetry_multiplicity&#39;)                occ_idx = header_keys.index(&#39;_atom_site_occupancy&#39;)                                multiplicity  = int(parts[mult_idx])                occupancy_str = parts[occ_idx]                occupancy_cleaned = occupancy_str.split(&#39;(&#39;)[0]                occupancy = float(occupancy_cleaned)                                         amount = multiplicity * occupancy                # contributions.append(amount)                site_info_list.append(&#123;                    &quot;multiplicity&quot;: multiplicity,                    &quot;occupancy&quot;: occupancy,                    &quot;amount&quot;: amount                &#125;)            except (ValueError, IndexError) as e:                print(f&quot;警告: 无法解析原子位点行: &#39;&#123;line&#125;&#39;. 原因: &#123;e&#125;&quot;)                continue    is_fractional = False    for info in site_info_list:        occupancy = info[&#39;occupancy&#39;]        if abs(occupancy - 1.0) &gt; 0.001:            print(f&quot;文件 &#39;&#123;os.path.basename(filepath)&#125;&#39; 中发现分数占据位点: 占有率 &#123;occupancy:.4f&#125;&quot;)            is_fractional = True            break      if is_fractional:        return site_info_list    else:        print(f&quot;文件 &#39;&#123;os.path.basename(filepath)&#125;&#39; 所有位点均为整数占据。&quot;)        all_occupied_dir = &#39;6-all_occupied&#39;        try:            os.makedirs(all_occupied_dir, exist_ok=True)            shutil.copy(filepath, os.path.join(all_occupied_dir, os.path.basename(filepath)))            print(f&quot;已将文件移动到 &#39;&#123;all_occupied_dir&#125;/&#39; 目录。&quot;)        except Exception as e:            print(f&quot;移动文件 &#39;&#123;filepath&#125;&#39; 时出错: &#123;e&#125;&quot;)                return []def create_modified_cif_content(filepath, new_occupancies):    &quot;&quot;&quot;    读取原始CIF文件，仅替换_atom_site_loop_中的占有率值。        Args:        filepath (str): 原始CIF文件的路径。        new_occupancies (list[float]): 按顺序排列的新的占有率值。            Returns:        str: 一个包含完整CIF文件内容的字符串，如果失败则返回None。    &quot;&quot;&quot;    try:        with open(filepath, &#39;r&#39;) as f:            lines = f.readlines()    except Exception as e:        print(f&quot;错误: 无法打开文件 &#123;filepath&#125;. 原因: &#123;e&#125;&quot;)        return []    new_cif_lines = []    in_atom_loop = False    header_keys = []    occ_idx = -1    atom_site_counter = 0    line_splitter = re.compile(r&#39;&#39;&#39;((?:[^ &quot;&#39;]|&quot;[^&quot;]*&quot;|&#39;[^&#39;]*&#39;)+)&#39;&#39;&#39;)    for line in lines:        stripped_line = line.strip()        if stripped_line.lower() == &#39;loop_&#39;:            in_atom_loop = False            header_keys = []            occ_idx = -1            new_cif_lines.append(line)            continue        if stripped_line.lower().startswith(&#39;_atom_site_&#39;) and not stripped_line.lower().startswith(&#39;_atom_site_aniso_&#39;):            in_atom_loop = True            header_keys.append(stripped_line.lower())            new_cif_lines.append(line)            continue                # 如果我们在原子数据区        if in_atom_loop and header_keys and not stripped_line.startswith(&#39;_&#39;) and not stripped_line.startswith(&#39;#End&#39;) and stripped_line:             # 第一次进入数据区时，找到占有率列的索引            if occ_idx == -1:                try:                    occ_idx = header_keys.index(&#39;_atom_site_occupancy&#39;)                except ValueError:                    print(&quot;错误: _atom_site_loop_ 中未找到 _atom_site_occupancy 标签。&quot;)                    return None            # 分割行，替换值，然后重新组合            parts = [p for p in line_splitter.split(stripped_line) if p.strip()]            # if parts ==[]:            #     continue            # print(header_keys)            # print(parts)            if len(parts) == len(header_keys):                if atom_site_counter &lt; len(new_occupancies):                    # 格式化新的占有率，保留足够精度                    parts[occ_idx] = f&quot;&#123;new_occupancies[atom_site_counter]:.5f&#125;&quot;                    # 重新组合成一行，注意保留原始的空格格式                    # 这是一个简化处理，假设列之间是单个或多个空格                    new_line = &quot;  &quot;.join(parts) + &quot;\n&quot;                    new_cif_lines.append(new_line)                    atom_site_counter += 1                else:                    print(&#39;占用率列表不够长&#39;)                    return None            else:                 print(&#39;行格式不匹配&#39;)                 new_cif_lines.append(line)        else:            # if stripped_line.startswith(&#39;_&#39;)  and in_atom_loop:            if (stripped_line.startswith(&#39;_&#39;) or not stripped_line or stripped_line.startswith(&#39;#End&#39;)) and in_atom_loop:                in_atom_loop = False # 遇到新的标签，原子循环结束            new_cif_lines.append(line)        if not os.path.exists(&#39;3-intermediate_dir&#39;):        os.makedirs(&#39;3-intermediate_dir&#39;)    base_name = os.path.basename(filepath)    file_name_without_ext = os.path.splitext(base_name)[0]    intermediate_filename = f&quot;&#123;file_name_without_ext&#125;_modified.cif&quot;    intermediate_filepath = os.path.join(&#39;3-intermediate_dir&#39;, intermediate_filename)    with open(intermediate_filepath, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f:        f.write(&quot;&quot;.join(new_cif_lines))    print(f&quot;成功保存修正后的中间文件: &#123;intermediate_filepath&#125;&quot;)    return &quot;&quot;.join(new_cif_lines)#针对8个位点0.07占据 的情况的处理，先扩胞到100个原子以上，然后约化原子个数，然后重新反除回去，然后把点位的分数占据重新计算，然后再扩胞def process_via_text_modification(filepath):    print(f&quot;--- 处理文件 (文本模式): &#123;os.path.basename(filepath)&#125; ---&quot;)    output_dir = &#39;4-appro_structures&#39;    os.makedirs(output_dir, exist_ok=True)    if not os.path.exists(output_dir):        os.makedirs(output_dir)    # 步骤1: 解析文本获取原子位点信息    site_info = get_site_contributions_from_cif(filepath)    if not site_info:        print(f&quot;无法从 &#123;filepath&#125; 解析原子位点信息&quot;)        return    contributions = [info[&#39;amount&#39;] for info in site_info]    initial_total_atoms = sum(contributions)    print(f&quot;单胞中的初始总原子数: &#123;initial_total_atoms:.4f&#125;&quot;)    best_n = None    for n in range(1, 61):        if initial_total_atoms * n &gt;= 100:            best_n = n            break    print(f&quot;找到合适的扩胞倍数 n = &#123;best_n&#125;&quot;)    # 步骤3: 计算所有位点的新占有率    new_occupancies = []    for info in site_info:        scaled_amount_rounded = round(info[&#39;amount&#39;] * best_n)        if info[&#39;multiplicity&#39;] == 0 or best_n == 0:            new_occ = 0        else:            new_occ = scaled_amount_rounded / (info[&#39;multiplicity&#39;] * best_n)        new_occupancies.append(new_occ)    # 步骤4: 生成包含新占有率的CIF文件内容（纯文本操作）    modified_cif_content = create_modified_cif_content(filepath, new_occupancies)    try:        with warnings.catch_warnings():            warnings.simplefilter(&quot;ignore&quot;, UserWarning)            modified_structure = Structure.from_str(modified_cif_content, fmt=&quot;cif&quot;)    except Exception as e:        print(f&quot;错误: Pymatgen无法得到结构&quot;)        return        scaling_factors = get_scaling_factors(best_n)    supercell = modified_structure.copy()    supercell.make_supercell(scaling_factors)        # 步骤6: 保存最终的超胞结构    base_name = os.path.basename(filepath)    file_name_without_ext = os.path.splitext(base_name)[0]    scaling_str = f&quot;&#123;scaling_factors[0]&#125;&#123;scaling_factors[1]&#125;&#123;scaling_factors[2]&#125;&quot;    output_filename = f&quot;&#123;file_name_without_ext&#125;_scaled_&#123;scaling_str&#125;.cif&quot;    output_path = os.path.join(output_dir, output_filename)    with warnings.catch_warnings():        warnings.simplefilter(&quot;ignore&quot;, UserWarning)        supercell.to(fmt=&quot;cif&quot;, filename=output_path)    print(f&quot;成功创建并保存超胞: &#123;output_path&#125;&quot;)    print(f&quot;最终超胞化学式: &#123;supercell.composition.reduced_formula&#125;，总原子数: &#123;supercell.num_sites&#125;\n&quot;)def process_structure_file(filepath, output_dir, tolerance=0.0001):    &quot;&quot;&quot;    处理单个CIF文件，根据每个位点的原子贡献判断是否需要扩胞，并执行扩胞操作。    &quot;&quot;&quot;    print(f&quot;--- 正在处理文件: &#123;filepath&#125; ---&quot;)        # 【新流程】首先从CIF文本中直接解析每个位点的原子数贡献    site_info_list = get_site_contributions_from_cif(filepath)    if site_info_list == []:        return    site_amounts = [info[&#39;amount&#39;] for info in site_info_list]    total_sum = sum(site_amounts)    if not site_amounts:        print(f&quot;错误: 未能从 &#123;filepath&#125; 的 _atom_site_ 循环中提取到数据。&quot;)        return    if not os.path.exists(&#39;7-pymatgen_error&#39;):        os.makedirs(&#39;7-pymatgen_error&#39;)    try:        structure = Structure.from_file(filepath)    except Exception as e:        shutil.copy(filepath, os.path.join(&#39;7-pymatgen_error&#39;, os.path.basename(filepath)))        print(f&quot;错误: Pymatgen无法读取或解析文件 &#123;filepath&#125;. 原因: &#123;e&#125;&quot;)        return        if not os.path.exists(&#39;8-pymatgen_sym_error&#39;):        os.makedirs(&#39;8-pymatgen_sym_error&#39;)        try:        analyzer = SpacegroupAnalyzer(structure)        prim_cell = analyzer.find_primitive()    except Exception as e:        shutil.copy(filepath, os.path.join(&#39;8-pymatgen_sym_error&#39;, os.path.basename(filepath)))        print(f&quot;错误: Pymatgen无法找到原胞 &#123;filepath&#125;. 原因: &#123;e&#125;&quot;)        return    structur_site_num = structure.num_sites    prim_site_num = prim_cell.num_sites    if prim_site_num != structur_site_num:        n_scal = structur_site_num / prim_site_num        print(f&#39;单胞是原胞的&#123;n_scal&#125;倍&#39;)        site_amounts = [amount / n_scal for amount in site_amounts]        total_sum = total_sum / n_scal    if not os.path.exists(&#39;1-exceed_200&#39;):        os.makedirs(&#39;1-exceed_200&#39;)    if total_sum &gt;= 200:        shutil.copy(filepath, os.path.join(&#39;1-exceed_200&#39;, os.path.basename(filepath)))        print(&#39;原胞中原子个数超过200个原子，跳过&#39;)        return        print(f&quot;从CIF中解析出的各行原子数贡献: &#123;[round(x, 4) for x in site_amounts]&#125;&quot;)    # 检查是否有任何一个位点贡献值需要进行整数化处理    needs_scaling = False    for amount in site_amounts:        if abs(amount - round(amount)) &gt; tolerance:            needs_scaling = True            print(f&quot;位点贡献值 &#123;round(amount, 4)&#125; 与整数偏差大于 &#123;tolerance&#125;。&quot;)            break    if not needs_scaling:        print(&quot;该结构所有位点贡献值均在容差范围内，无需扩胞。&quot;)        # if total_sum &gt; 200:        #     print(&#39;虽然无需扩胞，但是该结构总原子数超过200个原子,将进行约化处理&#39;)        #     # shutil.copy(filepath, os.path.join(need_appro_dir, os.path.basename(filepath)))        #     process_via_text_modification(filepath)        #     return        if not os.path.exists(output_dir):            os.makedirs(output_dir)        base_name = os.path.basename(filepath)        file_name_without_ext = os.path.splitext(base_name)[0]        output_filename = f&quot;&#123;file_name_without_ext&#125;_scaled_111.cif&quot; # 固定后缀        output_path = os.path.join(output_dir, output_filename)        # 保存原胞到新路径        with warnings.catch_warnings():            warnings.simplefilter(&quot;ignore&quot;, UserWarning)            prim_cell.to(fmt=&quot;cif&quot;, filename=output_path)        print(f&quot;已将结构保存至: &#123;output_path&#125;\n&quot;)        return    # analyzer = SpacegroupAnalyzer(structure)    # prim_cell = analyzer.find_primitive()    # structur_site_num = structure.num_sites    # prim_site_num = prim_cell.num_sites    # if prim_site_num != structur_site_num:    #     n_scal = structur_site_num / prim_site_num    #     print(f&#39;单胞是原胞的&#123;n_scal&#125;倍&#39;)    #     site_amounts = [amount / n_scal for amount in site_amounts]    #     total_sum = total_sum / n_scal    # 如果需要扩胞，寻找最小的扩胞系数 (从2到10)    best_scaling_factor = None    for n in range(2, 61):        is_factor_suitable = True        for amount in site_amounts: # 使用从CIF解析出的列表进行检查            scaled_amount = amount * n            if abs(scaled_amount - round(scaled_amount)) &gt; tolerance:                is_factor_suitable = False                break                if is_factor_suitable:            best_scaling_factor = n            print(f&quot;找到合适的扩胞系数: &#123;n&#125;&quot;)            break        #如果有原子个数小于0.1的位点，后续sqs或order很难做，所以在这里收集起来,后续直接修改占据数或者舍弃    best_scaling_atom = site_amounts * n    lessthan1 = False    for site_number in best_scaling_atom:        if  site_number &lt; 0.1:            print(f&quot;文件 &#39;&#123;os.path.basename(filepath)&#125;&#39; 中发现有原子个数小于0.1&quot;)            lessthan1 = True            break      if lessthan1:        os.makedirs(&#39;5-lessthan1dir&#39;, exist_ok=True)        shutil.copy(filepath, os.path.join(&#39;5-lessthan1dir&#39;, os.path.basename(filepath)))        print(&quot;已将文件移动到 &#39;5-lessthan1dir/&#39; 目录。&quot;)        return    if best_scaling_factor is None:        print(f&quot;&#123;filepath&#125;在2-60倍范围内未找到合适的扩胞系数，将进行约化处理&quot;)        process_via_text_modification(filepath)        # shutil.copy(filepath, os.path.join(need_appro_dir, os.path.basename(filepath)))        return    # 使用Pymatgen读取结构，用于执行扩胞操作    # try:    #     structure = Structure.from_file(filepath)    # except Exception as e:    #     print(f&quot;错误: Pymatgen无法读取或解析文件 &#123;filepath&#125;. 原因: &#123;e&#125;&quot;)    #     return    # 计算扩胞矩阵并创建超胞    scaling_factors = get_scaling_factors(best_scaling_factor)    total_atom_number = total_sum * best_scaling_factor    if total_atom_number &gt; 200:        print(&#39;该结构总原子数超过200个原子&#39;)        print(f&#39;该结构总原子数为&#123;total_atom_number&#125;，将进行约化处理&#39;)        # shutil.copy(filepath, os.path.join(need_appro_dir, os.path.basename(filepath)))        process_via_text_modification(filepath)        return    supercell = prim_cell.copy()    supercell.make_supercell(scaling_factors)    # for i, site in enumerate(supercell.sites):    # # site.specie.symbol 获取元素的符号 (如 &#39;Li&#39;, &#39;Mg&#39;)    # # i 是从0开始的索引，我们加1让它从1开始    # # f&quot;&#123;...&#125;&quot; 是Python的格式化字符串    #     site.label = f&quot;&#123;site.species.symbol&#125;&#123;i+1&#125;&quot;    #     print(&quot;标签重新生成完毕。&quot;)    # 保存扩胞后的结构    if not os.path.exists(output_dir):        os.makedirs(output_dir)    base_name = os.path.basename(filepath)    file_name_without_ext = os.path.splitext(base_name)[0]    scaling_str = f&quot;&#123;scaling_factors[0]&#125;&#123;scaling_factors[1]&#125;&#123;scaling_factors[2]&#125;&quot;    output_filename = f&quot;&#123;file_name_without_ext&#125;_scaled_&#123;scaling_str&#125;.cif&quot;    output_path = os.path.join(output_dir, output_filename)    with warnings.catch_warnings():        warnings.simplefilter(&quot;ignore&quot;, UserWarning)        supercell.to(fmt=&quot;cif&quot;, filename=output_path)    # supercell.to(fmt=&quot;cif&quot;, filename=output_path)    print(f&quot;成功创建超胞，并保存至: &#123;output_path&#125;&quot;)    print(f&quot;新结构的化学式: &#123;supercell.composition.reduced_formula&#125;\n&quot;)if __name__ == &#39;__main__&#39;:    input_dir = &#39;origin_str&#39;    for file in os.listdir(input_dir):        if file.endswith(&quot;.cif&quot;):            test_filename = os.path.join(input_dir, file)                process_structure_file(test_filename, output_dir=&quot;2-output_structures&quot;)</code></pre><h1 id="1-pymatgen检验相同结构"><a href="#1-pymatgen检验相同结构" class="headerlink" title="1.pymatgen检验相同结构"></a>1.pymatgen检验相同结构</h1><p>fcc、bcc等对应着特殊空间群里特定位置的占据</p><h2 id="1-一次性检验"><a href="#1-一次性检验" class="headerlink" title="1.一次性检验"></a>1.一次性检验</h2><pre><code class="python">import os from pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;./&#39;for i in range(1000):    filename = &quot;&#123;&#125;.vasp&quot;.format(i)    if not os.path.exists(os.path.join(dir,filename)):        continue    print(filename)    structure = Structure.from_file(filename)    structure.remove_species(&quot;O&quot;)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)</code></pre><h2 id="2-逐个检验"><a href="#2-逐个检验" class="headerlink" title="2.逐个检验"></a>2.逐个检验</h2><pre><code class="python">import numpy as npimport sys,os,timefrom pymatgen.core import Structurefrom pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()all_str = []for i in range(1,100000):    file = f&#39;&#123;i&#125;.vasp&#39;    if not os.path.exists(file):       continue    print(file)    strua = Structure.from_file(file)    match_found = False    for str in all_str:        if matcher.fit(strua,str):           match_found = True           break    if not match_found:        all_str.append(strua)        print(&#39;%d structures&#39; % len(all_str))from pymatgen.io.vasp.inputs import Poscarn = 0for strua in all_str:    Vasp_Str = Poscar(strua)    n +=1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)</code></pre><h2 id="3-检验两个结构"><a href="#3-检验两个结构" class="headerlink" title="3. 检验两个结构"></a>3. 检验两个结构</h2><pre><code class="python">import numpy as npimport sys,os,timefrom pymatgen.core import Structurefrom pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()aa = Structure.from_file(&#39;16.vasp&#39;)bb = Structure.from_file(&#39;4199.vasp&#39;)matcher.fit(aa,bb)</code></pre><h1 id="2-pymatgen-enumeration"><a href="#2-pymatgen-enumeration" class="headerlink" title="2. pymatgen-enumeration"></a>2. pymatgen-enumeration</h1><blockquote><p>对于一个有空间群、对称性的cif文件，vesta可以显示wyckoff占位的信息，选中原子即可</p></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20251107093023293.png" alt="image-20251107093023293"></p><pre><code class="python">from pymatgen.core import Structurefrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerfrom pymatgen.transformations.advanced_transformations import EnumerateStructureTransformationfrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom pymatgen.analysis.bond_valence import BVAnalyzerstructure = Structure.from_file(&quot;a.cif&quot;)data = &#123;&quot;Na&quot;:1, &quot;Y&quot;:3, &quot;Si&quot;:4,&quot;O&quot;:-2,&quot;Al&quot;:3,&quot;K&quot;:1&#125;analyzer = SpacegroupAnalyzer(structure)prim_cell = analyzer.find_primitive()prim_cell.add_oxidation_state_by_element(data)#print(prim_cell)enum = EnumerateStructureTransformation()enumerated = enum.apply_transformation(prim_cell, 100)structures = [d[&quot;structure&quot;] for d in enumerated]print(&quot;%d structures returned.&quot; % len(structures))from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in structures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)#from pymatgen.io.cif import CifWriter#n = 0#for Cry_Str in structures:#    n = n + 1#    cif_filename = f&quot;&#123;n&#125;.cif&quot;#    cif_writer = CifWriter(Cry_Str)#    cif_writer.write_file(cif_filename)   #energy= [d[&quot;energy&quot;] for d in enumerated]#fl = open(&#39;energy.dat&#39;,&#39;w&#39;)#fl.writelines(str(energy))#data = &#123;&quot;K&quot;:1, &quot;O&quot;:-2, &quot;Fe&quot;:2,&quot;Ti&quot;:4.25,&quot;Mn&quot;:3.75&#125;#abc=[]#for i in range(1,101):#    m = Structure.from_file(&#39;%d.vasp&#39;%i)#    m.add_oxidation_state_by_element(data)#    energy = EwaldElectrostaticModel().get_energy(m)#    abc.append(energy)#f2 = open(&#39;energy2.dat&#39;,&#39;w&#39;)#f2.writelines(str(abc))with open(&#39;energy.dat&#39;, &#39;w&#39;) as f:    for item in abc:        f.write(str(item) + &#39;\n&#39;)#strcturelist2.sort(key = EwaldElectrostaticModel().get_energy)</code></pre><h1 id="3-pymatgen-orderdisorder"><a href="#3-pymatgen-orderdisorder" class="headerlink" title="3. pymatgen-orderdisorder"></a>3. pymatgen-orderdisorder</h1><pre><code class="python">import osfrom pymatgen.io.cif import CifParser, CifWriterfrom pymatgen.transformations.standard_transformations import SubstitutionTransformation, OrderDisorderedStructureTransformation, SupercellTransformationfrom pymatgen.core import Structurefrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerfrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom pymatgen.transformations.advanced_transformations import EnumerateStructureTransformationfrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstructure = Structure.from_file(&quot;a.cif&quot;)data = &#123;&quot;Ni&quot;:2, &quot;Co&quot;:3, &quot;S&quot;:-2,&quot;Se&quot;:-2&#125;analyzer = SpacegroupAnalyzer(structure)prim_cell = analyzer.find_primitive()prim_cell.add_oxidation_state_by_element(data)print(prim_cell)order = OrderDisorderedStructureTransformation()standard_structures=order.apply_transformation(prim_cell,return_ranked_list=1000)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d[&quot;structure&quot;] for d in standard_structures]) #这应该是个字典，把字典中的structure关键字提出来组&gt;成列表,groups里只剩下结构了#structures = [d[&quot;structure&quot;] for d in standard_structures]#print(&quot;%d structures returned.&quot; % len(structures))#from pymatgen.io.vasp.inputs import Poscar#n = 0#for Cry_Str in structures:    #print(Cry_Str)#    Vasp_Str = Poscar(Cry_Str)#    n = n+1#    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)#产生等差数列 for i in range(len(groups))#提取出列举时计算的能量 energy = [x(&#39;energy&#39;) for x in standard_structures]#验证同一类群下的结构静电能相等#suoyoua = [x for x in groups[0]]#for cry in suoyoua:#   energy = EwaldElectrostaticModel().get_energy(cry)#  print(energy)#验证输出的顺序是按照能量升序进行的#suoyou1 = [x[0] for x in groups]#for cry in suoyou1:#    energy = EwaldElectrostaticModel().get_energy(cry)#    print(energy)</code></pre><h3 id="批量做"><a href="#批量做" class="headerlink" title="批量做"></a>批量做</h3><pre><code class="python">import osfrom pymatgen.transformations.standard_transformations import OrderDisorderedStructureTransformationfrom pymatgen.core import Structurefrom pymatgen.io.vasp.inputs import Poscarstructure = Structure.from_file(&quot;a.cif&quot;)COMMON_OXIDATION_STATES = &#123;    &#39;H&#39;: 1, &#39;He&#39;: 0, &#39;Li&#39;: 1, &#39;Be&#39;: 2, &#39;B&#39;: 3, &#39;C&#39;: 4, &#39;N&#39;: -3, &#39;O&#39;: -2, &#39;F&#39;: -1,     &#39;Ne&#39;: 0, &#39;Na&#39;: 1, &#39;Mg&#39;: 2, &#39;Al&#39;: 3, &#39;Si&#39;: 4, &#39;P&#39;: 5, &#39;S&#39;: -2, &#39;Cl&#39;: -1, &#39;Ar&#39;: 0,     &#39;K&#39;: 1, &#39;Ca&#39;: 2, &#39;Sc&#39;: 3, &#39;Ti&#39;: 4, &#39;V&#39;: 5, &#39;Cr&#39;: 3, &#39;Mn&#39;: 2, &#39;Fe&#39;: 3, &#39;Co&#39;: 2,     &#39;Ni&#39;: 2, &#39;Cu&#39;: 2, &#39;Zn&#39;: 2, &#39;Ga&#39;: 3, &#39;Ge&#39;: 4, &#39;As&#39;: 3, &#39;Se&#39;: -2, &#39;Br&#39;: -1, &#39;Kr&#39;: 0,     &#39;Rb&#39;: 1, &#39;Sr&#39;: 2, &#39;Y&#39;: 3, &#39;Zr&#39;: 4, &#39;Nb&#39;: 5, &#39;Mo&#39;: 6, &#39;Tc&#39;: 7, &#39;Ru&#39;: 4, &#39;Rh&#39;: 3,     &#39;Pd&#39;: 2, &#39;Ag&#39;: 1, &#39;Cd&#39;: 2, &#39;In&#39;: 3, &#39;Sn&#39;: 4, &#39;Sb&#39;: 3, &#39;Te&#39;: -2, &#39;I&#39;: -1, &#39;Xe&#39;: 0,     &#39;Cs&#39;: 1, &#39;Ba&#39;: 2, &#39;La&#39;: 3, &#39;Ce&#39;: 3, &#39;Pr&#39;: 3, &#39;Nd&#39;: 3, &#39;Pm&#39;: 3, &#39;Sm&#39;: 3, &#39;Eu&#39;: 3,     &#39;Gd&#39;: 3, &#39;Tb&#39;: 3, &#39;Dy&#39;: 3, &#39;Ho&#39;: 3, &#39;Er&#39;: 3, &#39;Tm&#39;: 3, &#39;Yb&#39;: 3, &#39;Lu&#39;: 3, &#39;Hf&#39;: 4,     &#39;Ta&#39;: 5, &#39;W&#39;: 6, &#39;Re&#39;: 4, &#39;Os&#39;: 4, &#39;Ir&#39;: 4, &#39;Pt&#39;: 4, &#39;Au&#39;: 3, &#39;Hg&#39;: 2, &#39;Tl&#39;: 1,     &#39;Pb&#39;: 2, &#39;Bi&#39;: 3, &#39;Po&#39;: 2, &#39;At&#39;: -1, &#39;Rn&#39;: 0, &#39;Fr&#39;: 1, &#39;Ra&#39;: 2, &#39;Ac&#39;: 3, &#39;Th&#39;: 4,     &#39;Pa&#39;: 5, &#39;U&#39;: 6, &#39;Np&#39;: 5, &#39;Pu&#39;: 4, &#39;Am&#39;: 3, &#39;Cm&#39;: 3, &#39;Bk&#39;: 3, &#39;Cf&#39;: 3, &#39;Es&#39;: 3,     &#39;Fm&#39;: 3, &#39;Md&#39;: 3, &#39;No&#39;: 2, &#39;Lr&#39;: 3&#125;# structure.add_oxidation_state_by_guess()oxi_states_for_structure = &#123;    el.symbol: COMMON_OXIDATION_STATES.get(el.symbol)    for el in structure.composition.elements    if el.symbol in COMMON_OXIDATION_STATES&#125;structure.add_oxidation_state_by_element(oxi_states_for_structure)order = OrderDisorderedStructureTransformation()standard_structures=order.apply_transformation(structure,return_ranked_list=2)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d[&quot;structure&quot;] for d in standard_structures]) #这应该是个字典，把字典中的structure关键字提出来组&gt;成列表,groups里只剩下结构了#structures = [d[&quot;structure&quot;] for d in standard_structures]#print(&quot;%d structures returned.&quot; % len(structures))#from pymatgen.io.vasp.inputs import Poscar#n = 0#for Cry_Str in structures:    #print(Cry_Str)#    Vasp_Str = Poscar(Cry_Str)#    n = n+1#    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)</code></pre><h1 id="4-计算静电能"><a href="#4-计算静电能" class="headerlink" title="4.计算静电能"></a>4.计算静电能</h1><pre><code class="python">import osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.inputs import Poscarfrom pymatgen.analysis.energy_models import EwaldElectrostaticModelabc=[]data = &#123;&quot;Ca&quot;:-6, &quot;H&quot;:1&#125;for i in range(1,5034):    m = Structure.from_file(&#39;%d.vasp&#39;%i)    m.add_oxidation_state_by_element(data)    energy = EwaldElectrostaticModel().get_energy(m)    abc.append(energy)with open(&#39;energy.dat&#39;, &#39;w&#39;) as f:    for item in abc:        f.write(str(item) + &#39;\n&#39;)    </code></pre><h1 id="5-sqs"><a href="#5-sqs" class="headerlink" title="5. sqs"></a>5. sqs</h1><h2 id="使用pymatgen进行sqs操作"><a href="#使用pymatgen进行sqs操作" class="headerlink" title="使用pymatgen进行sqs操作"></a>使用pymatgen进行sqs操作</h2><p>需要安装好ATAT</p><pre><code class="python">from pymatgen.core import Structure,Compositionfrom pymatgen.transformations.advanced_transformations import SQSTransformationimport ospath_a = os.getcwd()print(path_a)os.makedirs(&#39;sqs_debug_folder&#39;, exist_ok=True)s_original = Structure.from_file(&quot;a.cif&quot;)new_species_list = []for site in s_original.sites:    current_species_dict = site.species.get_el_amt_dict()    total_occupancy = sum(current_species_dict.values())    if 0 &lt; total_occupancy &lt; 1.0:        vacancy_fraction = 1.0 - total_occupancy        current_species_dict[&#39;X&#39;] = vacancy_fraction    new_species_list.append(Composition(current_species_dict))s_for_sqs = Structure(s_original.lattice, new_species_list, s_original.frac_coords)trans = SQSTransformation(    scaling=[1,1,1],    sqs_method=&quot;mcsqs&quot;,    instances=1,    search_time=3,    cluster_size_and_shell = &#123;2: 2, 3: 1&#125;,    reduction_algo=False,    directory=&quot;./sqs_debug_folder&quot;)sqs_with_dummies = trans.apply_transformation(s_for_sqs)sqs_final = sqs_with_dummies.copy()sqs_final.remove_species([&quot;X&quot;])output_filepath = os.path.join(path_a, &#39;sqs_mcsqs.cif&#39;)sqs_final.to(fmt=&quot;cif&quot;, filename=output_filepath)</code></pre><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094118471.png" alt="image-20250928094118471"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094133086.png" alt="image-20250928094133086"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094148749.png" alt="image-20250928094148749"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094158907.png" alt="image-20250928094158907"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094215493.png" alt="image-20250928094215493"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094226129.png" alt="image-20250928094226129"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094237188.png" alt="image-20250928094237188"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094255582.png" alt="image-20250928094255582"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094310423.png" alt="image-20250928094310423"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094319531.png" alt="image-20250928094319531"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094331307.png" alt="image-20250928094331307"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250928094340922.png" alt="image-20250928094340922"></p><h2 id="1-前处理"><a href="#1-前处理" class="headerlink" title="1.前处理"></a>1.前处理</h2><h3 id="1-转换rndstr格式的结构"><a href="#1-转换rndstr格式的结构" class="headerlink" title="1. 转换rndstr格式的结构"></a><strong>1. 转换rndstr格式的结构</strong></h3><pre><code class="python">from pymatgen.core import Structurestructure = Structure.from_file(&quot;a.cif&quot;)structure.to(filename=&#39;rndstr.in&#39;)</code></pre><h3 id="2-提交多个ip的任务"><a href="#2-提交多个ip的任务" class="headerlink" title="2.提交多个ip的任务"></a><strong>2.提交多个ip的任务</strong></h3><pre><code class="shell">for i in $(seq 1 1 20)docat &gt; sqs_$i.slurm &lt;&lt; liz#!/bin/sh#SBATCH -J analyze#SBATCH -p xieyu#SBATCH -N 1#SBATCH -n 1#SBATCH -w c0406source activate lizmcsqs -n=36 -ip=$ilizsbatch sqs_$i.slurmdone#grep Objective_function bestcorr*.out|sort -n</code></pre><h2 id="2-后处理"><a href="#2-后处理" class="headerlink" title="2. 后处理"></a>2. 后处理</h2><h3 id="1-提取bestcorr"><a href="#1-提取bestcorr" class="headerlink" title="1. 提取bestcorr"></a><strong>1. 提取bestcorr</strong></h3><pre><code class="shell">for i in $(seq 1 1 12)dotail -n 1 bestcorr.out &gt;&gt; corr.datdone</code></pre><h3 id="2-对多任务ip产生的best-out文件处理"><a href="#2-对多任务ip产生的best-out文件处理" class="headerlink" title="2.对多任务ip产生的best.out文件处理"></a><strong>2.对多任务ip产生的best.out文件处理</strong></h3><p>&#x3D;&#x3D;<strong>注意，数字前的空格会对结构在vesta中的显示产生影响</strong>&#x3D;&#x3D; </p><blockquote><p><strong>sqs2poscar文件在 同步空间&#x2F;计算脚本&#x2F;sqs处</strong></p></blockquote><pre><code class="shell">for i in $(seq 1 1 12)do ./sqs2poscar bestsqs$i.outmv bestsqs$i.out-POSCAR $i.vaspsed -i &#39;2s/.*/1/&#39; $i.vaspsed -i &#39;3s/-10\.57524/ -10.57524/&#39; $i.vaspdone </code></pre><h3 id="3-检验相同的结构"><a href="#3-检验相同的结构" class="headerlink" title="3. 检验相同的结构"></a><strong>3. 检验相同的结构</strong></h3><pre><code class="python">import osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []for i in range(1,13):    filename = &quot;&#123;&#125;.vasp&quot;.format(i)    print(filename)    structure = Structure.from_file(filename)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])  #这应该是个字典，把字典中的structure关键字提出来组成列表,groups里只剩下结构了#structures = [d[&quot;structure&quot;] for d in standard_structures]#print(&quot;%d structures returned.&quot; % len(structures))#from pymatgen.io.vasp.inputs import Poscar#n = 0#for Cry_Str in structures:    #print(Cry_Str)#    Vasp_Str = Poscar(Cry_Str)#    n = n+1#    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)#产生等差数列 for i in range(len(groups))#提取出列举时计算的能量 energy = [x(&#39;energy&#39;) for x in standard_structures]#验证同一类群下的结构静电能相等#suoyoua = [x for x in groups[0]]#for cry in suoyoua:#   energy = EwaldElectrostaticModel().get_energy(cry)#  print(energy)#验证输出的顺序是按照能量升序进行的#suoyou1 = [x[0] for x in groups]#for cry in suoyou1:#    energy = EwaldElectrostaticModel().get_energy(cry)#    print(energy)</code></pre><h1 id="6-暴力枚举"><a href="#6-暴力枚举" class="headerlink" title="6.暴力枚举"></a>6.暴力枚举</h1><p>计算静电能使用ares的静电能模块，在海康nas的软件文件夹中有</p><h2 id="1-枚举脚本"><a href="#1-枚举脚本" class="headerlink" title="1.枚举脚本"></a>1.枚举脚本</h2><p>在替换多种元素时，可以只替换一种元素，然后在生成的poscar中直接修改元素种类和个数</p><pre><code class="python">#!/usr/bin/python2#Change the format of crystal structure#python Str_Format.py infile outfiletry:    from pymatgen.core.structure          import IStructure, Structure    from pymatgen.io.vasp.inputs          import Poscarexcept:    print (&#39;You should install pymatgen first.&#39;)    exit(0)from monty.io import zopenimport randomimport sys,os,timeimport numpy as npdef readvasp(infile):    with zopen(infile, &quot;rt&quot;) as f:        contents = f.read()    Cry_Str = IStructure.from_str(contents, fmt=&quot;poscar&quot;)    lattice = Cry_Str.lattice    species = [ x.species.elements[0] for x in Cry_Str.sites ]    coords  = [ x.coords for x in Cry_Str.sites ]    Cry_Str = Structure(lattice,species,coords,coords_are_cartesian=True)    return Cry_Strdef replace(Cry_Str, replace_list, outfile):    natom = len(Cry_Str.sites)    for ele in replace_list:        for i in ele[0]:            Cry_Str.replace(i,ele[1])    Cry_Str.sort()    Vasp_Str = Poscar(Cry_Str)    Vasp_Str.write_file(outfile)    returnif __name__ == &#39;__main__&#39;:    instr = &#39;a.vasp&#39;    Cry_Str = readvasp(instr)    #modify    Cry_Str.make_supercell([1,1,1])    for i in range(10):        Cry_tp = Cry_Str.copy()        re_index = np.arange(28,82)        np.random.shuffle(re_index)        re_index1 = re_index[:3]        re_index2 = re_index[7:9]        replace_list = [ [re_index1, &#39;B&#39;],[re_index2,&#39;O&#39;] ]        tmp = &#39;%d.vasp&#39;%i        replace(Cry_tp, replace_list, tmp)</code></pre><h2 id="2-多任务脚本-检验相同结构"><a href="#2-多任务脚本-检验相同结构" class="headerlink" title="2. 多任务脚本 检验相同结构"></a>2. 多任务脚本 检验相同结构</h2><pre><code class="shell">for i in $(seq 0 1000 25000)do cat &gt; job_$i.sh &lt;&lt; liz#!/bin/sh#SBATCH -J liz#SBATCH -p xieyu#SBATCH -N 1#SBATCH -n 1#SBATCH -w c0607source activate lizpython jianyan_$i.pylizcat &gt; jianyan_$i.py &lt;&lt; lizimport osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;/work/home/liz/workspace/D-LuNH-sqs/2-Lu-25电子/1-暴力破解/3-zuhe&#39;for i in range($i,$((i+1000))):    filename = &quot;&#123;&#125;.vasp&quot;.format(i)    if not os.path.exists(os.path.join(dir,filename)):        continue    print(filename)    structure = Structure.from_file(filename)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = $ifor Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)lizsbatch job_$i.shdone</code></pre><h2 id="3-改良版"><a href="#3-改良版" class="headerlink" title="3.改良版"></a>3.改良版</h2><h3 id="1-限定最小距离"><a href="#1-限定最小距离" class="headerlink" title="1.限定最小距离"></a>1.限定最小距离</h3><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurestrua = Structure.from_file(&quot;a.vasp&quot;)#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#a_frac_coords=strua.sites[re_index1[0]].frac_coords#b_frac_coords=strua.sites[re_index1[1]].frac_coords#c_frac_coords=strua.sites[re_index1[2]].frac_coords#arr1=a_frac_coords-b_frac_coords#arr1[arr1 &gt;= 0.5] = 1#arr1[arr1 &lt;= -0.5] = -1#arr1[np.logical_and(arr1 &gt; -0.5, arr1 &lt; 0.5)] = 0#print(arr1)#strua.sites[re_index1[1]].frac_coords +=arr1#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#diff1=np.sqrt(np.dot(strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords,strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords))#if diff1&lt;3#print(strua.sites[re_index1[1]].frac_coords)#h2=a_frac_coords-c_frac_coords#h3=b_frac_coords-c_frac_coords#lattice = strua.lattice#lattice_matrix =np.array(lattice.matrix)#coords  = [ x.coords for x in strua.sites ]#矩阵乘法 np.dot(A,B)#矩阵改变维数 A.reshape(1,3)#sites从0开始#np.dot(a,a)求平方和#strua.sites[0].coords 输出笛卡尔坐标 .frac_coords分数坐标#print(np.sqrt(np.dot(strua.sites[70].coords-strua.sites[71].coords,strua.sites[70].coords-strua.sites[71].coords)))#求两个原子的距离for i in range(20):    Cry_tp = strua.copy()    re_index = np.arange(27,82)    np.random.shuffle(re_index)    re_index1 = re_index[:3]    replace_list = [ [re_index1, &#39;N&#39;] ]    if Cry_tp.get_distance(re_index1[0],re_index1[1])&lt;3.6 or Cry_tp.get_distance(re_index1[0],re_index1[2])&lt;3.6 or Cry_tp.get_distance(re_index1[1],re_index1[2])&lt;3.6:        continue    for ele in replace_list:        for j in ele[0]:            Cry_tp.replace(j,ele[1])    Cry_tp.sort()    Vasp_Str = Poscar(Cry_tp)    Vasp_Str.write_file(&#39;%d.vasp&#39;%i)</code></pre><h3 id="2-限定最小距离且保证替代发生在不同的位置"><a href="#2-限定最小距离且保证替代发生在不同的位置" class="headerlink" title="2.限定最小距离且保证替代发生在不同的位置"></a>2.限定最小距离且保证替代发生在不同的位置</h3><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurestrua = Structure.from_file(&quot;a.vasp&quot;)#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#a_frac_coords=strua.sites[re_index1[0]].frac_coords#b_frac_coords=strua.sites[re_index1[1]].frac_coords#c_frac_coords=strua.sites[re_index1[2]].frac_coords#arr1=a_frac_coords-b_frac_coords#arr1[arr1 &gt;= 0.5] = 1#arr1[arr1 &lt;= -0.5] = -1#arr1[np.logical_and(arr1 &gt; -0.5, arr1 &lt; 0.5)] = 0#print(arr1)#strua.sites[re_index1[1]].frac_coords +=arr1#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#diff1=np.sqrt(np.dot(strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords,strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords))#if diff1&lt;3#print(strua.sites[re_index1[1]].frac_coords)#h2=a_frac_coords-c_frac_coords#h3=b_frac_coords-c_frac_coords#lattice = strua.lattice#lattice_matrix =np.array(lattice.matrix)#coords  = [ x.coords for x in strua.sites ]#矩阵乘法 np.dot(A,B)#矩阵改变维数 A.reshape(1,3)#sites从0开始#np.dot(a,a)求平方和#strua.sites[0].coords 输出笛卡尔坐标 .frac_coords分数坐标#print(np.sqrt(np.dot(strua.sites[70].coords-strua.sites[71].coords,strua.sites[70].coords-strua.sites[71].coords)))#求两个原子的距离for i in range(20):    Cry_tp = strua.copy()    re_index = np.arange(27,108)    np.random.shuffle(re_index)    re_index1 = re_index[:3]    if all(27&lt;= num &lt;=53 for num in re_index1):        continue    if all(54&lt;= num &lt;=107 for num in re_index1):        continue    replace_list = [ [re_index1, &#39;N&#39;] ]    if Cry_tp.get_distance(re_index1[0],re_index1[1])&lt;3.6 or Cry_tp.get_distance(re_index1[0],re_index1[2])&lt;3.6 or Cry_tp.get_distance(re_index1[1],re_index1[2])&lt;3.6:        continue    for ele in replace_list:        for j in ele[0]:            Cry_tp.replace(j,ele[1])    Cry_tp.sort()    Vasp_Str = Poscar(Cry_tp)    Vasp_Str.write_file(&#39;%d.vasp&#39;%i)</code></pre><h3 id="3-限定空间群"><a href="#3-限定空间群" class="headerlink" title="3.限定空间群"></a>3.限定空间群</h3><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerfrom pymatgen.analysis.structure_matcher import StructureMatcherstrua = Structure.from_file(&quot;a.vasp&quot;)#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#a_frac_coords=strua.sites[re_index1[0]].frac_coords#b_frac_coords=strua.sites[re_index1[1]].frac_coords#c_frac_coords=strua.sites[re_index1[2]].frac_coords#arr1=a_frac_coords-b_frac_coords#arr1[arr1 &gt;= 0.5] = 1#arr1[arr1 &lt;= -0.5] = -1#arr1[np.logical_and(arr1 &gt; -0.5, arr1 &lt; 0.5)] = 0#print(arr1)#strua.sites[re_index1[1]].frac_coords +=arr1#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#diff1=np.sqrt(np.dot(strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords,strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords))#if diff1&lt;3#print(strua.sites[re_index1[1]].frac_coords)#h2=a_frac_coords-c_frac_coords#h3=b_frac_coords-c_frac_coords#lattice = strua.lattice#lattice_matrix =np.array(lattice.matrix)#coords  = [ x.coords for x in strua.sites ]#矩阵乘法 np.dot(A,B)#矩阵改变维数 A.reshape(1,3)#sites从0开始#np.dot(a,a)求平方和#strua.sites[0].coords 输出笛卡尔坐标 .frac_coords分数坐标#print(np.sqrt(np.dot(strua.sites[70].coords-strua.sites[71].coords,strua.sites[70].coords-strua.sites[71].coords)))#求两个原子的距离total_structure=[]for i in range(200000):    Cry_tp = strua.copy()    re_index = np.arange(0,96)    np.random.shuffle(re_index)    re_index1 = re_index[:4]    Cry_tp.remove_sites(re_index1)    a=SpacegroupAnalyzer(Cry_tp).get_space_group_number()    if a&lt;= 140:      continue    print(i)    print(a)    Vasp_Str = Poscar(Cry_tp)    Vasp_Str.write_file(&#39;%d.vasp&#39;%i)    total_structure.append(Cry_tp)matcher = StructureMatcher()groups = matcher.group_structures([d for d in total_structure])groupstructures = [x[0] for x in groups]from pymatgen.io.vasp.inputs import Poscarn = 10001for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)</code></pre><h3 id="4-多任务做改良版"><a href="#4-多任务做改良版" class="headerlink" title="4.多任务做改良版"></a>4.多任务做改良版</h3><pre><code class="shell">for i in $(seq 1 100 10100)docat &gt; baoli_$i.py &lt;&lt; lizfrom pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.symmetry.analyzer import SpacegroupAnalyzerfrom pymatgen.analysis.structure_matcher import StructureMatcherstrua = Structure.from_file(&quot;a.vasp&quot;)#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#a_frac_coords=strua.sites[re_index1[0]].frac_coords#b_frac_coords=strua.sites[re_index1[1]].frac_coords#c_frac_coords=strua.sites[re_index1[2]].frac_coords#arr1=a_frac_coords-b_frac_coords#arr1[arr1 &gt;= 0.5] = 1#arr1[arr1 &lt;= -0.5] = -1#arr1[np.logical_and(arr1 &gt; -0.5, arr1 &lt; 0.5)] = 0#print(arr1)#strua.sites[re_index1[1]].frac_coords +=arr1#print(strua.sites[re_index1[1]])#print(strua.get_distance(re_index1[0],re_index1[1]))#diff1=np.sqrt(np.dot(strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords,strua.sites[re_index1[0]].coords-strua.sites[re_index1[1]].coords))#if diff1&lt;3#print(strua.sites[re_index1[1]].frac_coords)#h2=a_frac_coords-c_frac_coords#h3=b_frac_coords-c_frac_coords#lattice = strua.lattice#lattice_matrix =np.array(lattice.matrix)#coords  = [ x.coords for x in strua.sites ]#矩阵乘法 np.dot(A,B)#矩阵改变维数 A.reshape(1,3)#sites从0开始#np.dot(a,a)求平方和#strua.sites[0].coords 输出笛卡尔坐标 .frac_coords分数坐标#print(np.sqrt(np.dot(strua.sites[70].coords-strua.sites[71].coords,strua.sites[70].coords-strua.sites[71].coords)))#求两个原子的距离total_structure=[]for i in range(20000):    Cry_tp = strua.copy()    re_index = np.arange(0,96)    np.random.shuffle(re_index)    re_index1 = re_index[:4]    Cry_tp.remove_sites(re_index2)    a=SpacegroupAnalyzer(Cry_tp).get_space_group_number()    if a&lt;= 140:      continue    print(i)    print(a)    Vasp_Str = Poscar(Cry_tp)    Vasp_Str.write_file(&#39;%d.vasp&#39;%i)    total_structure.append(Cry_tp)matcher = StructureMatcher()groups = matcher.group_structures([d for d in total_structure])groupstructures = [x[0] for x in groups]from pymatgen.io.vasp.inputs import Poscarn = $((i))for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)lizdone</code></pre><h2 id="4-使用枚举方法处理蓝钻结构"><a href="#4-使用枚举方法处理蓝钻结构" class="headerlink" title="4.使用枚举方法处理蓝钻结构"></a>4.使用枚举方法处理蓝钻结构</h2><blockquote><ol><li>这是第一次系统使用暴力枚举法产生结构，其中，由于pymatgen用于计算ewald加和的程序太慢，使用了组内ARES中用于计算ewald能量的脚本ewalds.x</li><li>一共有两种方式产生，一种是完全随机的，另一种相当于对整个分子的替代（使用了pymatgen中找原子附近其他原子的功能</li><li>脚本、流程都经过了反复的改进</li></ol></blockquote><h3 id="1-完全随机的方法（完全解放双手了）"><a href="#1-完全随机的方法（完全解放双手了）" class="headerlink" title="1.完全随机的方法（完全解放双手了）"></a>1.完全随机的方法（完全解放双手了）</h3><blockquote><ol><li>一共需要四个文件</li></ol><p>a.vasp </p><p>multitask.sh</p><p>ewalds.x </p><p>changeatom.py:用于产生随机的结构</p><p>以及一个后处理文件</p><p>get-all.sh</p><p>2.分为三级别目录</p><p>一级： 1-21</p><p>二级： 1-48</p><p>三级： 1-random  2-low ；其中 1-random是随机产生的结构，并计算了静电能，2-low是能量最低的前30个结构以及对应的能量</p><ol start="3"><li>通过设置不同的价态，可以得到不同种原子靠近或者远离的结构</li></ol></blockquote><p>流程(产生1000万个随机结构）：</p><h4 id="1-提任务"><a href="#1-提任务" class="headerlink" title="1. 提任务"></a>1. 提任务</h4><p><code>for i in $(seq 1 1 10);do mkdir $i;cp a.vasp ewalds.x multitask.sh changeatom.py $i;cd $i; ./multitask.sh ;cd ..;done</code></p><h4 id="2-changeatom-py"><a href="#2-changeatom-py" class="headerlink" title="2. changeatom.py"></a>2. <code>changeatom.py</code></h4><p>需要更改的地方是产生结构的数量、以及替换的种类和位置</p><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelstrua = Structure.from_file(&quot;a.vasp&quot;)#需要考虑for i in range(10000):    print(i)    Cry_tp = strua.copy()    re_index = np.arange(0,512)    np.random.shuffle(re_index)    re_index1 = re_index[:7]    re_index2 = re_index[7:9]    replace_list = [ [re_index1, &#39;B&#39;],[re_index2,&#39;O&#39;] ]    for ele in replace_list:        for j in ele[0]:            Cry_tp.replace(j,ele[1])    Cry_tp.sort()    Vasp_Str = Poscar(Cry_tp)    Vasp_Str.write_file(&#39;%d.vasp&#39;%i)</code></pre><h4 id="3-multitask-sh-做一些文件夹-，提交产生结构的任务，提交计算ewald能量的任务"><a href="#3-multitask-sh-做一些文件夹-，提交产生结构的任务，提交计算ewald能量的任务" class="headerlink" title="3. multitask.sh :做一些文件夹 ，提交产生结构的任务，提交计算ewald能量的任务"></a>3. <code>multitask.sh</code> :做一些文件夹 ，提交产生结构的任务，提交计算ewald能量的任务</h4><p>需要注意的是，使用ewalds.x需要先source环境</p><pre><code class="shell">for i in $(seq 1 1 48)domkdir $icd $imkdir 1-random 2-lowcd ..cp changeatom.py $i/1-randomcp ewalds.x $i/1-randomcp a.vasp $i/1-randomcd $i/1-randomcat &gt; job.slurm &lt;&lt; liz#!/bin/sh#SBATCH  --job-name=opt #SBATCH  --output=log.out.%j#SBATCH  --error=log.err.%j#SBATCH  --partition=xieyu#SBATCH  --nodes=1#SBATCH  --ntasks=1#SBATCH  --cpus-per-task=1source activate lizpython changeatom.py &gt; log 2&gt;&amp;1lizsbatch job.slurmcd ..cd ..donesleep 1mfor i in $(seq 1 1 48)docd $i/1-randomcat &gt; ewaldenergy.slurm &lt;&lt;liz2#!/bin/sh#SBATCH  --job-name=opt #SBATCH  --output=log.out.%j#SBATCH  --error=log.err.%j#SBATCH  --partition=xieyu#SBATCH  --nodes=1#SBATCH  --ntasks=1#SBATCH  --cpus-per-task=1source /work/env/intel2020for j in \$(seq 0 1 9999);doecho -n \$j.vasp &gt;&gt; energy.dat./ewalds.x \$j.vasp 3 5 -2 &gt;&gt; energy.datdonesort -k3n energy.dat &gt; ewaldenergy.dathead -n 30 ewaldenergy.dat &gt; ../2-low/ewaldenergy.dathead -n 30 ewaldenergy.dat | awk &#39;&#123;print \$1&#125;&#39; | while read -r line; docp &quot;\$line&quot; ../2-low/doneliz2sbatch ewaldenergy.slurmcd ..cd ..done</code></pre><h4 id="4-后处理文件-get-all-sh-提取能量最低的结构"><a href="#4-后处理文件-get-all-sh-提取能量最低的结构" class="headerlink" title="4. 后处理文件 get-all.sh   提取能量最低的结构"></a>4. 后处理文件 <code>get-all.sh </code>  提取能量最低的结构</h4><pre><code class="shell">rm all-sorted.datfor i in $(seq 1 1 21)docd $ifor j in $(seq 1 1 48)docd $jcd 2-low#awk -v var=&quot;$j&quot; &#39;&#123; $1 = var &quot; &quot; $1 &#125; 1&#39; ewaldenergy.dat &gt; data.datawk -v var=&quot;$i&quot; -v var2=&quot;$j&quot; &#39;&#123; $1 = var &quot; &quot;  var2 &quot; &quot; $1 &#125; 1&#39; ewaldenergy.dat &gt; data.datcat data.dat &gt;&gt; ../../data.datrm data.datcd ..cd ..donecd ..donefor h in $(seq 1 1 21)docat $h/data.dat &gt;&gt; ./all.datrm $h/data.datdonesort -k5n all.dat  &gt; all-sorted.datrm all.datecho &#39;sucess1&#39;# 设置计数器count=1rm -r allmkdir all# 读取包含三列信息的文件head -n 100 all-sorted.dat | while read -r line; do  # 提取大目录、小目录和文件名  directory=$(echo &quot;$line&quot; | awk &#39;&#123;print $1&#125;&#39;)  subdirectory=$(echo &quot;$line&quot; | awk &#39;&#123;print $2&#125;&#39;)  filename=$(echo &quot;$line&quot; | awk &#39;&#123;print $3&#125;&#39;)  # 构建目标文件名  target_filename=&quot;$&#123;count&#125;_$&#123;directory&#125;_$&#123;subdirectory&#125;_$&#123;filename&#125;.vasp&quot;  # 复制文件到当前目录并重命名  cp &quot;$&#123;directory&#125;/$&#123;subdirectory&#125;/2-low/$&#123;filename&#125;&quot; &quot;./all/$&#123;target_filename&#125;&quot;  # 增加计数器  ((count++))doneecho &#39;sucess2&#39;</code></pre><h3 id="2-替换为B-3O和B-4O构型的方法（还需要手动做一些流程）"><a href="#2-替换为B-3O和B-4O构型的方法（还需要手动做一些流程）" class="headerlink" title="2. 替换为B_3O和B_4O构型的方法（还需要手动做一些流程）"></a>2. 替换为B_3O和B_4O构型的方法（还需要手动做一些流程）</h3><blockquote><ol><li>替换为特定构型的方法利用了pymatgen中寻找相邻原子的方法，基本的流程是</li></ol><p>  先随机找两个可能替换氧原子的位置</p><p>  检验氧原子的距离是否符合要求（在本例中设定两个氧原子不要靠的太近，以免产生O-B-O键</p><p>  查找两个位置附近的原子的位置</p><p>  替换附近原子为B</p><p>  检验相同结构</p><p>  计算静电能</p><ol start="2"><li>另一套流程是：(下文的流程)</li></ol><p>  先随机替换氧的位置</p><p>  检验氧原子距离</p><p>  检验氧的相同结构</p><p>  替换B</p><p>  检验相同结构</p><p>  计算静电能</p><p>  这样流程的好处是可以大大减少产生结构的数量，缺点是不能把算能量的步骤集成</p><ol start="3"><li>值得注意的一点是，在这次的产生结构的脚本中，是可以枚举O的所有可能情况的，所以最终结果是枚举的</li></ol></blockquote><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230612211658194.png" alt="image-20230612211658194"></p><h4 id="1-枚举氧的位置"><a href="#1-枚举氧的位置" class="headerlink" title="1. 枚举氧的位置"></a>1. 枚举氧的位置</h4><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsstrua = Structure.from_file(&quot;a.vasp&quot;)hh=np.arange(512)combinations_list1 = list(combinations(hh, 2))print(len(combinations_list1))combinations_array1 = np.array(combinations_list1)numberbb = 0numberaa = 0for ar in combinations_array1:    ara=strua.copy()    numberbb +=1    print(numberbb)    if ara.get_distance(ar[0],ar[1])&gt;11 or ara.get_distance(ar[0],ar[1])&lt;3:        continue    numberaa +=1    replace_list = [[ar,&#39;O&#39;] ]    for ele in replace_list:        for j in ele[0]:            ara.replace(j,ele[1])    ara.sort()    Vasp_Str = Poscar(ara)    Vasp_Str.write_file(&#39;%d.vasp&#39;% numberaa)    </code></pre><h4 id="2-检验相同的结构"><a href="#2-检验相同的结构" class="headerlink" title="2. 检验相同的结构"></a>2. 检验相同的结构</h4><p>需要多次分批次检验，否则太慢了，有一个多任务脚本和一个py脚本</p><p>在不同文件夹中多次进行多脚本计算，会比较方便</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20230612172111471.png" alt="image-20230612172111471"></p><p><code>cp  ../*new.vasp ./1-first</code></p><p>第一次检验</p><pre><code class="shell">for i in $(seq 0 1000 130000)docat &gt; job_$i.sh &lt;&lt; liz#!/bin/sh#SBATCH  --job-name=opt #SBATCH  --output=log.out.%j#SBATCH  --error=log.err.%j#SBATCH  --partition=xieyu#SBATCH  --nodes=1#SBATCH  --ntasks=1#SBATCH  --cpus-per-task=1source activate lizpython jianyan_$i.py &gt;&gt; jianyan_$i.dat 2&gt;&amp;1lizcat &gt; jianyan_$i.py &lt;&lt; lizimport osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/3-B3O-B4O/4-xinzuo/1-替换O/1-str&#39;for j in range($i,$((i+1000))):    filename = &quot;&#123;&#125;.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = $ifor Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)lizsbatch job_$i.shdone</code></pre><p>第二次检验</p><pre><code class="python">for i in $(seq 0 10000 130000)docat &gt; job_$i.sh &lt;&lt; liz#!/bin/sh#SBATCH  --job-name=opt #SBATCH  --output=log.out.%j#SBATCH  --error=log.err.%j#SBATCH  --partition=xieyu#SBATCH  --nodes=1#SBATCH  --ntasks=1#SBATCH  --cpus-per-task=1source activate lizpython jianyan_$i.py &gt;&gt; jianyan_$i.dat 2&gt;&amp;1lizcat &gt; jianyan_$i.py &lt;&lt; lizimport osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/3-B3O-B4O/4-xinzuo/2-去除相同结构/1-first&#39;for j in range($i,$((i+11000))):    filename = &quot;&#123;&#125;new.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = $ifor Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)lizsbatch job_$i.shdone      </code></pre><p>最后一步的检验</p><pre><code class="python">import osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/3-B3O-B4O/4-xinzuo/2-去除相同结构/2-new&#39;for j in range(124000):    filename = &quot;&#123;&#125;new.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)        </code></pre><h4 id="3-替换B原子"><a href="#3-替换B原子" class="headerlink" title="3. 替换B原子"></a>3. 替换B原子</h4><blockquote><p>先替换B4，再替换B3</p></blockquote><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsdir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/3-B3O-B4O/4-xinzuo/2-去除相同结构/2-new&#39;counter = 0for j in range(124000):    filename = &quot;&#123;&#125;new.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    o_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;O&quot;]    o_array = np.array(o_indices)    ara=structure.copy()    list1=ara.get_neighbors(ara[o_array[0]], r=2.0)    list2=ara.get_neighbors(ara[o_array[1]], r=2.0)    list1index=np.array([site.index for site in list1])    list2index=np.array([site.index for site in list2])    replace_list = [ [list1index, &#39;B&#39;] ]    for ele in replace_list:        for j in ele[0]:            ara.replace(j,ele[1])    combinations_list = list(combinations(list2index, 3))    combinations_array = np.array(combinations_list)    for br in combinations_array:        counter += 1        brb=ara.copy()        replace_listara = [ [br, &#39;B&#39;] ]        for ele in replace_listara:           for j in ele[0]:                 brb.replace(j,ele[1])        brb.sort()        Vasp_Str = Poscar(brb)        Vasp_Str.write_file(&#39;%d.vasp&#39;% counter)        print(counter)</code></pre><h4 id="4-再次检验相同结构"><a href="#4-再次检验相同结构" class="headerlink" title="4. 再次检验相同结构"></a>4. 再次检验相同结构</h4><pre><code class="python">import osfrom pymatgen.core import Structurefrom pymatgen.io.vasp.sets import batch_write_input, MPRelaxSetfrom pymatgen.io.vasp.inputs import Poscarstandard_structures= []dir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/3-B3O-B4O/4-xinzuo/3-替换B&#39;for j in range(500):    filename = &quot;&#123;&#125;.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    standard_structures.append(structure)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]  # 将嵌套列表的每个列表的第一个元素提取出来做成一个新列表from pymatgen.io.vasp.inputs import Poscarn = 0for Cry_Str in groupstructures:    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)</code></pre><h4 id="5-计算能量，整理结构"><a href="#5-计算能量，整理结构" class="headerlink" title="5. 计算能量，整理结构"></a>5. 计算能量，整理结构</h4><pre><code class="shell">rm energy.datfor i in $(seq 1 1 99);do echo -n $&#123;i&#125;new.vasp &gt;&gt; energy.dat./ewalds.x ../4-去除相同结构/$&#123;i&#125;new.vasp -1 0 2 &gt;&gt; energy.datdonesort -k3n energy.dat  &gt; sorted.datmkdir strcd strcount=1head -n 100 ../sorted.dat |while read -r line;dofilename=$(echo &quot;$line&quot; | awk &#39;&#123;print $1&#125;&#39;)target_filename=$&#123;count&#125;.vaspcp ../../4-去除相同结构/$&#123;filename&#125; ./$&#123;target_filename&#125;count=$((count+ 1))donecd ..</code></pre><h3 id="3-替换为B-2O和B-3O的方法"><a href="#3-替换为B-2O和B-3O的方法" class="headerlink" title="3. 替换为B_2O和B_3O的方法"></a>3. 替换为B_2O和B_3O的方法</h3><h4 id="1-枚举氧的结构和其他步骤"><a href="#1-枚举氧的结构和其他步骤" class="headerlink" title="1.枚举氧的结构和其他步骤"></a>1.枚举氧的结构和其他步骤</h4><h4 id="2-1-替换B原子-联合取代、完全枚举"><a href="#2-1-替换B原子-联合取代、完全枚举" class="headerlink" title="2-1. 替换B原子(联合取代、完全枚举)"></a>2-1. 替换B原子(联合取代、完全枚举)</h4><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsdir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/1-512-B-7-O-2/3-B2O-B3O/1-O-结构&#39;counter = 0filename = &quot;1new.vasp&quot;filepath = os.path.join(dir, filename)print(filename)structure = Structure.from_file(filepath)o_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;O&quot;]o_array = np.array(o_indices)c_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;C&quot;]c_array = np.array(c_indices)ara=structure.copy()list1=ara.get_neighbors(ara[o_array[0]], r=2.0)list2=ara.get_neighbors(ara[o_array[1]], r=2.0)list1index=np.array([site.index for site in list1])list2index=np.array([site.index for site in list2])combined_array = np.concatenate((list1index, list2index))c_array2 = np.setdiff1d(c_array, combined_array)combinations_list1 = list(combinations(list1index, 2))combinations_list2 = list(combinations(list2index, 3))combinations_list3 = list(combinations(c_array2, 2))array1 = np.array(combinations_list1)array2 = np.array(combinations_list2)array3 = np.array(combinations_list3)print(array3.shape)print(len(array1))print(len(array2))print(len(array3))for i in range(len(array1)):    for j in range(len(array2)):        for k in range(len(array3)):            brb = ara.copy()            counter += 1           # print(array1[i])           # print(array2[j])           # print(array3[k])            combined_array33=np.concatenate((array1[i], array2[j], array3[k]))            replace_listara = [ [combined_array33, &#39;B&#39;] ]            for ele in replace_listara:                for m in ele[0]:                   brb.replace(m,ele[1])            brb.sort()            Vasp_Str = Poscar(brb)            Vasp_Str.write_file(&#39;%d.vasp&#39;% counter)            print(counter)</code></pre><h4 id="2-2-先分类再替代"><a href="#2-2-先分类再替代" class="headerlink" title="2-2 先分类再替代"></a>2-2 先分类再替代</h4><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsdir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/1-512-B-7-O-2/3-B2O-B3O/1-O-结构&#39;counter = 0filename = &quot;1new.vasp&quot;filepath = os.path.join(dir, filename)print(filename)structure = Structure.from_file(filepath)o_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;O&quot;]o_array = np.array(o_indices)c_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;C&quot;]c_array = np.array(c_indices)ara=structure.copy()list1=ara.get_neighbors(ara[o_array[0]], r=2.0)list2=ara.get_neighbors(ara[o_array[1]], r=2.0)list1index=np.array([site.index for site in list1])list2index=np.array([site.index for site in list2])combined_array = np.concatenate((list1index, list2index))print(combined_array)c_array2 = np.setdiff1d(c_array, combined_array)combinations_list1 = list(combinations(list1index, 2))combinations_list2 = list(combinations(list2index, 3))combinations_list3 = list(combinations(c_array2, 2))array1 = np.array(combinations_list1)array2 = np.array(combinations_list2)array3 = np.array(combinations_list3)print(array3.shape)print(len(array1))print(len(array2))print(len(array3))standard_structures=[]for i in range(len(array1)):    for j in range(len(array2)):#        for k in range(len(array3)):            brb = ara.copy()           # print(array1[i])           # print(array2[j])           # print(array3[k])            combined_array33=np.concatenate((array1[i], array2[j]))#            print(combined_array33)            replace_listara = [ [combined_array33, &#39;B&#39;] ]            for ele in replace_listara:                for m in ele[0]:                   brb.replace(m,ele[1])            standard_structures.append(brb)from pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()groups = matcher.group_structures([d for d in standard_structures])print(len(groups))groupstructures = [x[0] for x in groups]for straaa in groupstructures:    for k in range(len(array3)):        copy2=straaa.copy()        counter += 1        replace_listara = [ [array3[k], &#39;B&#39;] ]        for ele in replace_listara:            for m in ele[0]:                copy2.replace(m,ele[1])        copy2.sort()        Vasp_Str = Poscar(copy2)        Vasp_Str.write_file(&#39;%d.vasp&#39;% counter)        print(counter)</code></pre><h4 id="2-3-每个结构随机取样特定数目的结构"><a href="#2-3-每个结构随机取样特定数目的结构" class="headerlink" title="2-3 每个结构随机取样特定数目的结构"></a>2-3 每个结构随机取样特定数目的结构</h4><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsdir=&#39;/work/home/liz/workspace/3-temporary-duty/2-blue/4-zhengshi/1-512-B-7-O-2/3-B2O-B3O/1-O-结构&#39;counter = 0for j in range(100):    filename = &quot;&#123;&#125;new.vasp&quot;.format(j)    filepath = os.path.join(dir, filename)    if not os.path.exists(filepath):        continue    print(filename)    structure = Structure.from_file(filepath)    o_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;O&quot;]    o_array = np.array(o_indices)    c_indices = [i for i, site in enumerate(structure) if site.specie.name == &quot;C&quot;]    c_array = np.array(c_indices)    ara=structure.copy()    list1=ara.get_neighbors(ara[o_array[0]], r=2.0)    list2=ara.get_neighbors(ara[o_array[1]], r=2.0)    list1index=np.array([site.index for site in list1])    list2index=np.array([site.index for site in list2])    combined_array = np.concatenate((list1index, list2index))    c_array2 = np.setdiff1d(c_array, combined_array)    combinations_list1 = list(combinations(list1index, 2))    combinations_list2 = list(combinations(list2index, 3))    combinations_list3 = list(combinations(c_array2, 2))    array1 = np.array(combinations_list1)    array2 = np.array(combinations_list2)    array3 = np.array(combinations_list3)    arr1=np.arange(len(array1))    arr2=np.arange(len(array2))    arr3=np.arange(len(array3))    for ww in range(5000):        np.random.shuffle(arr1)        np.random.shuffle(arr2)        np.random.shuffle(arr3)        brb = ara.copy()        counter += 1        combined_array33=np.concatenate((array1[arr1[1]], array2[arr2[1]], array3[arr3[1]]))        replace_listara = [ [combined_array33, &#39;B&#39;] ]        for ele in replace_listara:            for m in ele[0]:               brb.replace(m,ele[1])        brb.sort()        Vasp_Str = Poscar(brb)        Vasp_Str.write_file(&#39;%d.vasp&#39;% counter)        print(counter)</code></pre><h3 id="4-枚举-迭代器进行工作"><a href="#4-枚举-迭代器进行工作" class="headerlink" title="4.枚举+迭代器进行工作"></a>4.枚举+迭代器进行工作</h3><blockquote><p>可以是可以，但是现在太慢了，问题出在挨个去对比结构相似性，一个可行的方案是不断地细分到不同的结构特征集合，用结构特征做一个初筛</p></blockquote><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsimport itertoolsfrom pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()strua = Structure.from_file(&quot;a.vasp&quot;)hh=np.arange(16,112)combinations_generator = itertools.combinations(hh, 6)numberbb = 0numberaa = 0all_structure=[]for ar in combinations_generator:    ara=strua.copy()    numberbb +=1    print(numberbb)    remove_list = list(ar)    ara.remove_sites(remove_list)    ara.sort()    match_found =False    for aa in all_structure:        if matcher.fit(ara,aa):            match_found = True            break    if not match_found:        all_structure.append(ara)        print(&quot;%d structures &quot; %len(all_structure))        bb=0for aa in all_structure:        bb +=1    Vasp_Str = Poscar(aa)    Vasp_Str.write_file(&#39;%d.vasp&#39;% bb)    </code></pre><h3 id="5-枚举-使用静电能作为哈希值而不是fit"><a href="#5-枚举-使用静电能作为哈希值而不是fit" class="headerlink" title="5.枚举-使用静电能作为哈希值而不是fit"></a>5.枚举-使用静电能作为哈希值而不是fit</h3><blockquote><p>这一方法验证错误，因为两个不同的结构可能具有相同的静电能</p></blockquote><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsimport itertoolsfrom pymatgen.analysis.structure_matcher import StructureMatcherstrua = Structure.from_file(&quot;a.vasp&quot;)hh = np.arange(16, 112)combinations_generator = itertools.combinations(hh, 6)data = &#123;&quot;Ca&quot;:6, &quot;H&quot;:-1&#125;structure_dict = &#123;&#125;  # 用于存储已处理的结构strua.add_oxidation_state_by_element(data)print(strua)aaa= 0for ar in combinations_generator:    aaa +=1    print(aaa)    ara = strua.copy()    remove_list = list(ar)    ara.remove_sites(remove_list)    ara.sort()        # 计算结构的哈希值作为字典的键    ara_hash = EwaldElectrostaticModel().get_energy(ara)        # 使用字典进行匹配检查    if ara_hash not in structure_dict:        structure_dict[ara_hash] = ara        print(&quot;New structure added! Total structures: %d&quot; % len(structure_dict))        for Cry_Str in structure_dict.values():    #print(Cry_Str)    Vasp_Str = Poscar(Cry_Str)    n = n+1    Vasp_Str.write_file(&#39;%d.vasp&#39;%n)        </code></pre><h3 id="6-可以分步骤来做，先替换四个，再替换两个-！"><a href="#6-可以分步骤来做，先替换四个，再替换两个-！" class="headerlink" title="6. 可以分步骤来做，先替换四个，再替换两个 ！"></a>6. 可以分步骤来做，先替换四个，再替换两个 ！</h3><p>先替换1个，再替换一个，再替换一个，再，，，，这样一定是可以做的</p><p>从上往下试，直到能够使用程序做了就停止（95–92），如果从下往上试（92—95）会面临文件数太多的困难</p><h2 id="5-检验"><a href="#5-检验" class="headerlink" title="5.检验"></a>5.检验</h2><h3 id="1-逐个检验相似性"><a href="#1-逐个检验相似性" class="headerlink" title="1. 逐个检验相似性"></a>1. 逐个检验相似性</h3><pre><code class="python">import numpy as npimport sys,os,timefrom pymatgen.core import Structurefrom pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()all_str = []for i in range(1,100000):    file = f&#39;&#123;i&#125;.vasp&#39;    if not os.path.exists(file):       continue    print(file)    strua = Structure.from_file(file)    match_found = False    for str in all_str:        if matcher.fit(strua,str):           match_found = True           break    if not match_found:        all_str.append(strua)        print(&#39;%d structures&#39; % len(all_str))from pymatgen.io.vasp.inputs import Poscarn = 0for strua in all_str:    Vasp_Str = Poscar(strua)    n +=1    Vasp_Str.write_file(&#39;%dnew.vasp&#39;%n)</code></pre><h3 id="2-检验两个结构"><a href="#2-检验两个结构" class="headerlink" title="2. 检验两个结构"></a>2. 检验两个结构</h3><pre><code class="python">import numpy as npimport sys,os,timefrom pymatgen.core import Structurefrom pymatgen.analysis.structure_matcher import StructureMatchermatcher = StructureMatcher()aa = Structure.from_file(&#39;16.vasp&#39;)bb = Structure.from_file(&#39;4199.vasp&#39;)matcher.fit(aa,bb)</code></pre><h1 id="7-找出缺失的原子"><a href="#7-找出缺失的原子" class="headerlink" title="7.找出缺失的原子"></a>7.找出缺失的原子</h1><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom itertools import combinationsstru_b=Structure.from_file(filename=&#39;supercell.vasp&#39;)frac_coords_b=stru_b.frac_coords.tolist()normalized_coords_b = [[x % 1 for x in coord] for coord in frac_coords_b]set2 = set(map(tuple, normalized_coords_b))for i in range(1, 1000001):    file = f&#39;&#123;i&#125;.vasp&#39;    stru_a=Structure.from_file(file)    # 检查文件是否存在    if not os.path.exists(file):        continue    frac_coords_a=stru_a.frac_coords.tolist()    normalized_coords_a = [[x % 1 for x in coord] for coord in frac_coords_a]    set1 = set(map(tuple, normalized_coords_a))    result= [list(i) for i in set2.difference(set1)]    print(result)    atom_indices=[]    for coord in result:        try:            index = normalized_coords_b.index(coord)            atom_indices.append(index)        except ValueError:          # 如果找不到匹配的坐标，可以添加一个标记，如-1，以表示未匹配的坐标            atom_indices.append(-1)    close_atom_pairs = []    combinations_of_missing_atoms = combinations(atom_indices, 2)    threshold_distance = 7    for pair in combinations_of_missing_atoms:        distance = stru_b.get_distance(pair[0], pair[1])        if distance &lt; threshold_distance:            close_atom_pairs.append(pair)    # 如果 close_atom_pairs 列表不为空，则表示存在距离小于阈值的原子对    if close_atom_pairs:        print(&quot;存在距离小于阈值的原子对：&quot;)        print(len(close_atom_pairs))    else:        print(&quot;不存在距离小于阈值的原子对。&quot;)</code></pre><h1 id="7-处理CaH6的333倍胞的问题（将324个H中的14个扣掉-可以枚举的方法"><a href="#7-处理CaH6的333倍胞的问题（将324个H中的14个扣掉-可以枚举的方法" class="headerlink" title="7. 处理CaH6的333倍胞的问题（将324个H中的14个扣掉-可以枚举的方法"></a>7. 处理CaH6的333倍胞的问题（将324个H中的14个扣掉-可以枚举的方法</h1><p>&#x3D;&#x3D;&#x2F;work&#x2F;home&#x2F;liz&#x2F;workspace&#x2F;3-temporary-duty&#x2F;3-zwb&#x2F;6-333&#x2F;1-kou3&#x2F;2&#x2F;1-2&#x2F;ee.py&#x3D;&#x3D;</p><pre><code class="python">from pymatgen.core.structure import IStructure, Structurefrom pymatgen.io.vasp.inputs import Poscarfrom monty.io import zopenimport randomimport sys,os,timeimport numpy as npfrom pymatgen.core import Structurefrom pymatgen.analysis.energy_models import EwaldElectrostaticModelfrom itertools import combinationsstrua = Structure.from_file(&quot;a.vasp&quot;)H_indice = [i for i, site in enumerate(strua) if site.species_string == &quot;H&quot;]H_indices=np.array(H_indice)distanceall=strua.distance_matrixdistance=distanceall[H_indices][:,H_indices]print(distance.shape)print(distance)N = distance.shape[0]two_dim_matrix = np.zeros((N, N), dtype=int)for i in range(N):    for j in range(N):        dist1 = distance[i][j]        if dist1 &lt;= 3.6:            two_dim_matrix[i][j] = 0        else:            two_dim_matrix[i][j] = 1allnumber = 0matching_columns = np.where(two_dim_matrix[0, 1:] == 1)[0]matching_columns = matching_columns  + 1print(matching_columns)for j in matching_columns:    print(j)    matching_columns2=np.where(two_dim_matrix[j, j+1:] == 1)[0]    matching_columns2 = matching_columns2 + j + 1    matching_columns2 = np.intersect1d(matching_columns2,matching_columns)   # print(len(matching_columns2))    for jj in matching_columns2:        matching_columns3=np.where(two_dim_matrix[jj, jj+1:] == 1)[0]        matching_columns3 = matching_columns3 + jj + 1        matching_columns3 = np.intersect1d(matching_columns2,matching_columns3)      #  print(len(matching_columns3))        for jjj in matching_columns3:            matching_columns4=np.where(two_dim_matrix[jjj, jjj+1:] == 1)[0]            matching_columns4 = matching_columns4 + jjj + 1            matching_columns4 = np.intersect1d(matching_columns3,matching_columns4)           # print(len(matching_columns4))            for jjjj in matching_columns4:                matching_columnsddd=np.where(two_dim_matrix[jjjj, jjjj+1:] == 1)[0]                matching_columnsddd = matching_columnsddd + jjjj + 1                matching_columnsddd = np.intersect1d(matching_columns4,matching_columnsddd)               # print(len(matching_columnsddd))                for gg in matching_columnsddd:                    matching_columns5=np.where(two_dim_matrix[gg, gg+1:] == 1)[0]                    matching_columns5 = matching_columns5 + gg + 1                    matching_columns5 = np.intersect1d(matching_columnsddd,matching_columns5)                   # print(len(matching_columns5))                    for ggg in matching_columns5:                        matching_columns6=np.where(two_dim_matrix[ggg, ggg+1:] == 1)[0]                        matching_columns6 = matching_columns6 + ggg + 1                        matching_columns6 = np.intersect1d(matching_columns5,matching_columns6)                       # print(len(matching_columns6))                        for gggg in matching_columns6:                            matching_columns7=np.where(two_dim_matrix[gggg, gggg+1:] == 1)[0]                            matching_columns7 = matching_columns7 + gggg + 1                            matching_columns7 = np.intersect1d(matching_columns6,matching_columns7)                           # print(len(matching_columns7))                            for dd in matching_columns7:                                matching_columns8=np.where(two_dim_matrix[dd, dd+1:] == 1)[0]                                matching_columns8 = matching_columns8 + dd + 1                                matching_columns8 = np.intersect1d(matching_columns7,matching_columns8)                               # print(len(matching_columns8))                                for ddd in matching_columns8:                                    matching_columns9=np.where(two_dim_matrix[ddd, ddd+1:] == 1)[0]                                    matching_columns9 = matching_columns9 + ddd + 1                                    matching_columns9 = np.intersect1d(matching_columns8,matching_columns9)                                   # print(len(matching_columns9))                                    for dddd in matching_columns9:                                        matching_columns10=np.where(two_dim_matrix[dddd, dddd+1:] == 1)[0]                                        matching_columns10 = matching_columns10 + dddd + 1                                        matching_columns10 = np.intersect1d(matching_columns9,matching_columns10)                                       # print(len(matching_columns10))                                        for ww in matching_columns10:                                            matching_columns11=np.where(two_dim_matrix[ww, ww+1:] == 1)[0]                                            matching_columns11 = matching_columns11 + ww + 1                                            matching_columns11 = np.intersect1d(matching_columns10,matching_columns11)                                           # print(len(matching_columns11))                                            for www in matching_columns11:                                                matching_columns12=np.where(two_dim_matrix[www, www+1:] == 1)[0]                                                matching_columns12 = matching_columns12 + www + 1                                                matching_columns12 = np.intersect1d(matching_columns11,matching_columns12)                                               # print(len(matching_columns12))                                                for wwww in matching_columns12:                                                    matching_columns13=np.where(two_dim_matrix[wwww, wwww+1:] == 1)[0]                                                    matching_columns13 = matching_columns13 + wwww + 1                                                    listaaa=[0,j,jj,jjj,jjjj,gg,ggg,gggg,dd,ddd,dddd,ww,www,wwww]                                                   # print(listaaa)                                                    allnumber += 1                                                    #print(allnumber)                                                   # print(listaaa)                                                    if allnumber % 100000 == 0:                                                       print(allnumber)                                                       print(listaaa)                                                       listbbb= H_indices[listaaa]                                                       print(listbbb)                                                       ara=strua.copy()                                                       for ele in listbbb:                                                           ara.replace(ele,&#39;O&#39;)                                                       ara.sort()                                                       Vasp_Str = Poscar(ara)                                                       Vasp_Str.write_file(&#39;%d.vasp&#39;% allnumber)                                                        #                                                   for wwwww in matching_columns14:#                                                        matching_columns15=np.where(two_dim_matrix[wwwww, wwwww+1:] == 1)[0] #                                                       matching_columns15 = matching_columns15 + wwwww + 1</code></pre><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231028104117829.png" alt="image-20231028104117829"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231028104128748.png" alt="image-20231028104128748"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231028104147185.png" alt="image-20231028104147185"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20231028113559666.png" alt="image-20231028113559666"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量化操作&quot;&gt;&lt;a href=&quot;#1-自动计算一个分数占据结构的最佳扩胞方式并进行扩胞，可以与下面sqs的pymatgen脚本连用进行批量</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>bader电荷分析</title>
    <link href="http://example.com/2025/02/17/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/bader%E7%94%B5%E8%8D%B7%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2025/02/17/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/bader%E7%94%B5%E8%8D%B7%E5%88%86%E6%9E%90/</id>
    <published>2025-02-17T06:21:06.000Z</published>
    <updated>2025-02-17T06:59:41.382Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><p><a href="https://theory.cm.utexas.edu/henkelman/code/bader/">https://theory.cm.utexas.edu/henkelman/code/bader/</a></p><p>VASP从入门到入土：Bader电荷的计算 - 知乎<br><a href="https://zhuanlan.zhihu.com/p/673557738">https://zhuanlan.zhihu.com/p/673557738</a></p><p><code> LAECHG=.TRUE.</code></p><pre><code class="shell"> chgsum.pl AECCAR0 AECCAR2 bader CHGCAR -ref CHGCAR_sum</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://theory.cm.utexas.edu/henkelman/code/bader/&quot;&gt;https://theory.cm.utexas.edu/henkelman/code/bader/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;V</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>功函数计算</title>
    <link href="http://example.com/2025/02/11/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%8A%9F%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/"/>
    <id>http://example.com/2025/02/11/2-%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/%E5%8A%9F%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97/</id>
    <published>2025-02-11T03:18:17.000Z</published>
    <updated>2025-02-11T06:59:46.385Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-参考vaspkit的Au111表面功函数计算"><a href="#1-参考vaspkit的Au111表面功函数计算" class="headerlink" title="1. 参考vaspkit的Au111表面功函数计算"></a>1. 参考vaspkit的Au111表面功函数计算</h1><blockquote><p><a href="https://vaspkit.com/tutorials.html#potential-related">https://vaspkit.com/tutorials.html#potential-related</a></p><p><a href="http://bbs.keinsci.com/thread-27563-1-1.html">http://bbs.keinsci.com/thread-27563-1-1.html</a></p></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-参考vaspkit的Au111表面功函数计算&quot;&gt;&lt;a href=&quot;#1-参考vaspkit的Au111表面功函数计算&quot; class=&quot;headerlink&quot; title=&quot;1. 参考vaspkit的Au111表面功函数计算&quot;&gt;&lt;/a&gt;</summary>
      
    
    
    
    <category term="模拟方法" scheme="http://example.com/categories/%E6%A8%A1%E6%8B%9F%E6%96%B9%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>大模型</title>
    <link href="http://example.com/2025/01/21/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2025/01/21/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%A4%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2025-01-21T13:22:55.000Z</published>
    <updated>2025-06-16T12:41:21.611Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-原子大模型排名"><a href="#1-原子大模型排名" class="headerlink" title="1. 原子大模型排名"></a>1. 原子大模型排名</h1><p><a href="https://matbench-discovery.materialsproject.org/">https://matbench-discovery.materialsproject.org/</a></p><h1 id="2-使用ASE结合mattersim进行结构优化、分子动力学"><a href="#2-使用ASE结合mattersim进行结构优化、分子动力学" class="headerlink" title="2. 使用ASE结合mattersim进行结构优化、分子动力学"></a>2. 使用ASE结合mattersim进行结构优化、分子动力学</h1><h2 id="1-使用ASE进行包括晶胞的结构优化"><a href="#1-使用ASE进行包括晶胞的结构优化" class="headerlink" title="1. 使用ASE进行包括晶胞的结构优化"></a>1. 使用ASE进行包括晶胞的结构优化</h2><pre><code class="python">from mattersim.applications.moldyn import MolecularDynamicsfrom ase.io import read, writefrom mattersim.forcefield import MatterSimCalculatorfrom mattersim.applications.relax import Relaxerimport numpy as npfrom ase.optimize import LBFGSfrom ase.filters import ExpCellFilterfrom ase.io import Trajectory# calc = MatterSimCalculator(device=&#39;cuda&#39;)# calc = MatterSimCalculator(device=&#39;cuda:0&#39;)calc = MatterSimCalculator(load_path=&quot;MatterSim-v1.0.0-5M.pth&quot;, device=&#39;cuda:0&#39;)#relaxer = Relaxer(#    optimizer=&quot;BFGS&quot;, # the optimization method FIRE, BFGS#    filter=None, # filter to apply to the cell#    constrain_symmetry=True, # whether to constrain the symmetry#)atoms = read(&quot;POSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcecf = ExpCellFilter(atoms)traj = Trajectory(&#39;optimization.traj&#39;, &#39;w&#39;, atoms)#converged, relaxed_structure = relaxer.relax_structures(atoms,optimizer=&quot;BFGS&quot;, # the optimization method FIRE, BFGS#    filter=None, #    constrain_symmetry=True, fmax=0.01 )opt = LBFGS(ecf)opt.attach(traj.write, interval=1)opt.run(fmax=0.01) write(&quot;POSCAR.vasp&quot;, atoms, format=&quot;vasp&quot;)</code></pre><h2 id="2-from-wangq"><a href="#2-from-wangq" class="headerlink" title="2. from wangq"></a>2. from wangq</h2><pre><code class="python">from mattersim.applications.moldyn import MolecularDynamicsfrom ase.io import read, writefrom mattersim.forcefield import MatterSimCalculatorfrom mattersim.applications.relax import Relaxerimport numpy as np# calc = MatterSimCalculator(device=&#39;cuda&#39;)calc = MatterSimCalculator(load_path=&quot;MatterSim-v1.0.0-5M.pth&quot;, device=&#39;cuda:2&#39;)#默认是1M参数大模型，下面的是5M的relaxer = Relaxer(    optimizer=&quot;BFGS&quot;, # the optimization method FIRE, BFGS    filter=None, # filter to apply to the cell    constrain_symmetry=True, # whether to constrain the symmetry)atoms = read(&quot;SPOSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcconverged, relaxed_structure = relaxer.relax_structures(atoms,optimizer=&quot;FIRE&quot;, # the optimization method FIRE, BFGS    filter=None,     constrain_symmetry=True, fmax=0.1 )# # 获取晶胞矩阵# cell =   relaxed_structure.get_cell()# # 尝试使用 Cholesky 分解将晶胞矩阵转换为上三角矩阵# try:#     # 确保晶胞矩阵是正定的#     upper_triangular = np.linalg.cholesky(cell.dot(cell.T)).T# except np.linalg.LinAlgError:#     # 如果 Cholesky 分解失败，使用 QR 分解#     _, upper_triangular = np.linalg.qr(cell)# # 更新   relaxed_structure 对象的晶胞矩阵# relaxed_structure.set_cell(upper_triangular, scale_atoms=True)relaxed_structure.calc = calcensemble = &quot;NVT_BERENDSEN&quot;temperature = 1000timestep = 1.0taut = 100trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;nvt_runner = MolecularDynamics(atoms=relaxed_structure, ensemble=ensemble, temperature=temperature, timestep=timestep, taut=taut, trajectory=trajectory, logfile = logfile)nvt_runner.run(1000000)</code></pre><h2 id="3-处理vaspkit-的MSD-dat得到电导率"><a href="#3-处理vaspkit-的MSD-dat得到电导率" class="headerlink" title="3. 处理vaspkit 的MSD.dat得到电导率"></a>3. 处理vaspkit 的MSD.dat得到电导率</h2><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import linregressdef dealMSD(inputfile):    data = np.loadtxt(inputfile,skiprows = 1 )    data[:,0] = data[:,0] / 1000    n = data.shape[0]    start_idx = int(n*0.05)    end_idx = int(n*0.95)    time = data[start_idx:end_idx,0]    MSD = data[start_idx:end_idx,4]    slope, intercept, r_value, _, _ = linregress(time, MSD)    MSD_fit = slope * time + intercept        fig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)    ax1.plot(data[:, 0], data[:, 4],             label=&#39;600 K&#39;,             linewidth=2,          # 更粗的线条            color=&#39;#2ecc71&#39;,      # 柔和的绿色            alpha=0.8)      ax1.plot(time, MSD_fit, label=f&#39;Fit: y = &#123;slope:.2f&#125;x + &#123;intercept:.2f&#125;&#39;, color=&#39;red&#39;)    ax1.set_xlabel(&#39;t (ps)&#39;)    ax1.set_ylabel(r&#39;MSD ($\AA^2$)&#39;)    ax1.legend()    ax1.grid(True)    output_filename = inputfile.replace(&#39;/&#39;,&#39;-&#39;) + &#39;.png&#39;    plt.savefig(output_filename, bbox_inches=&#39;tight&#39;)    plt.close()#    plt.show()    return slopediffD = []temp = [600, 800,1000]for i in temp:    inputfile = f&quot;&#123;i&#125;/msd/MSD.dat&quot;    try:        slope = dealMSD(inputfile)        diffuD = slope / 60000        diffD.append(diffuD)    except FileNotFoundError:        print(f&quot;文件 &#123;inputfile&#125; 不存在&quot;)        diffD.append(0)logdiffD = np.log10(diffD)temp_inv = np.array([1000 / i for i in temp]) slope_D, intercept_D, _, _, _ = linregress(temp_inv, logdiffD)temp_inv_fit = np.linspace(1, 3.3, 100)logDfit = slope_D * temp_inv_fit + intercept_D#logDfit = slope_D * temp_inv + intercept_Dfig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)ax1.plot(temp_inv_fit, logDfit, label=f&#39;Fit: y = &#123;slope_D:.2f&#125;x + &#123;intercept_D:.2f&#125;&#39;, color=&#39;red&#39;)ax1.scatter(temp_inv, logdiffD,color = &#39;blue&#39;,marker = &#39;o&#39;)ax1.set_xlabel(&#39;1000/T (1/K)&#39;)ax1.set_ylabel(r&#39;log10(D) (Å$^2$/ps)&#39;)ax1.set_xlim(0.9, 3.5)ax1.set_ylim(slope_D * 3.5 + intercept_D, slope_D * 0.9 + intercept_D+ 0.3)y_min = slope_D * 3.5 + intercept_Dy_max = slope_D * 0.9 + intercept_D + 0.3y_ticks = np.arange(np.floor(y_min / 0.5) * 0.5, np.ceil(y_max / 0.5) * 0.5, 0.5)ax1.set_yticks(y_ticks)ax1.legend()ax1.grid(True)plt.savefig(&#39;arrnius.png&#39;,bbox_inches=&#39;tight&#39;)from pymatgen.core import Structurestructure = Structure.from_file(&quot;POSCAR&quot;)volume = structure.volumeatomsnumber = 30Eanumber = 0.198691478T = 300logD300 = slope_D * (1000 / T) + intercept_DD = 10 ** logD300sigma300 = 1.858 * atomsnumber / volume / 300 * (10**12) * DEa = -Eanumber * slope_Dprint(Ea, sigma300)</code></pre><h2 id="4-mattersim-MD直接输出MSD"><a href="#4-mattersim-MD直接输出MSD" class="headerlink" title="4. mattersim-MD直接输出MSD"></a>4. mattersim-MD直接输出MSD</h2><pre><code class="python">from mattersim.applications.moldyn import MolecularDynamicsfrom ase.io import read, writefrom mattersim.forcefield import MatterSimCalculatorfrom mattersim.applications.relax import Relaxerimport numpy as npimport os# calc = MatterSimCalculator(device=&#39;cuda&#39;)# calc = MatterSimCalculator(device=&#39;cuda:0&#39;)calc = MatterSimCalculator(load_path=&quot;MatterSim-v1.0.0-1M.pth&quot;, device=&#39;cuda:0&#39;)relaxer = Relaxer(    optimizer=&quot;BFGS&quot;, # the optimization method FIRE, BFGS    filter=None, # filter to apply to the cell    constrain_symmetry=True, # whether to constrain the symmetry)atoms = read(&quot;POSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcconverged, relaxed_structure = relaxer.relax_structures(atoms,optimizer=&quot;BFGS&quot;, # the optimization method FIRE, BFGS    filter=None,     constrain_symmetry=True, fmax=0.01 )# # 获取晶胞矩阵# cell =   relaxed_structure.get_cell()# # 尝试使用 Cholesky 分解将晶胞矩阵转换为上三角矩阵# try:#     # 确保晶胞矩阵是正定的#     upper_triangular = np.linalg.cholesky(cell.dot(cell.T)).T# except np.linalg.LinAlgError:#     # 如果 Cholesky 分解失败，使用 QR 分解#     _, upper_triangular = np.linalg.qr(cell)# # 更新   relaxed_structure 对象的晶胞矩阵# relaxed_structure.set_cell(upper_triangular, scale_atoms=True)relaxed_structure.calc = calc# import torch# torch.cuda.empty_cache() ensemble = &quot;NVT_BERENDSEN&quot;temperature = 600timestep = 2.0taut = 100trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;  nvt_runner = MolecularDynamics(atoms=relaxed_structure, ensemble=ensemble, temperature=temperature, timestep=timestep, taut=taut, trajectory=trajectory, loginterval=1,logfile = logfile)nvt_runner.run(250000)#将轨迹写入XDATCARatoms_xda = read(&#39;atoms.traj&#39;, index=&#39;:&#39;)write(&#39;XDATCAR&#39;, atoms_xda)os.remove(&#39;atoms.traj&#39;)import numpy as npimport matplotlibmatplotlib.use(&quot;Agg&quot;)import matplotlib.pyplot as pltinput_file = &#39;atoms.log&#39;data = np.loadtxt(input_file,skiprows=1)fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,6),dpi=100)ax1.plot(data[:, 0], data[:, 1],          label=&#39;t-Energy&#39;,          linewidth=2,          # 更粗的线条         color=&#39;#2ecc71&#39;,      # 柔和的绿色         alpha=0.8)            # 轻微的透明度ax1.set_xlabel(&#39;t (ps)&#39;)ax1.set_ylabel(&#39;Energy (eV)&#39;)ax1.set_title(&#39;t-Energy&#39;)ax1.legend()ax1.grid(True)ax2.plot(data[:, 0], data[:, 4],          label=&#39;t-Temperature&#39;,          linewidth=2,          color=&#39;#e74c3c&#39;,      # 柔和的红色         alpha=0.8)ax2.set_xlabel(&#39;t (ps)&#39;)ax2.set_ylabel(&#39;Temperature (K)&#39;)ax2.set_title(&#39;t-Energy&#39;)ax2.legend()ax2.grid(True)fig.tight_layout()plot_file = &#39;t-E-T.png&#39;fig.savefig(plot_file, dpi=300, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;)#vaspkit处理import subprocessdef run_vaspkit(task_id, selectype, element, skip_steps, frame_interval):    # 构造VASPKIT命令    command = f&quot;(echo &#123;task_id&#125;;echo &#123;selectype&#125;;echo &#123;element&#125;;echo &#123;skip_steps&#125;;echo &#123;frame_interval&#125;)| vaspkit&quot;    # 执行命令    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)    stdout, stderr = process.communicate()    # 输出结果    if process.returncode == 0:        print(&quot;VASPKIT执行成功！&quot;)        print(stdout.decode())    else:        print(&quot;VASPKIT执行失败！&quot;)        print(stderr.decode())# 示例调用task_id = 722selectype = 1element = &quot;Na&quot;skip_steps = 5000frame_interval = 1run_vaspkit(task_id, selectype, element, skip_steps, frame_interval)if os.path.exists(&#39;MSD.dat&#39;):    os.remove(&#39;XDATCAR&#39;)</code></pre><h2 id="5-mattersim使用Nose-Hoover"><a href="#5-mattersim使用Nose-Hoover" class="headerlink" title="5. mattersim使用Nose-Hoover"></a>5. mattersim使用Nose-Hoover</h2><pre><code class="python">from ase.io import read, writefrom ase.optimize import BFGSfrom ase.md.nose_hoover_chain import NoseHooverChainNVTfrom ase import unitsimport numpy as npimport osfrom ase.md.velocitydistribution import (     MaxwellBoltzmannDistribution,    Stationary,)from mattersim.forcefield import MatterSimCalculatorcalc = MatterSimCalculator(load_path=&quot;MatterSim-v1.0.0-1M.pth&quot;, device=&#39;cuda:0&#39;)atoms = read(&quot;POSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcdyn = BFGS(atoms)dyn.run(fmax=0.01)write(&quot;optimized_structure.vasp&quot;, atoms) temperature = 600timestep = 2.0 * units.fstdamp = 100trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;  loginterval = 1append_trajectory = FalseMaxwellBoltzmannDistribution(atoms,temperature_K=temperature,force_temp = True)Stationary(atoms)md1 = NoseHooverChainNVT(atoms=atoms,timestep=timestep,temperature_K = temperature,tdamp=tdamp,trajectory = None,logfile=&#39;equili.log&#39;,loginterval= loginterval,append_trajectory=append_trajectory)md1.run(5000)md2= NoseHooverChainNVT(atoms=atoms,timestep=timestep,temperature_K = temperature,tdamp=tdamp,trajectory = trajectory,logfile=logfile,loginterval= loginterval,append_trajectory=append_trajectory)md2.run(5000)#将轨迹写入XDATCARatoms_xda = read(&#39;atoms.traj&#39;, index=&#39;:&#39;)write(&#39;XDATCAR&#39;, atoms_xda)os.remove(&#39;atoms.traj&#39;)import numpy as npimport matplotlibmatplotlib.use(&quot;Agg&quot;)import matplotlib.pyplot as pltinput_file = &#39;atoms.log&#39;data = np.loadtxt(input_file,skiprows=1)fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,6),dpi=100)ax1.plot(data[:, 0], data[:, 1],          label=&#39;t-Energy&#39;,          linewidth=2,          # 更粗的线条         color=&#39;#2ecc71&#39;,      # 柔和的绿色         alpha=0.8)            # 轻微的透明度ax1.set_xlabel(&#39;t (ps)&#39;)ax1.set_ylabel(&#39;Energy (eV)&#39;)ax1.set_title(&#39;t-Energy&#39;)ax1.legend()ax1.grid(True)ax2.plot(data[:, 0], data[:, 4],          label=&#39;t-Temperature&#39;,          linewidth=2,          color=&#39;#e74c3c&#39;,      # 柔和的红色         alpha=0.8)ax2.set_xlabel(&#39;t (ps)&#39;)ax2.set_ylabel(&#39;Temperature (K)&#39;)ax2.set_title(&#39;t-Energy&#39;)ax2.legend()ax2.grid(True)fig.tight_layout()plot_file = &#39;t-E-T.png&#39;fig.savefig(plot_file, dpi=300, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;)#vaspkit处理import subprocessdef run_vaspkit(task_id, selectype, element, skip_steps, frame_interval):    # 构造VASPKIT命令    command = f&quot;(echo &#123;task_id&#125;;echo &#123;selectype&#125;;echo &#123;element&#125;;echo &#123;skip_steps&#125;;echo &#123;frame_interval&#125;)| vaspkit&quot;    # 执行命令    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)    stdout, stderr = process.communicate()    # 输出结果    if process.returncode == 0:        print(&quot;VASPKIT执行成功！&quot;)        print(stdout.decode())    else:        print(&quot;VASPKIT执行失败！&quot;)        print(stderr.decode())# 示例调用task_id = 722selectype = 1element = &quot;Na&quot;skip_steps = 5000frame_interval = 1run_vaspkit(task_id, selectype, element, skip_steps, frame_interval)if os.path.exists(&#39;MSD.dat&#39;):    os.remove(&#39;XDATCAR&#39;)</code></pre><h2 id="6-计算MSD-处理equili-log-atoms-log"><a href="#6-计算MSD-处理equili-log-atoms-log" class="headerlink" title="6. 计算MSD 处理equili.log &amp; atoms.log"></a>6. 计算MSD 处理equili.log &amp; atoms.log</h2><pre><code class="python">import matplotlib.pyplot as pltimport matplotlibimport numpy as npimport os from matplotlib import cmfrom scipy.stats import linregressmatplotlib.use(&#39;Agg&#39;)def ET(input_file, ax1,ax2):    data = np.loadtxt(input_file,skiprows=1)    ax1.plot(data[:, 0], data[:, 1],             label=&#39;t-Energy&#39;,             linewidth=2,          # 更粗的线条            color=&#39;#2ecc71&#39;,      # 柔和的绿色            alpha=0.8)            # 轻微的透明度    ax1.set_xlabel(&#39;t (ps)&#39;)    ax1.set_ylabel(&#39;Energy (eV)&#39;)    ax1.set_title(&#39;t-Energy&#39;)    ax1.legend()    ax1.grid(True)    ax2.plot(data[:, 0], data[:, 4],             label=&#39;t-Temperature&#39;,             linewidth=2,             color=&#39;#e74c3c&#39;,      # 柔和的红色            alpha=0.8)    ax2.set_xlabel(&#39;t (ps)&#39;)    ax2.set_ylabel(&#39;Temperature (K)&#39;)    ax2.legend()    ax2.grid(True)def plottem(tem):    data1 = np.loadtxt(os.path.join(base_folder, str(tem), &#39;MSD.dat&#39;))    data10 = data1[:, [0,4]]    x = data10[:, 0] / 1000   # Convert time to ps and scale by 2 (example transformation)    y_data = data10[:, 1]    return x, y_datadef MSD_T(ax5):    colors = cm.tab10    # Plot data for each temperature    for tem in tem_list:        x, y_data = plottem(tem)        n = x.shape[0]        start_idx = int(n*0.05)        end_idx = int(n*0.95)        time = x[start_idx:end_idx]        MSD = y_data[start_idx:end_idx]        slope, intercept, r_value, _, _ = linregress(time, MSD)        MSD_fit = slope * time + intercept        color = colors.colors[int(tem /100)-4]        ax5.plot(x, y_data,color=color, label=f&#39;&#123;tem&#125; K&#39;,linewidth=2)        ax5.plot(time, MSD_fit, color=&#39;red&#39;)        ax5.set_xlabel(&#39;Time (ps)&#39;,fontsize=20, fontweight=&#39;bold&#39;)        ax5.set_ylabel(r&#39;MSD ($\mathrm&#123;\AA&#125;^2$)&#39;,fontsize=20,fontweight=&#39;bold&#39;)        ax5.legend(prop=&#123;&#39;weight&#39;:&#39;bold&#39;,&#39;size&#39;:16&#125;)def dealMSD(inputfile):    data = np.loadtxt(inputfile,skiprows = 1 )    data[:,0] = data[:,0] / 1000     n = data.shape[0]    start_idx = int(n*0.05)    end_idx = int(n*0.95)    time = data[start_idx:end_idx,0]    MSD = data[start_idx:end_idx,4]    slope, intercept, r_value, _, _ = linregress(time, MSD)    MSD_fit = slope * time + intercept    return slopedef arrniums(ax6):    diffD = []    for i in tem_list:        inputfile = os.path.join(base_folder,str(i),&#39;MSD.dat&#39;)        try:            slope = dealMSD(inputfile)            diffuD = slope / 60000            diffD.append(diffuD)        except FileNotFoundError:            print(f&quot;文件 &#123;inputfile&#125; 不存在&quot;)            diffD.append(0)    logdiffD = np.log10(diffD)    temp_inv = np.array([1000 / i for i in tem_list])     slope_D, intercept_D, _, _, _ = linregress(temp_inv, logdiffD)    temp_inv_fit = np.linspace(0.7, 3.3, 100)    logDfit = slope_D * temp_inv_fit + intercept_D    #logDfit = slope_D * temp_inv + intercept_D    ax6.plot(temp_inv_fit, logDfit, label=f&#39;Fit: y = &#123;slope_D:.2f&#125;x + &#123;intercept_D:.2f&#125;&#39;, color=&#39;red&#39;)    ax6.scatter(temp_inv, logdiffD,color = &#39;blue&#39;,marker = &#39;o&#39;)    ax6.set_xlabel(&#39;1000/T (1/K)&#39;)    ax6.set_ylabel(r&#39;log10(D) (Å$^2$/ps)&#39;)    ax6.set_xlim(0.6, 3.5)    ax6.set_ylim(slope_D * 3.5 + intercept_D, slope_D * 0.9 + intercept_D+ 0.3)    y_min = slope_D * 3.5 + intercept_D    y_max = slope_D * 0.7 + intercept_D + 0.3    y_ticks = np.arange(np.floor(y_min / 0.5) * 0.5, np.ceil(y_max / 0.5) * 0.5, 0.5)    ax6.set_yticks(y_ticks)    ax6.legend()    ax6.grid(True)    # plt.savefig(&#39;arrnius.png&#39;,bbox_inches=&#39;tight&#39;)    from pymatgen.core import Structure    structure = Structure.from_file(&quot;POSCAR&quot;)    num_atoms = structure.num_sites    if num_atoms == 132:        volume = structure.volume        atomsnumber = 30        Eanumber = 0.198691478        T = 300        logD300 = slope_D * (1000 / T) + intercept_D        D = 10 ** logD300        sigma300 = 1.858 * atomsnumber / volume / 300 * (10**12) * D        Ea = -Eanumber * slope_D        print(Ea, sigma300)        text_str = f&#39;Ea = &#123;Ea:.2f&#125;  eV\nSigma300K = &#123;sigma300:.2f&#125; mS/cm&#39;        ax6.text(0.95, 0.95, text_str, transform=ax6.transAxes,                 fontsize=10, verticalalignment=&#39;top&#39;, horizontalalignment=&#39;right&#39;,                bbox=dict(facecolor=&#39;white&#39;, alpha=0.8, edgecolor=&#39;black&#39;))    else:        print(&#39;not the 132 atoms cell&#39;)def main():    fig = plt.figure(figsize = (12,60), dpi = 300)    gs = fig.add_gridspec(len(tem_list)* 2 + 1 ,2)    ax1= fig.add_subplot(gs[0,0])    ax2= fig.add_subplot(gs[0,1])    MSD_T(ax1)    arrniums(ax2)    for idx, tem in enumerate(tem_list):        row = idx*2 + 1         equili_folder = os.path.join(base_folder,str(tem),&#39;equili.log&#39;)        atoms_folder = os.path.join(base_folder,str(tem),&#39;atoms.log&#39;)        ax_equili_T = fig.add_subplot(gs[row,0])        ax_equili_E = fig.add_subplot(gs[row,1])        ax_atoms_T = fig.add_subplot(gs[row+1,0])         ax_atoms_E = fig.add_subplot(gs[row+1,1])        ET(equili_folder,ax_equili_T,ax_equili_E)        ET(atoms_folder,ax_atoms_T,ax_atoms_E)        ax_equili_T.set_title(f&#39;&#123;tem&#125; K&#39;)    plt.tight_layout()    plt.savefig(&#39;msd-TE.png&#39;)base_folder = &#39;./&#39;tem_list = [600,800,1000,1200]if __name__ == &#39;__main__&#39;:    main()</code></pre><h2 id="7-处理不同次的MSD，得到所有MSD"><a href="#7-处理不同次的MSD，得到所有MSD" class="headerlink" title="7. 处理不同次的MSD，得到所有MSD"></a>7. 处理不同次的MSD，得到所有MSD</h2><pre><code class="python"># %%import numpy as npimport osimport matplotlib.pyplot as pltfrom matplotlib import cmimport matplotlib.font_manager as fmbase_folder = &#39;./&#39;def plottem(tem, msd_data_list):    data1 = np.loadtxt(os.path.join(base_folder, str(tem), str(1), &#39;msd.dat&#39;))    data10 = data1[:, 0]    msd_data_list.append(data10)    for i in range(1, 4, 1):        target_path = os.path.join(base_folder, str(tem), str(i), &#39;msd.dat&#39;)        # print(target_path)        data = np.loadtxt(target_path, skiprows=1)        data_all = data[:, 4]        data_all = data_all *132 / 30        msd_data_list.append(data_all)        # Padding to match the maximum length of data    max_len = max(len(arr) for arr in msd_data_list)    padded_data = np.array([np.pad(arr, (0, max_len - len(arr)), &#39;constant&#39;, constant_values=np.nan) for arr in msd_data_list])        msd_data = padded_data.T    x = msd_data[:, 0] / 1000 * 2  # Convert time to ps and scale by 2 (example transformation)    y_data = msd_data[:, 1:]    return x, y_dataplt.rcParams[&#39;font.family&#39;] = &#39;Times New Roman&#39;plt.rcParams[&#39;font.size&#39;] = 16plt.rcParams[&#39;axes.linewidth&#39;] = 2plt.rcParams[&#39;axes.titlesize&#39;] = 16plt.rcParams[&#39;axes.labelsize&#39;] = 20plt.rcParams[&#39;axes.labelweight&#39;] = &#39;bold&#39;plt.rcParams[&#39;xtick.labelsize&#39;] = 16plt.rcParams[&#39;ytick.labelsize&#39;] = 16plt.rcParams[&#39;legend.fontsize&#39;] = 16# Create the plotplt.figure(figsize=(8, 6),dpi=300)colors = cm.tab10# Plot data for each temperaturefor tem in [600, 700, 800, 900, 1000]:    msd_data_list = []    x, y_data = plottem(tem, msd_data_list)    y_min = np.nanmin(y_data, axis=1)    y_max = np.nanmax(y_data, axis=1)    color = colors.colors[int(tem /100)-5]    # Plot the curves for the current temperature    for i in range(y_data.shape[1]):        plt.plot(x, y_data[:, i],color=color, label=f&#39;&#123;tem&#125; K&#39; if i==0 else &quot;&quot;)        # Fill between the min and max values    plt.fill_between(x, y_min, y_max, color=color, alpha=0.3)# Final plot settingsplt.xlabel(&#39;Time (ps)&#39;)plt.ylabel(r&#39;MSD ($\mathrm&#123;\AA&#125;^2$)&#39;)plt.xticks(fontweight = &#39;bold&#39;)plt.yticks(fontweight = &#39;bold&#39;)plt.legend(prop = &#123;&#39;weight&#39; :&#39;bold&#39;&#125;)plt.tight_layout()# plt.show()plt.savefig(&#39;888.png&#39;,dpi=600)</code></pre><h2 id="8-计算不同次分子动力学平均后的电导率"><a href="#8-计算不同次分子动力学平均后的电导率" class="headerlink" title="8.计算不同次分子动力学平均后的电导率"></a>8.计算不同次分子动力学平均后的电导率</h2><pre><code class="python">import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import linregressdef dealMSD(inputfile):    data = np.loadtxt(inputfile,skiprows = 1 )    data[:,0] = data[:,0] / 1000 * 2    n = data.shape[0]    start_idx = int(n*0.05)    end_idx = int(n*0.95)    time = data[start_idx:end_idx,0]    MSD = data[start_idx:end_idx,4]    slope, intercept, r_value, _, _ = linregress(time, MSD)    MSD_fit = slope * time + intercept        fig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)    ax1.plot(data[:, 0], data[:, 4],             label=&#39;600 K&#39;,             linewidth=2,          # 更粗的线条            color=&#39;#2ecc71&#39;,      # 柔和的绿色            alpha=0.8)      ax1.plot(time, MSD_fit, label=f&#39;Fit: y = &#123;slope:.2f&#125;x + &#123;intercept:.2f&#125;&#39;, color=&#39;red&#39;)    ax1.set_xlabel(&#39;t (ps)&#39;)    ax1.set_ylabel(r&#39;MSD ($\AA^2$)&#39;)    ax1.legend()    ax1.grid(True)    output_filename = inputfile.replace(&#39;/&#39;,&#39;-&#39;) + &#39;.png&#39;    plt.savefig(output_filename, bbox_inches=&#39;tight&#39;)    plt.close()#    plt.show()    return slopediffD = []temp = [600, 700,800,900,1000]for i in temp:    diff_temp = []    for j in [1,2,3]:        inputfile = f&quot;&#123;i&#125;/&#123;j&#125;/msd.dat&quot;        try:            slope = dealMSD(inputfile)            diffuD = slope / 60000            diff_temp.append(diffuD)            # diffD.append(diffuD)        except FileNotFoundError:            print(f&quot;文件 &#123;inputfile&#125; 不存在&quot;)            diff_temp.append(0)    diff_temp = np.array(diff_temp)    diff_temp_value = np.average(diff_temp)    diffD.append(diff_temp_value)logdiffD = np.log10(diffD)temp_inv = np.array([1000 / i for i in temp]) slope_D, intercept_D, _, _, _ = linregress(temp_inv, logdiffD)temp_inv_fit = np.linspace(1, 3.3, 100)logDfit = slope_D * temp_inv_fit + intercept_D#logDfit = slope_D * temp_inv + intercept_Dfig,ax1 = plt.subplots(figsize=(4,3),dpi = 600)ax1.plot(temp_inv_fit, logDfit, label=f&#39;Fit: y = &#123;slope_D:.2f&#125;x + &#123;intercept_D:.2f&#125;&#39;, color=&#39;red&#39;)ax1.scatter(temp_inv, logdiffD,color = &#39;blue&#39;,marker = &#39;o&#39;)ax1.set_xlabel(&#39;1000/T (1/K)&#39;)ax1.set_ylabel(r&#39;log10(D) (Å$^2$/ps)&#39;)ax1.set_xlim(0.9, 3.5)ax1.set_ylim(slope_D * 3.5 + intercept_D, slope_D * 0.9 + intercept_D+ 0.3)y_min = slope_D * 3.5 + intercept_Dy_max = slope_D * 0.9 + intercept_D + 0.3y_ticks = np.arange(np.floor(y_min / 0.5) * 0.5, np.ceil(y_max / 0.5) * 0.5, 0.5)ax1.set_yticks(y_ticks)ax1.legend()ax1.grid(True)plt.savefig(&#39;arrnius.png&#39;,bbox_inches=&#39;tight&#39;)from pymatgen.core import Structurestructure = Structure.from_file(&quot;POSCAR&quot;)volume = structure.volumeatomsnumber = 132Eanumber = 0.198691478T = 300logD300 = slope_D * (1000 / T) + intercept_DD = 10 ** logD300sigma300 = 1.858 * atomsnumber / volume / 300 * (10**12) * DEa = -Eanumber * slope_Dprint(Ea, sigma300)</code></pre><h1 id="3-一些结果"><a href="#3-一些结果" class="headerlink" title="3. 一些结果"></a>3. 一些结果</h1><p>大模型对NaInSiO的计算</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250121214124415.png" alt="image-20250121214124415"></p><p>大模型能够比较好的模拟出Na的离子扩散行为</p><h1 id="4-一些脚本"><a href="#4-一些脚本" class="headerlink" title="4. 一些脚本"></a>4. 一些脚本</h1><h2 id="1-把ase轨迹转换为XDATCAR的脚本"><a href="#1-把ase轨迹转换为XDATCAR的脚本" class="headerlink" title="1. 把ase轨迹转换为XDATCAR的脚本"></a>1. 把ase轨迹转换为XDATCAR的脚本</h2><p>使用ase自带的方法</p><p><code>ase convert --input-format traj --output-format vasp-xdatcar atoms.traj XDATCAR</code></p><p>python脚本</p><pre><code class="python">from ase.io import read, write# 读取原始轨迹文件atoms = read(&#39;atoms.traj&#39;, index=&#39;:&#39;)# 将轨迹写入 XDATCAR 格式write(&#39;XDATCAR&#39;, atoms)</code></pre><h2 id="2-蒸馏大模型数据变成符合acnn标准的xsf文件的脚本"><a href="#2-蒸馏大模型数据变成符合acnn标准的xsf文件的脚本" class="headerlink" title="2. 蒸馏大模型数据变成符合acnn标准的xsf文件的脚本"></a>2. 蒸馏大模型数据变成符合acnn标准的xsf文件的脚本</h2><pre><code class="python">from mattersim.applications.moldyn import MolecularDynamicsfrom ase.io import readfrom mattersim.forcefield import MatterSimCalculatorfrom mattersim.applications.relax import Relaxerimport numpy as np# 使用 MatterSimCalculator 初始化计算器calc = MatterSimCalculator(load_path=&quot;MatterSim-v1.0.0-5M.pth&quot;, device=&#39;cuda:0&#39;)# 设置优化器relaxer = Relaxer(    optimizer=&quot;BFGS&quot;,  # 优化方法，使用 FIRE 或 BFGS    filter=None,  # 是否应用过滤器    constrain_symmetry=True,  # 是否约束对称性)# 读取 POSCAR 文件atoms = read(&quot;POSCAR111.vasp&quot;, format=&#39;vasp&#39;)atoms.calc = calc# 进行结构弛豫converged, relaxed_structure = relaxer.relax_structures(atoms, optimizer=&quot;FIRE&quot;, filter=None, constrain_symmetry=True, fmax=0.05)# 设置计算器并准备进行分子动力学模拟relaxed_structure.calc = calc# 分子动力学参数ensemble = &quot;NVT_BERENDSEN&quot;temperature = 1000timestep = 1.0taut = 50trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;nvt_runner = MolecularDynamics(    atoms=relaxed_structure,    ensemble=ensemble,    temperature=temperature,    timestep=timestep,    taut=taut,    trajectory=trajectory,    loginterval=1,    logfile=logfile)# 保存每一帧为 .xsf 文件的函数def write2my(file_path, ene_i, lat_i, ele_i, coo_i, foc_i):    lat_i = lat_i.reshape(3, 3)    coo_i = coo_i.reshape(-1, 3)    foc_i = foc_i.reshape(-1, 3)    with open(file_path, &#39;w&#39;) as file:        file.write(f&quot;# total energy = &#123;ene_i&#125; eV\n\n&quot;)        file.write(&quot;CRYSTAL\n&quot;)        file.write(&quot;PRIMVEC\n&quot;)                for j in lat_i:            for k in j:                file.write(f&#39;&#123;k:20.8f&#125;&#39;)            file.write(&#39;\n&#39;)        file.write(&quot;PRIMCOORD\n&quot;)        file.write(f&quot;&#123;len(coo_i)&#125; 1\n&quot;)        for j in range(len(coo_i)):            element_name = chemical_symbols[ele_i[j]]            file.write(f&#39;&#123;element_name:2&#125; &#39;)            # 写入坐标            for k in coo_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            # 写入受力            for k in foc_i[j]:                file.write(f&quot;&#123;k:20.8f&#125;&quot;)            file.write(&quot;\n&quot;)# 模拟运行并每50步保存一次 .xsf 文件for step in range(1000000):  # 你可以根据需要设置步数    nvt_runner.run(1)  # 运行一个步长    atoms = nvt_runner.atoms  # 获取当前步长的原子对象        # 每50步保存一次数据    if step % 100 == 0:        # 获取总能量        energy = atoms.get_total_energy()                # 获取晶胞矩阵        cell = atoms.get_cell()                # 获取原子元素类型        atomic_numbers = atoms.get_atomic_numbers()                # 获取原子坐标        positions = atoms.get_positions()                # 获取原子受力        forces = atoms.get_forces()                # 保存当前帧数据为 .xsf 文件        frame_filename = f&quot;frame_&#123;step&#125;.xsf&quot;        write2my(frame_filename, energy, cell, atomic_numbers, positions, forces)</code></pre><h2 id="3-扩胞脚本"><a href="#3-扩胞脚本" class="headerlink" title="3. 扩胞脚本"></a>3. 扩胞脚本</h2><pre><code class="python">from pymatgen.io.vasp import Poscar# 读取 POSCAR 文件poscar = Poscar.from_file(&quot;POSCAR&quot;)# 扩胞为 2x2x3supercell = poscar.structure * [6, 6, 6]# 将扩胞后的结构写入新的 POSCAR 文件poscar = Poscar(supercell)poscar.write_file(&quot;POSCAR666.vasp&quot;)</code></pre><pre><code class="python">from ase.io import read,writefrom ase.build import make_supercellatoms = read(&quot;POSCAR&quot;)P = [[3, 0, 0],     [0, 3, 0],     [0, 0, 3]]supercell = make_supercell(atoms,P)supercell.write(&quot;POSCAR333&quot;)</code></pre><h2 id="4-计算体积脚本"><a href="#4-计算体积脚本" class="headerlink" title="4. 计算体积脚本"></a>4. 计算体积脚本</h2><pre><code class="python">from pymatgen.core import Structure# 读取POSCAR文件#structure = Structure.from_file(&quot;POSCAR.vasp&quot;)structure = Structure.from_file(&quot;POSCAR.vasp&quot;)# 计算体积volume = structure.volume# 输出体积print(f&quot;&#123;volume&#125; Å³&quot;)</code></pre><h2 id="5-元素替换脚本"><a href="#5-元素替换脚本" class="headerlink" title="5. 元素替换脚本"></a>5. 元素替换脚本</h2><pre><code class="python">import osimport shutil# 创建文件夹base_path = &quot;./&quot;  # 替换为你的根目录路径element_symbols = [&quot;La&quot;, &quot;Ce&quot;,&quot;Pr&quot;,&quot;Nd&quot;,&quot;Pm&quot;,&quot;Eu&quot;,&quot;Sm&quot;,&quot;Gd&quot;,&quot;Tb&quot;,&quot;Dy&quot;,&quot;Ho&quot;,&quot;Er&quot;,&quot;Tm&quot;,&quot;Sc&quot;,&quot;Ti&quot;,&quot;V&quot;,&quot;Nb&quot;,&quot;Ta&quot;,&quot;Cr&quot;,&quot;Mo&quot;,&quot;Mn&quot;,&quot;Fe&quot;,&quot;Ru&quot;,&quot;Co&quot;,&quot;Rh&quot;,&quot;Ir&quot;,&quot;Ni&quot;,&quot;Pd&quot;,&quot;Cu&quot;,&quot;B&quot;,&quot;Al&quot;,&quot;Ga&quot;,&quot;In&quot;,&quot;N&quot;,&quot;P&quot;,&quot;Sb&quot;,&quot;Bi&quot;]for idx, element in enumerate(element_symbols, start=1):    folder_name = f&quot;&#123;idx&#125;-&#123;element&#125;&quot;    folder_path = os.path.join(base_path, folder_name)        if not os.path.exists(folder_path):        os.makedirs(folder_path)# 读取原始 POSCAR 文件input_file = &quot;./POSCAR&quot;  # 替换为你的 POSCAR 文件路径with open(input_file, &quot;r&quot;) as file:    lines = file.readlines()# 逐个元素进行替换并创建文件夹for idx, element in enumerate(element_symbols, start=1):    folder_name = f&quot;&#123;idx&#125;-&#123;element&#125;&quot;    folder_path = os.path.join(base_path, folder_name)        modified_lines = [line.replace(&quot;Y&quot;, element) for line in lines]        # 写入修改后的内容到对应的文件夹    output_file = os.path.join(folder_path, &quot;POSCAR&quot;)    with open(output_file, &quot;w&quot;) as file:        file.writelines(modified_lines)        # 复制同名的 INCAR 文件到对应的文件夹    incar_source = &quot;./INCAR&quot;  # 替换为你的 INCAR 文件路径    incar_destination = os.path.join(folder_path, &quot;INCAR&quot;)    shutil.copy(incar_source, incar_destination)    lsf_source = &quot;./vasp.pbs&quot;  # 替换为你的 INCAR 文件路径    lsf_destination = os.path.join(folder_path, &quot;opt.lsf&quot;)    shutil.copy(lsf_source, lsf_destination)print(&quot;Replacement and file writing complete.&quot;)</code></pre><h1 id="5-使用m3gnet"><a href="#5-使用m3gnet" class="headerlink" title="5. 使用m3gnet"></a>5. 使用m3gnet</h1><pre><code class="python"># import warnings# from m3gnet.models import Relaxer# from pymatgen.core import Lattice, Structure# from ase.io import read, write# import tensorflow as tf# from ase.optimize import BFGS# for category in (UserWarning, DeprecationWarning):#     warnings.filterwarnings(&quot;ignore&quot;, category=category, module=&quot;tensorflow&quot;)# gpus = tf.config.list_physical_devices(&#39;GPU&#39;)# if len(gpus) &gt; 0:#     tf.config.set_visible_devices(gpus[0], &#39;GPU&#39;) # atoms = read(&quot;POSCAR222.vasp&quot;,format=&#39;vasp&#39;)#  # 只使用第一块 GPU# relaxer = Relaxer(optimizer = &quot;BFGS&quot;)  # This loads the default pre-trained model# relax_results = relaxer.relax(atoms,verbose=True)# final_structure = relax_results[&#39;final_structure&#39;]# final_energy_per_atom = float(relax_results[&#39;trajectory&#39;].energies[-1] / len(atoms))# print(f&quot;Relaxed lattice parameter is &#123;final_structure.lattice.abc[0]:.3f&#125; Å&quot;)# print(f&quot;Final energy is &#123;final_energy_per_atom:.3f&#125; eV/atom&quot;)from ase.optimize import BFGSfrom m3gnet.models import Relaxer,M3GNetCalculatorfrom ase.io import read, writeimport tensorflow as tfgpus = tf.config.list_physical_devices(&#39;GPU&#39;)if len(gpus) &gt; 0:    tf.config.set_visible_devices(gpus[0], &#39;GPU&#39;) relaxer = Relaxer()calculator = relaxer.calculatoratoms = read(&quot;POSCAR222.vasp&quot;,format=&#39;vasp&#39;)# print(f&quot;unrelaxed lattice parameter is &#123;atoms.lattice.abc[0]:.3f&#125; Å&quot;)atoms.calc = calculatoroptimizer = BFGS(atoms)optimizer.run(fmax=0.01) # print(f&quot;Relaxed lattice parameter is &#123;atoms.lattice.abc[0]:.3f&#125; Å&quot;)</code></pre><h1 id="6-ASE结合phonopy-使用大模型作为计算器"><a href="#6-ASE结合phonopy-使用大模型作为计算器" class="headerlink" title="6. ASE结合phonopy 使用大模型作为计算器"></a>6. ASE结合phonopy 使用大模型作为计算器</h1><h2 id="1-关于声子谱"><a href="#1-关于声子谱" class="headerlink" title="1.关于声子谱"></a>1.关于声子谱</h2><pre><code>有限位移法（Finite Displacement Method）是一种用于计算晶体声子谱的数值方法，其基本原理是通过对晶体结构中的原子进行微小的位移，计算相应的力，从而构建力常数矩阵，进而求解声子频率和振动模式。具体步骤如下：结构优化：首先，对晶体结构进行高精度的优化，以获得平衡的原子位置和晶格常数。在优化过程中，需要设置适当的收敛标准，如能量收敛（EDIFF）和力收敛（EDIFFG）等，以确保结构的准确性。构建超胞：在优化后的结构基础上，使用Phonopy等工具构建超胞。超胞的尺寸通常是原胞的整数倍，具体尺寸取决于所需的声子计算精度和计算资源。原子位移：对超胞中的每个原子进行微小的位移，通常在0.01到0.03埃之间。这些位移可以是沿着晶体轴的正负方向，或者在不同方向上进行。力计算：在每个位移结构上，进行单点能量计算，获取每个原子所受的力。这些力数据用于构建力常数矩阵。构建力常数矩阵：利用获得的力数据，构建力常数矩阵。该矩阵描述了原子间的相互作用，是计算声子谱的基础。声子谱计算：通过对力常数矩阵进行对角化，得到声子的频率和振动模式。进一步，可以绘制声子色散曲线，分析声子的行为。注意事项：有限位移法的计算精度高度依赖于力的计算精度，因此在结构优化和单点计算中需要设置严格的收敛标准。位移幅度的选择需要平衡计算精度和计算量，过大的位移可能导致力常数矩阵的非线性，过小的位移可能导致数值误差。计算过程中，可能需要考虑晶体的对称性，以减少计算量。通过上述步骤，有限位移法能够有效地计算晶体的声子谱，为研究材料的热力学性质、动力学稳定性等提供重要信息。</code></pre><h2 id="2-绘制投影声子态密度"><a href="#2-绘制投影声子态密度" class="headerlink" title="2. 绘制投影声子态密度"></a>2. 绘制投影声子态密度</h2><pre><code class="python">import numpy as npimport matplotlibmatplotlib.use(&#39;Agg&#39;)import matplotlib.pyplot as plt# 读取文件input_file1 = &#39;projected_dos.dat&#39;  # 输入文件名input_file2 = &#39;total_dos.dat&#39;output_file = &#39;tdosandpdos.dat&#39;  # 输出文件名# 读取数据data = np.loadtxt(input_file1)data2 = np.loadtxt(input_file2)# 处理数据# 第一列保持不变col0 = data2[:,1]col1 = data[:, 0]# 第2-31列求和放到第二列col2 = np.sum(data[:, 1:31], axis=1)# 第32-37列求和放到第三列col3 = np.sum(data[:, 31:37], axis=1)# 第38-61列求和放到第四列col4 = np.sum(data[:, 37:61], axis=1)# 第62-133列求和放到第五列col5 = np.sum(data[:, 61:133], axis=1)# 将处理后的数据保存到新文件processed_data = np.column_stack((col1, col2, col3, col4, col5,col0))np.savetxt(output_file, processed_data, fmt=&#39;%.6f&#39;, delimiter=&#39;\t&#39;)plt.figure(figsize=(10, 6), dpi=100)  # 增加 dpi 以提高分辨率# 绘制图形，增强样式plt.plot(processed_data[:, 0], processed_data[:, 1],          label=&#39;PDOS_Na&#39;,          linewidth=2,          # 更粗的线条         color=&#39;#2ecc71&#39;,      # 柔和的绿色         alpha=0.8)           # 轻微的透明度plt.plot(processed_data[:, 0], processed_data[:, 2],          label=&#39;PDOS_X&#39;,          linewidth=2,          color=&#39;#e74c3c&#39;,      # 柔和的红色         alpha=0.8)plt.plot(processed_data[:, 0], processed_data[:, 3],          label=&#39;PDOS_Si&#39;,          linewidth=2,          color=&#39;#3498db&#39;,      # 柔和的蓝色         alpha=0.8)plt.plot(processed_data[:, 0], processed_data[:, 4],          label=&#39;PDOS_O&#39;,          linewidth=2,          color=&#39;#9b59b6&#39;,      # 紫色         alpha=0.8)plt.plot(processed_data[:, 0], processed_data[:, 5],          label=&#39;TDOS&#39;,          linewidth=2.5,        # TDOS 线条稍粗         color=&#39;#34495e&#39;,      # 深灰色         alpha=0.9)# 自定义坐标轴plt.xlabel(&#39;ω (THz)&#39;, fontsize=12, labelpad=10)  # 使用希腊字母 ω 表示频率plt.ylabel(&#39;Number&#39;, fontsize=12, labelpad=10)plt.title(&#39;Partial and Total Density of States&#39;,           fontsize=14,           pad=15,           fontweight=&#39;bold&#39;)# 增强图例plt.legend(    fontsize=10,    frameon=True,           # 添加边框    framealpha=0.9,         # 轻微透明的边框    edgecolor=&#39;gray&#39;,       # 边框颜色    loc=&#39;best&#39;              # 自动选择最佳位置)# 自定义网格（可选 - 浅色网格）plt.grid(True, linestyle=&#39;--&#39;, alpha=0.3, color=&#39;gray&#39;)# 调整边框（坐标轴边框）ax = plt.gca()ax.spines[&#39;top&#39;].set_color(&#39;gray&#39;)ax.spines[&#39;right&#39;].set_color(&#39;gray&#39;)ax.spines[&#39;left&#39;].set_color(&#39;gray&#39;)ax.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;)ax.spines[&#39;top&#39;].set_alpha(0.5)ax.spines[&#39;right&#39;].set_alpha(0.5)# 添加次要刻度plt.minorticks_on()# 调整刻度参数plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=10)plt.tick_params(axis=&#39;both&#39;, which=&#39;minor&#39;, length=2)# 紧凑布局，防止标签被截断plt.tight_layout()# 保存高质量图像plot_file = &#39;DOS.png&#39;plt.savefig(plot_file, dpi=300, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;)</code></pre><h1 id="7-ASE-grace"><a href="#7-ASE-grace" class="headerlink" title="7. ASE+grace"></a>7. ASE+grace</h1><h2 id="1-BFGS优化-NVTBerendsen分子动力学"><a href="#1-BFGS优化-NVTBerendsen分子动力学" class="headerlink" title="1. BFGS优化+NVTBerendsen分子动力学"></a>1. BFGS优化+NVTBerendsen分子动力学</h2><p>NVTBerendsen在taut比较小时可能得不到正确的动力学结果 ，因此模拟还是要用nose Hoover</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320144100597.png" alt="image-20250320144100597"></p><pre><code class="python">from ase.io import read, writefrom ase.optimize import BFGSfrom ase.md.nvtberendsen import NVTBerendsenfrom ase import unitsfrom tensorpotential.calculator import grace_fmimport numpy as npimport osfrom ase.md.velocitydistribution import (     MaxwellBoltzmannDistribution,    Stationary,)calc = grace_fm(&#39;GRACE-2L-OMAT&#39;)atoms = read(&quot;POSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcdyn = BFGS(atoms)dyn.run(fmax=0.01) temperature = 600timestep = 2.0 * units.fstaut = 100trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;  loginterval = 1append_trajectory = FalseMaxwellBoltzmannDistribution(atoms,temperature_K=temperature,force_temp = True)Stationary(atoms)md = NVTBerendsen(atoms=atoms,timestep=timestep,temperature_K = temperature,taut=taut,trajectory = trajectory,logfile=logfile,loginterval= loginterval,append_trajectory=append_trajectory)md.run(250000)#后处理#将轨迹写入XDATCARatoms_xda = read(&#39;atoms.traj&#39;, index=&#39;:&#39;)write(&#39;XDATCAR&#39;, atoms_xda)os.remove(&#39;atoms.traj&#39;)import numpy as npimport matplotlibmatplotlib.use(&quot;Agg&quot;)import matplotlib.pyplot as pltinput_file = &#39;atoms.log&#39;data = np.loadtxt(input_file,skiprows=1)fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,6),dpi=100)ax1.plot(data[:, 0], data[:, 1],          label=&#39;t-Energy&#39;,          linewidth=2,          # 更粗的线条         color=&#39;#2ecc71&#39;,      # 柔和的绿色         alpha=0.8)            # 轻微的透明度ax1.set_xlabel(&#39;t (ps)&#39;)ax1.set_ylabel(&#39;Energy (eV)&#39;)ax1.set_title(&#39;t-Energy&#39;)ax1.legend()ax1.grid(True)ax2.plot(data[:, 0], data[:, 4],          label=&#39;t-Temperature&#39;,          linewidth=2,          color=&#39;#e74c3c&#39;,      # 柔和的红色         alpha=0.8)ax2.set_xlabel(&#39;t (ps)&#39;)ax2.set_ylabel(&#39;Temperature (K)&#39;)ax2.set_title(&#39;t-Energy&#39;)ax2.legend()ax2.grid(True)fig.tight_layout()plot_file = &#39;t-E-T.png&#39;fig.savefig(plot_file, dpi=300, bbox_inches=&#39;tight&#39;, facecolor=&#39;white&#39;)#vaspkit处理import subprocessdef run_vaspkit(task_id, selectype, element, skip_steps, frame_interval):    # 构造VASPKIT命令    command = f&quot;(echo &#123;task_id&#125;;echo &#123;selectype&#125;;echo &#123;element&#125;;echo &#123;skip_steps&#125;;echo &#123;frame_interval&#125;)| vaspkit&quot;    # 执行命令    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)    stdout, stderr = process.communicate()    # 输出结果    if process.returncode == 0:        print(&quot;VASPKIT执行成功！&quot;)        print(stdout.decode())    else:        print(&quot;VASPKIT执行失败！&quot;)        print(stderr.decode())# 示例调用task_id = 722selectype = 1element = &quot;Na&quot;skip_steps = 5000frame_interval = 1run_vaspkit(task_id, selectype, element, skip_steps, frame_interval)if os.path.exists(&#39;MSD.dat&#39;):    os.remove(&#39;XDATCAR&#39;)</code></pre><h2 id="2-使用ASE中的Nose-Hoover-NVT"><a href="#2-使用ASE中的Nose-Hoover-NVT" class="headerlink" title="2. 使用ASE中的Nose-Hoover NVT"></a>2. 使用ASE中的Nose-Hoover NVT</h2><pre><code class="python">from ase.io import read, writefrom ase.optimize import BFGSfrom ase.md.nose_hoover_chain import NoseHooverChainNVTfrom ase import unitsfrom tensorpotential.calculator import grace_fmimport numpy as npimport osfrom ase.md.velocitydistribution import (     MaxwellBoltzmannDistribution,    Stationary,)calc = grace_fm(&#39;GRACE-2L-OMAT&#39;)atoms = read(&quot;POSCAR&quot;,format=&#39;vasp&#39;)atoms.calc = calcdyn = BFGS(atoms)dyn.run(fmax=0.01)write(&quot;optimized_structure.vasp&quot;, atoms) temperature = 600timestep = 2.0 * units.fstdamp = 100trajectory = &quot;atoms.traj&quot;logfile = &quot;atoms.log&quot;  loginterval = 1append_trajectory = FalseMaxwellBoltzmannDistribution(atoms,temperature_K=temperature,force_temp = True)Stationary(atoms)md1 = NoseHooverChainNVT(atoms=atoms,timestep=timestep,temperature_K = temperature,tdamp=tdamp,trajectory = None,logfile=&#39;equili.log&#39;,loginterval= loginterval,append_trajectory=append_trajectory)md1.run(10000)md2= NoseHooverChainNVT(atoms=atoms,timestep=timestep,temperature_K = temperature,tdamp=tdamp,trajectory = trajectory,logfile=logfile,loginterval= loginterval,append_trajectory=append_trajectory)md2.run(250000)</code></pre><h1 id="8-Nose-Hoover和Berendsen的对比"><a href="#8-Nose-Hoover和Berendsen的对比" class="headerlink" title="8. Nose-Hoover和Berendsen的对比"></a>8. Nose-Hoover和Berendsen的对比</h1><blockquote><p>更大的晶胞也需要更长的弛豫时间</p></blockquote><ul><li><p>Nose-Hoover  tdamp&#x3D; 200 vs Berendsen taut&#x3D;100</p><p>Nose-Hoover和Berendsen的动力学过程不同</p><p>Nose-Hoover</p><p><img src="/./../../%E5%9B%BE%E7%89%87/t-E-T-1742462430030-9.png" alt="t-E-T"></p></li></ul><p>Berendsen</p><p><img src="/./../../%E5%9B%BE%E7%89%87/t-E-T.png" alt="t-E-T"></p><ul><li><p>Nose-Hoover tdamp &#x3D; 40 100 200 400 对比</p><p>40 </p><p><img src="/./../../%E5%9B%BE%E7%89%87/t-E-T-1742462498650-11.png" alt="t-E-T"></p></li></ul><p>​     100</p><p>   <img src="/./../../%E5%9B%BE%E7%89%87/t-E-T-1742462527710-13.png" alt="t-E-T"></p><p>200</p><p><img src="/./../../%E5%9B%BE%E7%89%87/t-E-T-1742462552301-15.png" alt="t-E-T"></p><p>400</p><p><img src="/./../../%E5%9B%BE%E7%89%87/t-E-T-1742462576244-17.png" alt="t-E-T"></p><p>from deepseek</p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320172836556.png" alt="image-20250320172836556"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320172853036.png" alt="image-20250320172853036"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320172925841.png" alt="image-20250320172925841"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320172947930.png" alt="image-20250320172947930"></p><p><img src="/./../../%E5%9B%BE%E7%89%87/image-20250320173043216.png" alt="image-20250320173043216"></p><h1 id="9-学习mattersim的phonon的方法"><a href="#9-学习mattersim的phonon的方法" class="headerlink" title="9. 学习mattersim的phonon的方法"></a>9. 学习mattersim的phonon的方法</h1><h2 id="1-包装ase的结构为phonon的格式"><a href="#1-包装ase的结构为phonon的格式" class="headerlink" title="1. 包装ase的结构为phonon的格式"></a>1. 包装ase的结构为phonon的格式</h2><p><a href="https://github.com/microsoft/mattersim/blob/main/src/mattersim/utils/phonon_utils.py">https://github.com/microsoft/mattersim/blob/main/src/mattersim/utils/phonon_utils.py</a></p><pre><code class="python">def to_phonopy_atoms(atoms: Atoms):    &quot;&quot;&quot;    Transform ASE atoms object to Phonopy object    Args:        atoms (Atoms): ASE atoms object to provide lattice informations.    &quot;&quot;&quot;    phonopy_atoms = PhonopyAtoms(        symbols=atoms.get_chemical_symbols(),        cell=atoms.get_cell(),        masses=atoms.get_masses(),        positions=atoms.get_positions(),    )    return phonopy_atomsdef to_ase_atoms(phonopy_atoms):    &quot;&quot;&quot;    Transform Phonopy object to ASE atoms object    Args:        phonopy_atoms (Phonopy): Phonopy object to provide lattice informations.    &quot;&quot;&quot;    atoms = Atoms(        symbols=phonopy_atoms.symbols,        cell=phonopy_atoms.cell,        masses=phonopy_atoms.masses,        positions=phonopy_atoms.positions,        pbc=True,    )    return atoms</code></pre><h2 id="2-修改mattersim以输出投影声子密度，并且指定固定的路径内的q点个数来匹配自己写的绘制声子谱的脚本（vasp模拟部分）（auto-band得到的q点个数在不同路径上不是固定的）"><a href="#2-修改mattersim以输出投影声子密度，并且指定固定的路径内的q点个数来匹配自己写的绘制声子谱的脚本（vasp模拟部分）（auto-band得到的q点个数在不同路径上不是固定的）" class="headerlink" title="2. 修改mattersim以输出投影声子密度，并且指定固定的路径内的q点个数来匹配自己写的绘制声子谱的脚本（vasp模拟部分）（auto_band得到的q点个数在不同路径上不是固定的）"></a>2. 修改mattersim以输出投影声子密度，并且指定固定的路径内的q点个数来匹配自己写的绘制声子谱的脚本（vasp模拟部分）（auto_band得到的q点个数在不同路径上不是固定的）</h2><p>修改&#x2F;lib&#x2F;python3.12&#x2F;site-packages&#x2F;mattersim&#x2F;applications&#x2F;phonon.py </p><pre><code class="python">def compute_phonon_spectrum_dos(        atoms: Atoms, phonon: Phonopy, k_point_mesh: Union[int, Iterable[int]]    ):        &quot;&quot;&quot;        Calculate phonon spectrum and DOS based on force constant matrix in        phonon object        Args:            atoms (Atoms): ASE atoms object to provide lattice information            phonon (Phonopy): Phonopy object which contains force constants matrix            k_point_mesh (Union[int, Iterable[int]]): The qpoints number in First                Brillouin Zone in three directions for DOS calculation.        &quot;&quot;&quot;        print(f&quot;Qpoints mesh for Brillouin Zone integration : &#123;k_point_mesh&#125;&quot;)        phonon.run_mesh(k_point_mesh)        print(            &quot;Dispersion relations using phonopy for &quot;            + str(atoms.symbols)            + &quot; ...&quot;            + &quot;\n&quot;        )        # plot phonon spectrum        phonon.auto_band_structure(plot=True, write_yaml=True, with_eigenvectors=True).savefig(            f&quot;&#123;str(atoms.symbols)&#125;_phonon_band.png&quot;, dpi=300        )        phonon.auto_total_dos(plot=True, write_dat=True).savefig(            f&quot;&#123;str(atoms.symbols)&#125;_phonon_dos.png&quot;, dpi=300        )        # Save additional files        phonon.save(settings=&#123;&quot;force_constants&quot;: True&#125;)</code></pre><p>主要修改了compute_force_constants 这个函数，替换为 </p><pre><code class="python"># -*- coding: utf-8 -*-import datetimeimport osfrom typing import Iterable, Unionimport numpy as npfrom ase import Atomsfrom phonopy import Phonopyfrom tqdm import tqdmfrom seekpath import get_pathfrom phonopy.phonon.band_structure import get_band_qpoints_and_path_connectionsfrom mattersim.utils.phonon_utils import (    get_primitive_cell,    to_ase_atoms,    to_phonopy_atoms,)from mattersim.utils.supercell_utils import get_supercell_parametersclass PhononWorkflow(object):    &quot;&quot;&quot;    This class is used to calculate the phonon dispersion relationship of    material using phonopy    &quot;&quot;&quot;    def __init__(        self,        atoms: Atoms,        find_prim: bool = False,        work_dir: str = None,        amplitude: float = 0.01,        supercell_matrix: np.ndarray = None,        qpoints_mesh: np.ndarray = None,        max_atoms: int = None,        calc_spec: bool = True,    ):        &quot;&quot;&quot;_summary        Args:            atoms (Atoms): ASE atoms object contains structure information and                calculator.            find_prim (bool, optional): If find the primitive cell and use it                to calculate phonon. Default to False.            work_dir (str, optional): workplace path to contain phonon result.                Defaults to data + chemical_symbols + &#39;phonon&#39;            amplitude (float, optional): Magnitude of the finite difference to                displace in force constant calculation, in Angstrom. Defaults                to 0.01 Angstrom.            supercell_matrix (nd.array, optional): Supercell matrix for constr                -uct supercell, priority over than max_atoms. Defaults to None.            qpoints_mesh (nd.array, optional): Qpoint mesh for IBZ integral,                priority over than max_atoms. Defaults to None.            max_atoms (int, optional): Maximum atoms number limitation for the                supercell generation. If not set, will automatic generate super                -cell based on symmetry. Defaults to None.            calc_spec (bool, optional): If calculate the spectrum and check                 imaginary frequencies. Default to True.        &quot;&quot;&quot;        assert (            atoms.calc is not None        ), &quot;PhononWorkflow only accepts ase atoms with an attached calculator&quot;        if find_prim:            self.atoms = get_primitive_cell(atoms)            self.atoms.calc = atoms.calc        else:            self.atoms = atoms        if work_dir is not None:            self.work_dir = work_dir        else:            current_datetime = datetime.datetime.now()            formatted_datetime = current_datetime.strftime(&quot;%Y-%m-%d-%H-%M&quot;)            self.work_dir = (                f&quot;&#123;formatted_datetime&#125;-&#123;atoms.get_chemical_formula()&#125;-phonon&quot;            )        self.amplitude = amplitude        if supercell_matrix is not None:            if supercell_matrix.shape == (3, 3):                self.supercell_matrix = supercell_matrix            elif supercell_matrix.shape == (3,):                self.supercell_matrix = np.diag(supercell_matrix)            else:                assert (                    False                ), &quot;Supercell matrix must be an array (3,1) or a matrix (3,3).&quot;        else:            self.supercell_matrix = supercell_matrix        if qpoints_mesh is not None:            assert qpoints_mesh.shape == (3,), &quot;Qpoints mesh must be an array (3,1).&quot;            self.qpoints_mesh = qpoints_mesh        else:            self.qpoints_mesh = qpoints_mesh        self.max_atoms = max_atoms        self.calc_spec = calc_spec    def compute_force_constants(self, atoms: Atoms, nrep_second: np.ndarray):        &quot;&quot;&quot;        Calculate force constants        Args:            atoms (Atoms): ASE atoms object to provide lattice informations.            nrep_second (np.ndarray): Supercell size used for 2nd force                constant calculations.        &quot;&quot;&quot;        print(f&quot;Supercell matrix for 2nd force constants : \n&#123;nrep_second&#125;&quot;)        # Generate phonopy object        phonon = Phonopy(            to_phonopy_atoms(atoms),            supercell_matrix=nrep_second,            primitive_matrix=&quot;auto&quot;,            log_level=2,        )        # Generate displacements        phonon.generate_displacements(distance=self.amplitude)        # Compute force constants        second_scs = phonon.supercells_with_displacements        second_force_sets = []        print(&quot;\n&quot;)        print(&quot;Inferring forces for displaced atoms and computing fcs ...&quot;)        for disp_second in tqdm(second_scs):            pa_second = to_ase_atoms(disp_second)            pa_second.calc = self.atoms.calc            second_force_sets.append(pa_second.get_forces())        phonon.forces = np.array(second_force_sets)        phonon.produce_force_constants()        phonon.symmetrize_force_constants()        return phonon    @staticmethod    def compute_phonon_spectrum_dos(        atoms: Atoms, phonon: Phonopy, k_point_mesh: Union[int, Iterable[int]]    ):        &quot;&quot;&quot;        Calculate phonon spectrum and DOS based on force constant matrix in        phonon object        Args:            atoms (Atoms): ASE atoms object to provide lattice information            phonon (Phonopy): Phonopy object which contains force constants matrix            k_point_mesh (Union[int, Iterable[int]]): The qpoints number in First                Brillouin Zone in three directions for DOS calculation.        &quot;&quot;&quot;        print(f&quot;Qpoints mesh for Brillouin Zone integration : &#123;k_point_mesh&#125;&quot;)        phonon.run_mesh(k_point_mesh)        print(            &quot;Dispersion relations using phonopy for &quot;            + str(atoms.symbols)            + &quot; ...&quot;            + &quot;\n&quot;        )        #phonon.auto_band_structure(plot=True, write_yaml=True, with_eigenvectors=False).savefig(           # f&quot;&#123;str(atoms.symbols)&#125;_auto_band.png&quot;, dpi=300)                 lattice = atoms.cell        scaled_positions = atoms.get_scaled_positions()        atomic_numbers = atoms.get_atomic_numbers()        path_data = get_path((lattice, scaled_positions, atomic_numbers))        point_coords = path_data[&quot;point_coords&quot;]        band_segments = path_data[&quot;path&quot;]        from phonopy.phonon.band_structure import get_band_qpoints_and_path_connections        raw_labels = []        path = []        connectionsall = []        for start_label, end_label in band_segments:            q_start = point_coords[start_label]            q_end = point_coords[end_label]            path.append([q_start,q_end])            raw_labels.append(start_label)            raw_labels.append(end_label)            connectionsall.append(&#39;False&#39;)        qpoints, connections = get_band_qpoints_and_path_connections(path, npoints=51)        def convert_labels(raw_labels):            latex_labels = []            for label in raw_labels:                if label.upper() == &quot;GAMMA&quot;:                    latex_labels.append(&quot;$\\Gamma$&quot;)                else:                    latex_labels.append(label)            return latex_labels        labels = convert_labels(raw_labels)        phonon.run_band_structure(qpoints, path_connections=connections, labels=labels)        bs = phonon.band_structure        bs.write_yaml()        plt = phonon.plot_band_structure()        plt.savefig(f&quot;&#123;str(atoms.symbols)&#125;_manual_band.png&quot;,dpi=300)        phonon.auto_total_dos(plot=True, write_dat=True).savefig(            f&quot;&#123;str(atoms.symbols)&#125;_phonon_dos.png&quot;, dpi=300        )        phonon.auto_projected_dos(plot=True,write_dat=True).savefig(            f&quot;&#123;str(atoms.symbols)&#125;_phonon_projected_dos.png&quot;, dpi=300        )        phonon.save(settings=&#123;&quot;force_constants&quot;: True&#125;)    @staticmethod    def check_imaginary_freq(phonon: Phonopy):        &quot;&quot;&quot;        Check whether phonon has imaginary frequency.        Args:            phonon (Phonopy): Phonopy object which contains phonon spectrum frequency.        &quot;&quot;&quot;        band_dict = phonon.get_band_structure_dict()        frequencies = np.concatenate(            [np.array(freq).flatten() for freq in band_dict[&quot;frequencies&quot;]], axis=None        )        has_imaginary = False        if np.all(np.array(frequencies) &gt;= -0.299):            pass        else:            print(&quot;Warning! Imaginary frequencies found!&quot;)            has_imaginary = True        return has_imaginary    def run(self):        &quot;&quot;&quot;        The entrypoint to start the workflow.        &quot;&quot;&quot;        current_path = os.path.abspath(&quot;.&quot;)        try:            # check folder exists            if not os.path.exists(self.work_dir):                os.makedirs(self.work_dir)            os.chdir(self.work_dir)            try:                # Generate supercell parameters based on optimized structures                nrep_second, k_point_mesh = get_supercell_parameters(                    self.atoms, self.supercell_matrix, self.qpoints_mesh, self.max_atoms                )            except Exception as e:                print(&quot;Error whille generating supercell parameters:&quot;, e)                raise            try:                # Calculate 2nd force constants                phonon = self.compute_force_constants(self.atoms, nrep_second)            except Exception as e:                print(&quot;Error while computing force constants:&quot;, e)                raise            if self.calc_spec:                try:                    # Calculate phonon spectrum                    self.compute_phonon_spectrum_dos(self.atoms, phonon, k_point_mesh)                    # check whether has imaginary frequency                    has_imaginary = self.check_imaginary_freq(phonon)                except Exception as e:                    print(&quot;Error while computing phonon spectrum and dos:&quot;, e)                    raise            else:                has_imaginary = &#39;Not calculated, set calc_spec True&#39;                phonon.save(settings=&#123;&quot;force_constants&quot;: True&#125;)        except Exception as e:            print(&quot;An error occurred during the Phonon workflow:&quot;, e)            raise        finally:            os.chdir(current_path)        return has_imaginary, phonon</code></pre>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-原子大模型排名&quot;&gt;&lt;a href=&quot;#1-原子大模型排名&quot; class=&quot;headerlink&quot; title=&quot;1. 原子大模型排名&quot;&gt;&lt;/a&gt;1. 原子大模型排名&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://matbench-d</summary>
      
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>一些课题</title>
    <link href="http://example.com/2025/01/09/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E4%B8%80%E4%BA%9B%E8%AF%BE%E9%A2%98/"/>
    <id>http://example.com/2025/01/09/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E4%B8%80%E4%BA%9B%E8%AF%BE%E9%A2%98/</id>
    <published>2025-01-09T06:10:02.000Z</published>
    <updated>2025-11-26T08:50:19.973Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="91102f2f339d47b3d8cf68adb5d2b7278bef7c7d9ce6e55769377ec7d933443a">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="课题研究" scheme="http://example.com/categories/%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>镶嵌图形</title>
    <link href="http://example.com/2025/01/02/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E9%95%B6%E5%B5%8C%E5%9B%BE%E5%BD%A2/"/>
    <id>http://example.com/2025/01/02/6-%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/%E9%95%B6%E5%B5%8C%E5%9B%BE%E5%BD%A2/</id>
    <published>2025-01-02T03:07:02.000Z</published>
    <updated>2025-01-07T00:45:23.403Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="79d088dd5b4958d74509930a13fec04f9144751aa75bbfb8785681847fb55252">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="课题研究" scheme="http://example.com/categories/%E8%AF%BE%E9%A2%98%E7%A0%94%E7%A9%B6/"/>
    
    
  </entry>
  
  <entry>
    <title>交流站</title>
    <link href="http://example.com/2024/12/25/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E4%BA%A4%E6%B5%81%E7%AB%99/"/>
    <id>http://example.com/2024/12/25/9-%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/%E4%BA%A4%E6%B5%81%E7%AB%99/</id>
    <published>2024-12-25T08:15:50.000Z</published>
    <updated>2025-11-24T07:02:37.571Z</updated>
    
    <content type="html"><![CDATA[<p>[toc]</p><h1 id="1-技术站"><a href="#1-技术站" class="headerlink" title="1. 技术站"></a>1. 技术站</h1><ol><li><a href="https://www.coolaler.com/forums/threads/word-grindeq-math-utilities-2024.382145/">台湾硬件交流</a></li><li><a href="https://www.nodeseek.com/">NodeSeek</a></li><li><a href="https://linux.do/">LINUX DO </a></li><li><a href="https://idcflare.com/">IDC Flare</a></li><li><a href="https://news.ycombinator.com/news">Hacker News</a></li><li><a href="https://tweakers.net/">荷兰科技论坛tweakers</a></li></ol><h1 id="2-知识站"><a href="#2-知识站" class="headerlink" title="2. 知识站"></a>2. 知识站</h1><ol><li><a href="https://www.chaspark.com/#/home">黄大年茶思屋</a></li><li><a href="https://physics.aps.org/">Physics - spotlighting exceptional research</a></li><li><a href="https://philippdubach.com/">philippdubach.com — Home - philippdubach.com</a></li></ol><h1 id="3-新闻站"><a href="#3-新闻站" class="headerlink" title="3. 新闻站"></a>3. 新闻站</h1><ol><li><a href="https://kite.kagi.com/">kite</a></li><li><a href="https://river.to/occasus/vncwdhq2xje7wp/rawzi0ruu6p6a1">Light for you</a></li></ol><h1 id="4-博客站"><a href="#4-博客站" class="headerlink" title="4. 博客站"></a>4. 博客站</h1><ol><li><a href="https://matthogg.fyi/">Matt Hogg FYI</a></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;[toc]&lt;/p&gt;
&lt;h1 id=&quot;1-技术站&quot;&gt;&lt;a href=&quot;#1-技术站&quot; class=&quot;headerlink&quot; title=&quot;1. 技术站&quot;&gt;&lt;/a&gt;1. 技术站&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https://www.coolaler.com/for</summary>
      
    
    
    
    <category term="网站收集" scheme="http://example.com/categories/%E7%BD%91%E7%AB%99%E6%94%B6%E9%9B%86/"/>
    
    
  </entry>
  
</feed>
